{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF2: Image Classification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNOmWEGbYiPuf1Tene+RQ6p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a532b79a2df848ff9085819636d62980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_82936392274b4802a51e583d68defa16",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6950d327f99d4ab7aca8e9d1bd349712",
              "IPY_MODEL_b46ed4ac9f79408ca47aa798aad78a0c"
            ]
          }
        },
        "82936392274b4802a51e583d68defa16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6950d327f99d4ab7aca8e9d1bd349712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8a34716ef2d84f14837e185c18c91041",
            "_dom_classes": [],
            "description": "Dl Completed...",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 19,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 19,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c0b93c08b2914ec9928fb6b79e48cd6b"
          }
        },
        "b46ed4ac9f79408ca47aa798aad78a0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_814c6695dc1e4133b20dac56d27f8215",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 19/19 [00:00&lt;00:00, 26.69 file/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3304cadd97794077b8571e2e752410a5"
          }
        },
        "8a34716ef2d84f14837e185c18c91041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c0b93c08b2914ec9928fb6b79e48cd6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "814c6695dc1e4133b20dac56d27f8215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3304cadd97794077b8571e2e752410a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sweaterr/1_CODE/blob/master/TF2_Image_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqfte9otPEAr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "outputId": "3fb19df4-0ec4-45f2-ce9a-b4c80c6a545b"
      },
      "source": [
        "! pip install tensorflow==2.1.0"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.4.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.1.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.11.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.33.6)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (2.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.17.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.16.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.11.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (42.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.7)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5GxU-KMPCDF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "2d095e61-3eca-4af8-cff6-b443f6663cad"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2B_sYGfq-VN",
        "colab_type": "text"
      },
      "source": [
        "# Autograph\n",
        "\n",
        "Automatically converting Python code into its graphical representation is done with the use of AutoGraph. In TensorFlow 2.0, AutoGraph is automatically applied to a function when it is decorated with @tf.function. This decorator creates callable graphs from Python functions.\n",
        "\n",
        "A function, once decorated correctly, is processed by tf.function and the tf.autograph module in order to convert it into its graphical representation. The following diagram shows a schematic representation of what happens when a decorated function is called:\n",
        "\n",
        "![](https://learning.oreilly.com/library/view/hands-on-neural-networks/9781789615555/assets/ba5dc45f-0079-4085-b76d-2f6fb034fae8.png)\n",
        "\n",
        "Schematic representation of what happens when a function, f, decorated with @tf.function, which is called on the first call and on any other subsequent call\n",
        "On the first call of the annotated function, the following occurs:\n",
        "\n",
        "1. The function is executed and traced. Eager execution is disabled in this context, and so every tf.* method defines a tf.Operation node that produces a tf.Tensor output, exactly like it does in TensorFlow 1.x.\n",
        "1. The tf.autograph module is used to detect Python constructs that can be converted into their graph equivalent. The graph representation is built from the function trace and AutoGraph information. This is done in order to preserve the execution order that's defined in Python.\n",
        "1. The tf.Graph object has now been built.\n",
        "1. Based on the function name and the input parameters, a unique ID is created and associated with the graph. The graph is then cached into a map so that it can be reused when a second invocation occurs and the ID matches.\n",
        "\n",
        "Converting a function into its graph representation usually requires us to think; in TensorFlow 1.x, not every function that works in eager mode can be converted painlessly into its graph version.\n",
        "\n",
        "For instance, a variable in eager mode is a Python object that follows the Python rules regarding its scope. In graph mode, as we found out in the previous chapter, a variable is a persistent object that will continue to exist, even if its associated Python variable goes out of scope and is garbage-collected.\n",
        "\n",
        "Therefore, special attention has to be placed on software design: if a function has to be graph-accelerated and it creates a status (using tf.Variable and similar objects), it is up to the developer to take care of avoiding having to recreate the variable every time the function is called.\n",
        "\n",
        "For this reason, tf.function parses the function body multiple times while looking for the tf.Variable definition. If, at the second invocation, it finds out that a variable object is being recreated, it raises an exception:\n",
        "\n",
        "```\n",
        "ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
        "```\n",
        "\n",
        "In practice, if we have defined a function that performs a simple operation that uses a tf.Variable inside it, we have to ensure that the object is only created once.\n",
        "\n",
        "The following function works correctly in eager mode, but it fails to execute if it is decorated with @tf.function and is raising the preceding exception:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mryovAA_rhN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f():\n",
        "    a = tf.constant([[10,10],[11.,1.]])\n",
        "    x = tf.constant([[1.,0.],[0.,1.]])\n",
        "    b = tf.Variable(12.)\n",
        "    y = tf.matmul(a, x) + b\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvclex3dPMWV",
        "colab_type": "text"
      },
      "source": [
        "Handling functions that create a state means that we have to rethink our usage of graph-mode. A state is a persistent object, such as a variable, and the variable can't be redeclared more than once. Due to this, the function definition can be changed in two ways:\n",
        "\n",
        "By passing the variable as an input parameter\n",
        "By breaking the function scope and inheriting a variable from the external scope\n",
        "The first option requires changing the function definition that's making it:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbVmFaGoPPDb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "de68e826-810b-48dc-adb6-bdc04ee56ae1"
      },
      "source": [
        "@tf.function\n",
        "def f(b):\n",
        "    a = tf.constant([[10,10],[11.,1.]])\n",
        "    x = tf.constant([[1.,0.],[0.,1.]])\n",
        "    y = tf.matmul(a, x) + b\n",
        "    return y\n",
        "\n",
        "var = tf.Variable(12.)\n",
        "print(f(var))\n",
        "print(f(15))\n",
        "print(f(tf.constant(1, tf.float32)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[22. 22.]\n",
            " [23. 13.]], shape=(2, 2), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[25. 25.]\n",
            " [26. 16.]], shape=(2, 2), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[11. 11.]\n",
            " [12.  2.]], shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ7EK2NzPRoS",
        "colab_type": "text"
      },
      "source": [
        "f now accepts a Python input variable, b. This variable can be a tf.Variable, a tf.Tensor, and also a NumPy object or a Python type. Every time the input type changes, a new graph is created in order to make an accelerated version of the function that works for any required input type (this is required because of how a TensorFlow graph is statically typed).\n",
        "\n",
        "The second option, on the other hand, requires breaking down the function scope, making the variable available outside the scope of the function itself. In this case, there are two paths we can follow:\n",
        "\n",
        "* Not recommended: Use global variables\n",
        "* Recommended: Use Keras-like objects\n",
        "The first path, which is not recommended, consists of declaring the variable outside the function body and using it inside, ensuring that it will only be declared once:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cZtfoDyPTqP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "4b096184-489f-4878-dc64-9af1051265d4"
      },
      "source": [
        "b = None\n",
        "\n",
        "@tf.function\n",
        "def f():\n",
        "    a = tf.constant([[10, 10], [11., 1.]])\n",
        "    x = tf.constant([[1., 0.], [0., 1.]])\n",
        "    global b\n",
        "    if b is None:\n",
        "        b = tf.Variable(12.)\n",
        "    y = tf.matmul(a, x) + b\n",
        "    return y\n",
        "\n",
        "f()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[22., 22.],\n",
              "       [23., 13.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOYZiJjOPVZ1",
        "colab_type": "text"
      },
      "source": [
        "The second path, which is recommended, is to use an object-oriented approach and declare the variable as a private attribute of a class. Then, you need to make the objects that were instantiated callable by putting the function body inside the __call__ method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3a9l8lGPXqo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "d298d387-08d9-4845-e5de-dd50610213f1"
      },
      "source": [
        "class F():\n",
        "    def __init__(self):\n",
        "        self._b = None\n",
        "\n",
        "    @tf.function\n",
        "    def __call__(self):\n",
        "        a = tf.constant([[10, 10], [11., 1.]])\n",
        "        x = tf.constant([[1., 0.], [0., 1.]])\n",
        "        if self._b is None:\n",
        "            self._b = tf.Variable(12.)\n",
        "        y = tf.matmul(a, x) + self._b\n",
        "        return y\n",
        "\n",
        "f = F()\n",
        "f()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[22., 22.],\n",
              "       [23., 13.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKcpowNqPZdN",
        "colab_type": "text"
      },
      "source": [
        "AutoGraph and the graph acceleration process work best when it comes to optimizing the training process.\n",
        "\n",
        "In fact, the most computationally-intensive part of the training is the forward pass, followed by gradient computation and parameter updates. In the previous example, following the new structure that the absence of tf.Session allows us to follow, we separate the training step from the training loop. The training step is a function without a state that uses variables inherited from the outer scope. Therefore, it can be converted into its graph representation and accelerated just by decorating it with the @tf.function decorator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUQzvF3dPbUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inputs, labels):\n",
        "  # function body\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VsC-fwdPqp0",
        "colab_type": "text"
      },
      "source": [
        "You are invited to measure the speedup that was introduced by the graph conversion of the train_step function.\n",
        "\n",
        "The speedup is not guaranteed since eager execution is already fast and there are simple scenarios in which eager execution is as fast as its graphical counterpart. However, the performance boost is visible when the models become more complex and deeper.\n",
        "AutoGraph automatically converts Python constructs into their tf.* equivalent, but since converting source code that preserves semantics is not an easy task, there are scenarios in which it is better to help AutoGraph perform source code transformation.\n",
        "\n",
        "In fact, there are constructs that work in eager execution that are already drop-in replacements for Python constructs. In particular, tf.range replaces range, tf.print replaces print, and tf.assert replaces assert.\n",
        "\n",
        "For instance, AutoGraph is not able to automatically convert print into tf.print in order to preserve its semantic. Therefore, if we want a graph-accelerated function to print something when executed in graph mode, we have to write the function using tf.print instead of print.\n",
        "\n",
        "You are invited to define simple functions that use tf.range instead of  range and print instead of tf.print, and then visualize how the source code is converted using the tf.autograph module.\n",
        "\n",
        "For instance, take a look at the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwQUOHH3PuTA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "85ecb184-eeee-4cf5-a276-8e19f7acb0b4"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "@tf.function\n",
        "def f():\n",
        "    x = 0\n",
        "    for i in range(10):\n",
        "        print(i)\n",
        "        x += i\n",
        "    return x\n",
        "\n",
        "\n",
        "f()\n",
        "print(tf.autograph.to_code(f.python_function))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "def tf__f_1():\n",
            "  do_return = False\n",
            "  retval_ = ag__.UndefinedReturnValue()\n",
            "  with ag__.FunctionScope('f', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
            "    x = 0\n",
            "\n",
            "    def get_state():\n",
            "      return ()\n",
            "\n",
            "    def set_state(_):\n",
            "      pass\n",
            "\n",
            "    def loop_body(iterates, x):\n",
            "      i = iterates\n",
            "      print(i)\n",
            "      x += i\n",
            "      return x,\n",
            "    x, = ag__.for_stmt(ag__.converted_call(range, (10,), None, fscope), None, loop_body, get_state, set_state, (x,), ('x',), ())\n",
            "    do_return = True\n",
            "    retval_ = fscope.mark_return_value(x)\n",
            "  do_return,\n",
            "  return ag__.retval(retval_)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DWvfFWxPw5b",
        "colab_type": "text"
      },
      "source": [
        "This produces 0,1,2, ..., 10 when f is called—does this happens every time f is invoked, or only the first time?\n",
        "\n",
        "You are invited to carefully read through the following AutoGraph-generated function (this is machine-generated, and so it is hard to read) in order to understand why f behaves in this way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E0txXnEPzBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf__f():\n",
        "  try:\n",
        "    with ag__.function_scope('f'):\n",
        "      do_return = False\n",
        "      retval_ = None\n",
        "      x = 0\n",
        "\n",
        "      def loop_body(loop_vars, x_1):\n",
        "        with ag__.function_scope('loop_body'):\n",
        "          i = loop_vars\n",
        "          with ag__.utils.control_dependency_on_returns(ag__.print_(i)):\n",
        "            x, i_1 = ag__.utils.alias_tensors(x_1, i)\n",
        "            x += i_1\n",
        "            return x,\n",
        "      x, = ag__.for_stmt(ag__.range_(10), None, loop_body, (x,))\n",
        "      do_return = True\n",
        "      retval_ = x\n",
        "      return retval_\n",
        "  except:\n",
        "    ag__.rewrite_graph_construction_error(ag_source_map__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWCm0C9PP3U9",
        "colab_type": "text"
      },
      "source": [
        "Migrating an old codebase from Tensorfow 1.x to 2.0 can be a time-consuming process. This is why the TensorFlow authors created a conversion tool that allows us to automatically migrate the source code (it even works on Python notebooks!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4C_KBJiRwq1",
        "colab_type": "text"
      },
      "source": [
        "#  The tf.data.Dataset object\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl1ziY2rRTYi",
        "colab_type": "code",
        "outputId": "9212fb86-5889-43c8-c4e0-8dd59d15b4a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices({\n",
        "    \"a\":\n",
        "    tf.random.uniform([4]),\n",
        "    \"b\":\n",
        "    tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)\n",
        "})\n",
        "for value in dataset:\n",
        "    print(value[\"a\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.38550448, shape=(), dtype=float32)\n",
            "tf.Tensor(0.07681441, shape=(), dtype=float32)\n",
            "tf.Tensor(0.63705075, shape=(), dtype=float32)\n",
            "tf.Tensor(0.65296555, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I48AQyQR3TT",
        "colab_type": "code",
        "outputId": "67173cd6-033e-4062-8718-d24e633f49b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "def noise():\n",
        "    while True:\n",
        "        yield tf.random.uniform((100,))\n",
        "\n",
        "\n",
        "dataset = tf.data.Dataset.from_generator(noise, (tf.float32))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "(32, 100)\n",
            "1\n",
            "(32, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GYBn_uNXcjg",
        "colab_type": "text"
      },
      "source": [
        "The only peculiarity of the from_generator method is the need to pass the type of the parameters (tf.float32, in this case) as the second parameter; this is required since to build a graph we need to know the type of the parameters in advance.\n",
        "\n",
        "Using method chaining, it is possible to create new dataset objects, transforming the one just built to get the data our machine learning model expects as input. For example, if we want to sum 10 to every component of the noise vector, shuffle the dataset content, and create batches of 32 vectors each, we can do so by calling just three methods:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By6lL01sRWd_",
        "colab_type": "code",
        "outputId": "bb8116e6-4028-4070-8548-954ab8e0a417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "buffer_size = 10\n",
        "batch_size = 32\n",
        "dataset = dataset.map(lambda x: x + 10).shuffle(buffer_size).batch(batch_size)\n",
        "for idx, noise in enumerate(dataset):\n",
        "    if idx == 2:\n",
        "        break\n",
        "    print(idx)\n",
        "    print(noise.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "(32, 32, 100)\n",
            "1\n",
            "(32, 32, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfmUkN8jXkbZ",
        "colab_type": "text"
      },
      "source": [
        "The map method is the most widely used method of the `tf.data.Dataset` object since it allows us to apply a function to every element of the input dataset, producing a new, transformed dataset.\n",
        "\n",
        "The `shuffle` method is used in every training pipeline since this transformation randomly shuffles the input dataset using a fixed-sized buffer; this means that the shuffled data first fetches the buffer_size element from its input, then shuffles them and produces the output.\n",
        "\n",
        "The `batch` method gathers the `batch_size` elements from its input and creates a batch as output. The only constraint of this transformation is that all elements of the batch must have the same shape.\n",
        "\n",
        "To train a model, it has to be fed with all the elements of the training set for multiple epochs. The `tf.data.Dataset` class offers the `repeat(num_epochs)` method to do this. Thus, the input data pipeline can be summarized as shown in the following diagram:\n",
        "\n",
        "![](https://learning.oreilly.com/library/view/hands-on-neural-networks/9781789615555/assets/640c3fdc-3c31-47b8-8380-1f321542659d.png)\n",
        "\n",
        "The diagram shows the typical data input pipeline: the transformation from raw data to data ready to be used by the model, just by chaining method calls. \n",
        "\n",
        "Prefetching and caching are optimization tips that are explained in the next section.\n",
        "\n",
        "Please note that until not a single word has been said about the concept of thread, synchronization, or remote filesystems.\n",
        "\n",
        "All this is hidden by the tf.data API:\n",
        "\n",
        "* The input paths (for example, when using the `tf.data.Dataset.list_files` method) can be remote. TensorFlow internally uses the `tf.io.gfile` package, which is a file input/output wrapper without thread locking. This module makes it possible to read from a local filesystem or a remote filesystem in the same way. For instance, it is possible to read from a Google Cloud Storage bucket by using its address in the `gs://bucket/` format, without the need to worry about authentication, remote requests, and all the boilerplate required to work with a remote filesystem.\n",
        "* Every transformation applied to the data is executed using all the CPU resources efficiently—a number of threads equal to the number of CPU cores are created together with the dataset object and are used to process the data sequentially and in parallel whenever parallel transformation is possible.\n",
        "* The synchronization among these threads is all managed by the `tf.data` API.\n",
        "\n",
        "All the transformations described by chaining method calls are executed by threads on the CPU that `tf.data.Dataset` instantiates to perform operations that can be executed in parallel automatically, which is a great performance boost.\n",
        "\n",
        "Furthermore, `tf.data.Dataset` is high-level enough to make invisible all the threads execution and synchronization, but the automated solution can be suboptimal: the target device could be not completely used, and it is up to the user to remove the bottlenecks to reach the 100% usage of the target devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miZybzL6Z5t6",
        "colab_type": "text"
      },
      "source": [
        "# Performance optimizations\n",
        "\n",
        "The `tf.data` API as shown so far describes a sequential data input pipeline that transforms the data from a raw to a useful format by applying transformations.\n",
        "\n",
        "All these operations are executed on the CPU while the target device (CPUs, TPUs, or, in general, the consumer) waits for the data. If the target device consumes the data faster than it is produced, there will be moments of 0% utilization of the target devices.\n",
        "\n",
        "In parallel programming, this problem has been solved by using prefetching.\n",
        "\n",
        "### Prefetching\n",
        "When the consumer is working, the producer shouldn't be idle but must work in the background to produce the data the consumer will need in the next iteration.\n",
        "\n",
        "The `tf.data` API offers the prefetch(n) method to apply a transformation that allows overlapping the work of the producer and the consumer. The best practice is adding prefetch(n) at the end of the input pipeline to overlap the transformation performed on the CPU with the computation done on the target.\n",
        "\n",
        "Choosing n is easy: n is the number of elements consumed by a training step, and since the vast majority of models are trained using batches of data, one batch per training step, then n=1.\n",
        "\n",
        "The process of reading from disks, especially if reading big files, reading from slow HDDs, or using remote filesystems can be time-consuming. Caching is often used to reduce this overhead.\n",
        "\n",
        "### Cache elements\n",
        "\n",
        "The cache transformation can be used to cache the data in memory, completely removing the accesses to the data sources. This can bring huge benefits when using remote filesystems, or when the reading process is slow. Caching data after the first epoch is only possible if the data can fit into memory.\n",
        "\n",
        "The cache method acts as a barrier in the transformation pipeline: everything executed before the cache method is executed only once, thus placing this transformation in the pipeline can bring immense benefits. In fact, it can be applied after a computationally intensive transformation or after any slow process to speed up everything that comes next.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x32ALCR-iEv6",
        "colab_type": "text"
      },
      "source": [
        "# Building your dataset\n",
        "\n",
        "The following example shows how to build a `tf.data.Dataset` object using the Fashion-MNIST dataset. This is the first complete example of a dataset that uses all the best practices described previously; please take the time to understand why the method chaining is performed in this way and where the performance optimizations have been applied.\n",
        "\n",
        "In the following code, we define the `train_dataset` function, which returns the `tf.data.Dataset` object ready to use:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bNybwW-gzuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow.keras.datasets import fashion_mnist \n",
        " \n",
        " \n",
        "def train_dataset(batch_size=32, num_epochs=1): \n",
        "    (train_x, train_y), (test_x, test_y) = fashion_mnist.load_data()\n",
        "    input_x, input_y = train_x, train_y \n",
        "\n",
        "    def scale_fn(image, label): \n",
        "        return (tf.image.convert_image_dtype(image, tf.float32) - 0.5) * 2.0, label \n",
        " \n",
        "    dataset = tf.data.Dataset.from_tensor_slices( \n",
        "        (tf.expand_dims(input_x, -1), tf.expand_dims(input_y, -1)) \n",
        "    ).map(scale_fn) \n",
        " \n",
        "    dataset = dataset.cache().repeat(num_epochs)\n",
        "    dataset = dataset.shuffle(batch_size)\n",
        " \n",
        "    return dataset.batch(batch_size).prefetch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FoOMturkWe3",
        "colab_type": "text"
      },
      "source": [
        "A training dataset, however, should contain augmented data in order to address the overfitting problem. Applying data augmentation on image data is straightforward using the TensorFlow `tf.image` package.\n",
        "\n",
        "# Data augmentation\n",
        "The ETL process defined so far only transforms the raw data, applying transformations that do not change the image content. Data augmentation, instead, requires to apply meaningful transformation the raw data with the aim of creating a bigger dataset and train, thus, a model more robust to these kinds of variations.\n",
        "\n",
        "Working with images, it is possible to use the whole API offered by the tf.image package to augment the dataset. The augmentation step consists in the definition of a function and its application to the training set, using the dataset map method.\n",
        "\n",
        "The set of valid transformations depends on the dataset—if we were using the MNIST dataset, for instance, flipping the input image upside down won't be a good idea (nobody wants to feed an image of the number 6 labeled as 9), but since we are using the fashion-MNIST dataset we can flip and rotate the input image as we like (a pair of trousers remains a pair of trousers, even if randomly flipped or rotated).\n",
        "\n",
        "The tf.image package already contains functions with stochastic behavior, designed for data augmentation. These functions apply the transformation to the input image with a 50% chance; this is the desired behavior since we want to feed the model with both original and augmented images. Thus, a function that applies meaningful transformations to the input data can be defined as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FOr082TkFXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augment(image):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEdtQWn1kh0W",
        "colab_type": "code",
        "outputId": "bbd20abe-de31-4ce9-9c23-2f44958f8166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652,
          "referenced_widgets": [
            "a532b79a2df848ff9085819636d62980",
            "82936392274b4802a51e583d68defa16",
            "6950d327f99d4ab7aca8e9d1bd349712",
            "b46ed4ac9f79408ca47aa798aad78a0c",
            "8a34716ef2d84f14837e185c18c91041",
            "c0b93c08b2914ec9928fb6b79e48cd6b",
            "814c6695dc1e4133b20dac56d27f8215",
            "3304cadd97794077b8571e2e752410a5"
          ]
        }
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# See available datasets\n",
        "print(tfds.list_builders())\n",
        "# Construct 2 tf.data.Dataset objects\n",
        "# The training dataset and the test dataset\n",
        "ds_train, ds_test = tfds.load(name=\"mnist\", split=[\"train\", \"test\"])\n",
        "builder = tfds.builder(\"mnist\")\n",
        "print(builder.info)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['abstract_reasoning', 'aeslc', 'aflw2k3d', 'amazon_us_reviews', 'bair_robot_pushing_small', 'big_patent', 'bigearthnet', 'billsum', 'binarized_mnist', 'binary_alpha_digits', 'c4', 'caltech101', 'caltech_birds2010', 'caltech_birds2011', 'cars196', 'cassava', 'cats_vs_dogs', 'celeb_a', 'celeb_a_hq', 'chexpert', 'cifar10', 'cifar100', 'cifar10_1', 'cifar10_corrupted', 'citrus_leaves', 'clevr', 'cmaterdb', 'cnn_dailymail', 'coco', 'coil100', 'colorectal_histology', 'colorectal_histology_large', 'curated_breast_imaging_ddsm', 'cycle_gan', 'deep_weeds', 'definite_pronoun_resolution', 'diabetic_retinopathy_detection', 'dmlab', 'downsampled_imagenet', 'dsprites', 'dtd', 'duke_ultrasound', 'dummy_dataset_shared_generator', 'dummy_mnist', 'emnist', 'esnli', 'eurosat', 'fashion_mnist', 'flores', 'food101', 'gap', 'gigaword', 'glue', 'groove', 'higgs', 'horses_or_humans', 'i_naturalist2017', 'image_label_folder', 'imagenet2012', 'imagenet2012_corrupted', 'imagenet_resized', 'imdb_reviews', 'iris', 'kitti', 'kmnist', 'lfw', 'lm1b', 'lost_and_found', 'lsun', 'malaria', 'math_dataset', 'mnist', 'mnist_corrupted', 'moving_mnist', 'multi_news', 'multi_nli', 'multi_nli_mismatch', 'newsroom', 'nsynth', 'omniglot', 'open_images_v4', 'oxford_flowers102', 'oxford_iiit_pet', 'para_crawl', 'patch_camelyon', 'pet_finder', 'places365_small', 'plant_leaves', 'plant_village', 'plantae_k', 'quickdraw_bitmap', 'reddit_tifu', 'resisc45', 'rock_paper_scissors', 'rock_you', 'scene_parse150', 'scicite', 'scientific_papers', 'shapes3d', 'smallnorb', 'snli', 'so2sat', 'squad', 'stanford_dogs', 'stanford_online_products', 'starcraft_video', 'sun397', 'super_glue', 'svhn_cropped', 'ted_hrlr_translate', 'ted_multi_translate', 'tf_flowers', 'the300w_lp', 'titanic', 'trivia_qa', 'uc_merced', 'ucf101', 'visual_domain_decathlon', 'voc', 'wider_face', 'wikihow', 'wikipedia', 'wmt14_translate', 'wmt15_translate', 'wmt16_translate', 'wmt17_translate', 'wmt18_translate', 'wmt19_translate', 'wmt_t2t_translate', 'wmt_translate', 'xnli', 'xsum']\n",
            "\u001b[1mDownloading and preparing dataset mnist (11.06 MiB) to /root/tensorflow_datasets/mnist/1.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead set\n",
            "data_dir=gs://tfds-data/datasets.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a532b79a2df848ff9085819636d62980",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Dl Completed...', max=19, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "tfds.core.DatasetInfo(\n",
            "    name='mnist',\n",
            "    version=1.0.0,\n",
            "    description='The MNIST database of handwritten digits.',\n",
            "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
            "    }),\n",
            "    total_num_examples=70000,\n",
            "    splits={\n",
            "        'test': 10000,\n",
            "        'train': 60000,\n",
            "    },\n",
            "    supervised_keys=('image', 'label'),\n",
            "    citation=\"\"\"@article{lecun2010mnist,\n",
            "      title={MNIST handwritten digit database},\n",
            "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
            "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
            "      volume={2},\n",
            "      year={2010}\n",
            "    }\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCz7BN7BlWVZ",
        "colab_type": "text"
      },
      "source": [
        "# Eager integration\n",
        "\n",
        "The tf.data.Dataset object is iterable, which means one can either enumerate its elements using a for loop or create a Python iterator using the iter keyword. Please note that being iterable does not imply being a Python iterator as pointed out at the beginning of this chapter.\n",
        "\n",
        "Iterating over a dataset object is extremely easy: we can use the standard Python for loop to extract a batch at each iteration.\n",
        "\n",
        "Configuring the input pipeline by using a dataset object is a better solution than the one used so far.\n",
        "\n",
        "The manual process of extracting elements from a dataset by computing the indices is error-prone and inefficient, while the tf.data.Dataset objects are highly-optimized. Moreover, the dataset objects are fully compatible with tf.function, and therefore the whole training loop can be graph-converted and accelerated.\n",
        "\n",
        "Furthermore, the lines of code get reduced a lot, increasing the readability. The following code block represents the graph-accelerated (via @tf.function) custom training loop from the previous chapter, Chapter 4, TensorFlow 2.0 Architecture; the loop uses the train_dataset function defined previously:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxqONRbNlAcx",
        "colab_type": "code",
        "outputId": "b9e5b6ac-9cca-418a-efa0-0ba60e0289d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "\n",
        "def train_dataset(batch_size=32, num_epochs=1):\n",
        "    (train_x, train_y), (test_x, test_y) = fashion_mnist.load_data()\n",
        "    input_x, input_y = train_x, train_y\n",
        "\n",
        "    def scale_fn(image, label):\n",
        "        return (\n",
        "            tf.image.convert_image_dtype(image, tf.float32) - 0.5) * 2.0, label\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((tf.expand_dims(\n",
        "        input_x, -1), tf.expand_dims(input_y, -1))).map(scale_fn)\n",
        "\n",
        "    dataset = dataset.cache().repeat(num_epochs)\n",
        "    dataset = dataset.shuffle(batch_size)\n",
        "\n",
        "    return dataset.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def make_model(n_classes):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(\n",
        "            32, (5, 5), activation=tf.nn.relu, input_shape=(28, 28, 1)),\n",
        "        tf.keras.layers.MaxPool2D((2, 2), (2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
        "        tf.keras.layers.MaxPool2D((2, 2), (2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(n_classes)\n",
        "    ])\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Define the model\n",
        "    n_classes = 10\n",
        "    model = make_model(n_classes)\n",
        "\n",
        "    # Input data\n",
        "    dataset = train_dataset(num_epochs=10)\n",
        "\n",
        "    # Training parameters\n",
        "    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    step = tf.Variable(1, name=\"global_step\")\n",
        "    optimizer = tf.optimizers.Adam(1e-3)\n",
        "    accuracy = tf.metrics.Accuracy()\n",
        "\n",
        "    # Train step function\n",
        "    @tf.function\n",
        "    def train_step(inputs, labels):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(inputs)\n",
        "            loss_value = loss(labels, logits)\n",
        "\n",
        "        gradients = tape.gradient(loss_value, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        step.assign_add(1)\n",
        "\n",
        "        accuracy_value = accuracy(labels, tf.argmax(logits, -1))\n",
        "        return loss_value, accuracy_value\n",
        "\n",
        "    @tf.function\n",
        "    def loop():\n",
        "        for features, labels in dataset:\n",
        "            loss_value, accuracy_value = train_step(features, labels)\n",
        "            if tf.equal(tf.math.floormod(step, 10), 0):\n",
        "                tf.print(step, \": \", loss_value, \" - accuracy: \",\n",
        "                         accuracy_value)\n",
        "\n",
        "    loop()\n",
        "\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 :  1.4840436  - accuracy:  0.350694448\n",
            "20 :  0.793415368  - accuracy:  0.508223712\n",
            "30 :  0.767462611  - accuracy:  0.571120679\n",
            "40 :  0.835911274  - accuracy:  0.607371807\n",
            "50 :  0.308491945  - accuracy:  0.638392866\n",
            "60 :  0.847064912  - accuracy:  0.661546588\n",
            "70 :  0.513512  - accuracy:  0.677536249\n",
            "80 :  0.64157629  - accuracy:  0.689477861\n",
            "90 :  0.480841696  - accuracy:  0.695926964\n",
            "100 :  0.244961098  - accuracy:  0.70612371\n",
            "110 :  0.510952771  - accuracy:  0.711295843\n",
            "120 :  0.373629093  - accuracy:  0.720063031\n",
            "130 :  0.224795282  - accuracy:  0.727470934\n",
            "140 :  0.895719826  - accuracy:  0.735611498\n",
            "150 :  0.501918197  - accuracy:  0.744966447\n",
            "160 :  0.755409122  - accuracy:  0.748820782\n",
            "170 :  0.620160401  - accuracy:  0.748520732\n",
            "180 :  0.562458873  - accuracy:  0.751396656\n",
            "190 :  0.525883555  - accuracy:  0.755787\n",
            "200 :  0.483723462  - accuracy:  0.758951\n",
            "210 :  0.513407707  - accuracy:  0.762410283\n",
            "220 :  0.576628566  - accuracy:  0.765268266\n",
            "230 :  0.510008097  - accuracy:  0.768422484\n",
            "240 :  0.577234745  - accuracy:  0.770920515\n",
            "250 :  0.696275532  - accuracy:  0.772966862\n",
            "260 :  0.456604332  - accuracy:  0.774855196\n",
            "270 :  0.603316307  - accuracy:  0.775557637\n",
            "280 :  0.725667596  - accuracy:  0.776657701\n",
            "290 :  0.430484116  - accuracy:  0.778979242\n",
            "300 :  0.480357587  - accuracy:  0.7806229\n",
            "310 :  0.478825748  - accuracy:  0.781047761\n",
            "320 :  0.384993851  - accuracy:  0.784874618\n",
            "330 :  0.364844501  - accuracy:  0.786854088\n",
            "340 :  0.392728895  - accuracy:  0.789362073\n",
            "350 :  0.416669428  - accuracy:  0.791547298\n",
            "360 :  0.416496217  - accuracy:  0.792653203\n",
            "370 :  0.637631059  - accuracy:  0.793106377\n",
            "380 :  0.617004216  - accuracy:  0.793123364\n",
            "390 :  0.479615062  - accuracy:  0.794424832\n",
            "400 :  0.349443525  - accuracy:  0.795661032\n",
            "410 :  0.377082288  - accuracy:  0.797906458\n",
            "420 :  0.338190913  - accuracy:  0.798553109\n",
            "430 :  0.226166517  - accuracy:  0.799242437\n",
            "440 :  0.419349343  - accuracy:  0.799829185\n",
            "450 :  0.25218606  - accuracy:  0.800459325\n",
            "460 :  0.722154  - accuracy:  0.801742911\n",
            "470 :  0.415120631  - accuracy:  0.803304911\n",
            "480 :  0.529638469  - accuracy:  0.803888321\n",
            "490 :  0.324116915  - accuracy:  0.805406451\n",
            "500 :  0.163186908  - accuracy:  0.806550622\n",
            "510 :  0.387740791  - accuracy:  0.807465613\n",
            "520 :  0.370328814  - accuracy:  0.808285177\n",
            "530 :  0.241958782  - accuracy:  0.808542073\n",
            "540 :  0.650492787  - accuracy:  0.809427202\n",
            "550 :  0.324517161  - accuracy:  0.809369326\n",
            "560 :  0.246180654  - accuracy:  0.80970484\n",
            "570 :  0.390949756  - accuracy:  0.809753954\n",
            "580 :  0.293233067  - accuracy:  0.81034112\n",
            "590 :  0.436010301  - accuracy:  0.811332762\n",
            "600 :  0.450362206  - accuracy:  0.811613083\n",
            "610 :  0.42082262  - accuracy:  0.812192142\n",
            "620 :  0.198468268  - accuracy:  0.814014554\n",
            "630 :  0.142296806  - accuracy:  0.814586639\n",
            "640 :  0.643427074  - accuracy:  0.815043032\n",
            "650 :  0.379962713  - accuracy:  0.81562984\n",
            "660 :  0.248657852  - accuracy:  0.816341043\n",
            "670 :  0.353318304  - accuracy:  0.816890895\n",
            "680 :  0.397788703  - accuracy:  0.817792714\n",
            "690 :  0.178272367  - accuracy:  0.818486929\n",
            "700 :  0.454052478  - accuracy:  0.819027185\n",
            "710 :  0.305986345  - accuracy:  0.819992959\n",
            "720 :  0.4267869  - accuracy:  0.820453763\n",
            "730 :  0.230116665  - accuracy:  0.820730448\n",
            "740 :  0.517196059  - accuracy:  0.8212111\n",
            "750 :  0.40188989  - accuracy:  0.82197094\n",
            "760 :  0.752922058  - accuracy:  0.822587311\n",
            "770 :  0.469137579  - accuracy:  0.823268831\n",
            "780 :  0.229076117  - accuracy:  0.82397306\n",
            "790 :  0.21074748  - accuracy:  0.82505542\n",
            "800 :  0.281986505  - accuracy:  0.825836957\n",
            "810 :  0.204114825  - accuracy:  0.826097\n",
            "820 :  0.502401948  - accuracy:  0.826656\n",
            "830 :  0.242932498  - accuracy:  0.827012956\n",
            "840 :  0.439593703  - accuracy:  0.82780838\n",
            "850 :  0.554205596  - accuracy:  0.828364253\n",
            "860 :  0.184569  - accuracy:  0.828979909\n",
            "870 :  0.386089861  - accuracy:  0.829042\n",
            "880 :  0.216299444  - accuracy:  0.829671502\n",
            "890 :  0.210614532  - accuracy:  0.83007592\n",
            "900 :  0.295919895  - accuracy:  0.830749452\n",
            "910 :  0.131989151  - accuracy:  0.830961227\n",
            "920 :  0.640274704  - accuracy:  0.831440449\n",
            "930 :  0.456746757  - accuracy:  0.831505656\n",
            "940 :  0.297085047  - accuracy:  0.831902266\n",
            "950 :  0.149060935  - accuracy:  0.83229059\n",
            "960 :  0.209628701  - accuracy:  0.832866251\n",
            "970 :  0.246148095  - accuracy:  0.833010852\n",
            "980 :  0.386067808  - accuracy:  0.83321631\n",
            "990 :  0.326907039  - accuracy:  0.833670378\n",
            "1000 :  0.196784079  - accuracy:  0.834271789\n",
            "1010 :  0.20616807  - accuracy:  0.834427655\n",
            "1020 :  0.17932643  - accuracy:  0.835071146\n",
            "1030 :  0.254345655  - accuracy:  0.83573252\n",
            "1040 :  0.471597105  - accuracy:  0.836411238\n",
            "1050 :  0.690529704  - accuracy:  0.837017417\n",
            "1060 :  0.351477206  - accuracy:  0.837346554\n",
            "1070 :  0.270788103  - accuracy:  0.83790344\n",
            "1080 :  0.230776727  - accuracy:  0.838478923\n",
            "1090 :  0.264772207  - accuracy:  0.838785589\n",
            "1100 :  0.335869581  - accuracy:  0.83905822\n",
            "1110 :  0.262343377  - accuracy:  0.839410484\n",
            "1120 :  0.313949049  - accuracy:  0.83958894\n",
            "1130 :  0.303440511  - accuracy:  0.839930236\n",
            "1140 :  0.502021611  - accuracy:  0.840100944\n",
            "1150 :  0.26069653  - accuracy:  0.840268731\n",
            "1160 :  0.264595628  - accuracy:  0.840379655\n",
            "1170 :  0.383951485  - accuracy:  0.840702534\n",
            "1180 :  0.339038134  - accuracy:  0.841099441\n",
            "1190 :  0.3034361  - accuracy:  0.841515958\n",
            "1200 :  0.19177261  - accuracy:  0.842003763\n",
            "1210 :  0.528374672  - accuracy:  0.842224956\n",
            "1220 :  0.154627174  - accuracy:  0.842622042\n",
            "1230 :  0.385717422  - accuracy:  0.843038\n",
            "1240 :  0.460844338  - accuracy:  0.843422115\n",
            "1250 :  0.222766846  - accuracy:  0.843925118\n",
            "1260 :  0.296827912  - accuracy:  0.844023049\n",
            "1270 :  0.328378201  - accuracy:  0.8444888\n",
            "1280 :  0.297155052  - accuracy:  0.844702899\n",
            "1290 :  0.142864361  - accuracy:  0.845059156\n",
            "1300 :  0.268036604  - accuracy:  0.845289648\n",
            "1310 :  0.284211963  - accuracy:  0.845540464\n",
            "1320 :  0.294927478  - accuracy:  0.845669091\n",
            "1330 :  0.363215417  - accuracy:  0.845983803\n",
            "1340 :  0.175511196  - accuracy:  0.846503913\n",
            "1350 :  0.319873065  - accuracy:  0.846784651\n",
            "1360 :  0.314950168  - accuracy:  0.847107232\n",
            "1370 :  0.457857907  - accuracy:  0.847516418\n",
            "1380 :  0.500934124  - accuracy:  0.847851694\n",
            "1390 :  0.28689906  - accuracy:  0.847912192\n",
            "1400 :  0.34318611  - accuracy:  0.848284483\n",
            "1410 :  0.448608339  - accuracy:  0.848296642\n",
            "1420 :  0.356134415  - accuracy:  0.848506868\n",
            "1430 :  0.201528504  - accuracy:  0.848867238\n",
            "1440 :  0.165030584  - accuracy:  0.849005401\n",
            "1450 :  0.471098363  - accuracy:  0.849314153\n",
            "1460 :  0.149903744  - accuracy:  0.849554479\n",
            "1470 :  0.379898846  - accuracy:  0.849834085\n",
            "1480 :  0.262414932  - accuracy:  0.849814057\n",
            "1490 :  0.321241319  - accuracy:  0.849962234\n",
            "1500 :  0.375293523  - accuracy:  0.850421131\n",
            "1510 :  0.382021666  - accuracy:  0.850729\n",
            "1520 :  0.332968116  - accuracy:  0.85095048\n",
            "1530 :  0.263423443  - accuracy:  0.851230383\n",
            "1540 :  0.246674523  - accuracy:  0.851527\n",
            "1550 :  0.19763355  - accuracy:  0.851739049\n",
            "1560 :  0.416437894  - accuracy:  0.851928294\n",
            "1570 :  0.277205616  - accuracy:  0.851975799\n",
            "1580 :  0.287197173  - accuracy:  0.852200747\n",
            "1590 :  0.222434804  - accuracy:  0.852462232\n",
            "1600 :  0.432499349  - accuracy:  0.852720439\n",
            "1610 :  0.26950556  - accuracy:  0.85301429\n",
            "1620 :  0.343647093  - accuracy:  0.853227317\n",
            "1630 :  0.235064104  - accuracy:  0.853437662\n",
            "1640 :  0.263462067  - accuracy:  0.85362643\n",
            "1650 :  0.394983  - accuracy:  0.853737116\n",
            "1660 :  0.242234305  - accuracy:  0.853978276\n",
            "1670 :  0.234780252  - accuracy:  0.85410428\n",
            "1680 :  0.209792107  - accuracy:  0.854321778\n",
            "1690 :  0.150141776  - accuracy:  0.854462683\n",
            "1700 :  0.472824126  - accuracy:  0.854362845\n",
            "1710 :  0.170797244  - accuracy:  0.854520202\n",
            "1720 :  0.421310782  - accuracy:  0.854766607\n",
            "1730 :  0.480564922  - accuracy:  0.854901671\n",
            "1740 :  0.34283331  - accuracy:  0.855250835\n",
            "1750 :  0.304570794  - accuracy:  0.855417371\n",
            "1760 :  0.254306048  - accuracy:  0.855528712\n",
            "1770 :  0.506606758  - accuracy:  0.855568111\n",
            "1780 :  0.18287535  - accuracy:  0.855853\n",
            "1790 :  0.288208961  - accuracy:  0.855802834\n",
            "1800 :  0.230508298  - accuracy:  0.856065869\n",
            "1810 :  0.400508404  - accuracy:  0.856308758\n",
            "1820 :  0.387197435  - accuracy:  0.856428683\n",
            "1830 :  0.339336336  - accuracy:  0.856666923\n",
            "1840 :  0.267790794  - accuracy:  0.856732607\n",
            "1850 :  0.335806817  - accuracy:  0.856899\n",
            "1860 :  0.51318121  - accuracy:  0.857046783\n",
            "1870 :  0.1517822  - accuracy:  0.85705924\n",
            "1880 :  0.0828412846  - accuracy:  0.857404232\n",
            "1890 :  0.133095071  - accuracy:  0.857662797\n",
            "1900 :  0.360230088  - accuracy:  0.857852817\n",
            "1910 :  0.58329457  - accuracy:  0.858073592\n",
            "1920 :  0.307840347  - accuracy:  0.858161807\n",
            "1930 :  0.189475209  - accuracy:  0.858394921\n",
            "1940 :  0.228109598  - accuracy:  0.858593345\n",
            "1950 :  0.155416608  - accuracy:  0.858821809\n",
            "1960 :  0.353340089  - accuracy:  0.858968198\n",
            "1970 :  0.265844554  - accuracy:  0.858938575\n",
            "1980 :  0.288622469  - accuracy:  0.859003901\n",
            "1990 :  0.288045764  - accuracy:  0.859131455\n",
            "2000 :  0.471801549  - accuracy:  0.859304667\n",
            "2010 :  0.247744709  - accuracy:  0.859445\n",
            "2020 :  0.161356211  - accuracy:  0.85972327\n",
            "2030 :  0.209783942  - accuracy:  0.86009115\n",
            "2040 :  0.313391507  - accuracy:  0.860148966\n",
            "2050 :  0.317216426  - accuracy:  0.860190928\n",
            "2060 :  0.142451227  - accuracy:  0.860520899\n",
            "2070 :  0.143751204  - accuracy:  0.860862732\n",
            "2080 :  0.466106504  - accuracy:  0.860930741\n",
            "2090 :  0.215380594  - accuracy:  0.861162663\n",
            "2100 :  0.398963869  - accuracy:  0.861332774\n",
            "2110 :  0.139721781  - accuracy:  0.861486495\n",
            "2120 :  0.165427893  - accuracy:  0.861697733\n",
            "2130 :  0.27051869  - accuracy:  0.86181891\n",
            "2140 :  0.185496405  - accuracy:  0.862026632\n",
            "2150 :  0.16888988  - accuracy:  0.862130642\n",
            "2160 :  0.241782293  - accuracy:  0.862146854\n",
            "2170 :  0.32848841  - accuracy:  0.862393379\n",
            "2180 :  0.190045685  - accuracy:  0.862436891\n",
            "2190 :  0.196306199  - accuracy:  0.862608492\n",
            "2200 :  0.373222142  - accuracy:  0.862821162\n",
            "2210 :  0.148652613  - accuracy:  0.862975299\n",
            "2220 :  0.207429901  - accuracy:  0.863114\n",
            "2230 :  0.120258972  - accuracy:  0.863363624\n",
            "2240 :  0.460324168  - accuracy:  0.863499343\n",
            "2250 :  0.121164031  - accuracy:  0.863689423\n",
            "2260 :  0.365449131  - accuracy:  0.863753319\n",
            "2270 :  0.239658892  - accuracy:  0.863885522\n",
            "2280 :  0.287164152  - accuracy:  0.86405772\n",
            "2290 :  0.164826155  - accuracy:  0.864296615\n",
            "2300 :  0.389699221  - accuracy:  0.864288807\n",
            "2310 :  0.256238431  - accuracy:  0.86438936\n",
            "2320 :  0.222381264  - accuracy:  0.86451596\n",
            "2330 :  0.314622521  - accuracy:  0.864681721\n",
            "2340 :  0.495004177  - accuracy:  0.864779294\n",
            "2350 :  0.171216235  - accuracy:  0.864969134\n",
            "2360 :  0.306864917  - accuracy:  0.865130901\n",
            "2370 :  0.0778312385  - accuracy:  0.865291238\n",
            "2380 :  0.301413804  - accuracy:  0.865384638\n",
            "2390 :  0.200052679  - accuracy:  0.865516424\n",
            "2400 :  0.303156853  - accuracy:  0.865555942\n",
            "2410 :  0.407464921  - accuracy:  0.86566\n",
            "2420 :  0.135817692  - accuracy:  0.865672827\n",
            "2430 :  0.181311339  - accuracy:  0.865762651\n",
            "2440 :  0.389711529  - accuracy:  0.86585176\n",
            "2450 :  0.349932253  - accuracy:  0.865927398\n",
            "2460 :  0.200431466  - accuracy:  0.866104126\n",
            "2470 :  0.353534043  - accuracy:  0.866203427\n",
            "2480 :  0.370152354  - accuracy:  0.866264105\n",
            "2490 :  0.123326063  - accuracy:  0.86650008\n",
            "2500 :  0.189302042  - accuracy:  0.866696656\n",
            "2510 :  0.0928525  - accuracy:  0.866829395\n",
            "2520 :  0.164571047  - accuracy:  0.867047906\n",
            "2530 :  0.189367  - accuracy:  0.867091715\n",
            "2540 :  0.325097322  - accuracy:  0.867258251\n",
            "2550 :  0.322394818  - accuracy:  0.867411256\n",
            "2560 :  0.484413981  - accuracy:  0.867538571\n",
            "2570 :  0.429868519  - accuracy:  0.867640615\n",
            "2580 :  0.254140645  - accuracy:  0.867863\n",
            "2590 :  0.125906438  - accuracy:  0.868011296\n",
            "2600 :  0.400764287  - accuracy:  0.868062258\n",
            "2610 :  0.361033738  - accuracy:  0.868088841\n",
            "2620 :  0.214158848  - accuracy:  0.86816293\n",
            "2630 :  0.308166742  - accuracy:  0.86831969\n",
            "2640 :  0.383472323  - accuracy:  0.868522644\n",
            "2650 :  0.289390922  - accuracy:  0.868712246\n",
            "2660 :  0.0680907518  - accuracy:  0.868876934\n",
            "2670 :  0.454499662  - accuracy:  0.869087219\n",
            "2680 :  0.181574374  - accuracy:  0.869225919\n",
            "2690 :  0.347557545  - accuracy:  0.869235754\n",
            "2700 :  0.240902796  - accuracy:  0.869326591\n",
            "2710 :  0.12968564  - accuracy:  0.869555175\n",
            "2720 :  0.13573657  - accuracy:  0.869644165\n",
            "2730 :  0.130476788  - accuracy:  0.869743943\n",
            "2740 :  0.117092587  - accuracy:  0.869911492\n",
            "2750 :  0.0873647109  - accuracy:  0.870043635\n",
            "2760 :  0.291481078  - accuracy:  0.870152235\n",
            "2770 :  0.108015344  - accuracy:  0.870271325\n",
            "2780 :  0.333842784  - accuracy:  0.870378256\n",
            "2790 :  0.293249369  - accuracy:  0.870574117\n",
            "2800 :  0.38666141  - accuracy:  0.87062341\n",
            "2810 :  0.199043438  - accuracy:  0.87071687\n",
            "2820 :  0.390544474  - accuracy:  0.870809674\n",
            "2830 :  0.193963051  - accuracy:  0.87091285\n",
            "2840 :  0.232804  - accuracy:  0.87101531\n",
            "2850 :  0.272910684  - accuracy:  0.871095121\n",
            "2860 :  0.134519532  - accuracy:  0.871141553\n",
            "2870 :  0.114899278  - accuracy:  0.871372879\n",
            "2880 :  0.44778645  - accuracy:  0.871483147\n",
            "2890 :  0.189801604  - accuracy:  0.871646762\n",
            "2900 :  0.32930091  - accuracy:  0.871852338\n",
            "2910 :  0.192004323  - accuracy:  0.871970594\n",
            "2920 :  0.167901203  - accuracy:  0.872109473\n",
            "2930 :  0.286543965  - accuracy:  0.872204661\n",
            "2940 :  0.191584587  - accuracy:  0.872299254\n",
            "2950 :  0.281124681  - accuracy:  0.8724038\n",
            "2960 :  0.0652603135  - accuracy:  0.872581542\n",
            "2970 :  0.114314035  - accuracy:  0.87269491\n",
            "2980 :  0.262951165  - accuracy:  0.872786582\n",
            "2990 :  0.317269772  - accuracy:  0.872877657\n",
            "3000 :  0.0808004513  - accuracy:  0.873009741\n",
            "3010 :  0.34574  - accuracy:  0.87313062\n",
            "3020 :  0.218938947  - accuracy:  0.873136818\n",
            "3030 :  0.13282004  - accuracy:  0.873266757\n",
            "3040 :  0.254000425  - accuracy:  0.873334169\n",
            "3050 :  0.072140865  - accuracy:  0.873462617\n",
            "3060 :  0.24829787  - accuracy:  0.87363106\n",
            "3070 :  0.377195925  - accuracy:  0.873686433\n",
            "3080 :  0.534836709  - accuracy:  0.873924136\n",
            "3090 :  0.446793914  - accuracy:  0.874018669\n",
            "3100 :  0.175184458  - accuracy:  0.874233603\n",
            "3110 :  0.257308304  - accuracy:  0.87430644\n",
            "3120 :  0.422853529  - accuracy:  0.874408841\n",
            "3130 :  0.582484126  - accuracy:  0.874460697\n",
            "3140 :  0.0615575612  - accuracy:  0.874601781\n",
            "3150 :  0.22494027  - accuracy:  0.87476182\n",
            "3160 :  0.239034235  - accuracy:  0.87484175\n",
            "3170 :  0.110312432  - accuracy:  0.874911249\n",
            "3180 :  0.163152665  - accuracy:  0.875049174\n",
            "3190 :  0.649541378  - accuracy:  0.875098\n",
            "3200 :  0.258041561  - accuracy:  0.875175834\n",
            "3210 :  0.136991471  - accuracy:  0.875301898\n",
            "3220 :  0.223138839  - accuracy:  0.87548542\n",
            "3230 :  0.188693672  - accuracy:  0.87560004\n",
            "3240 :  0.152267441  - accuracy:  0.8758201\n",
            "3250 :  0.0551879928  - accuracy:  0.87596184\n",
            "3260 :  0.364050031  - accuracy:  0.876093149\n",
            "3270 :  0.305148184  - accuracy:  0.876128\n",
            "3280 :  0.491726458  - accuracy:  0.876277089\n",
            "3290 :  0.294227153  - accuracy:  0.876330197\n",
            "3300 :  0.197022036  - accuracy:  0.876487195\n",
            "3310 :  0.141293764  - accuracy:  0.876558244\n",
            "3320 :  0.231697023  - accuracy:  0.876675963\n",
            "3330 :  0.176864311  - accuracy:  0.876792967\n",
            "3340 :  0.2383288  - accuracy:  0.876909256\n",
            "3350 :  0.363736153  - accuracy:  0.876968861\n",
            "3360 :  0.283254057  - accuracy:  0.877065361\n",
            "3370 :  0.164452583  - accuracy:  0.877207637\n",
            "3380 :  0.14225027  - accuracy:  0.877358317\n",
            "3390 :  0.0691585094  - accuracy:  0.877517343\n",
            "3400 :  0.300655156  - accuracy:  0.877601862\n",
            "3410 :  0.166882396  - accuracy:  0.877713382\n",
            "3420 :  0.14951095  - accuracy:  0.877879143\n",
            "3430 :  0.571804345  - accuracy:  0.877916276\n",
            "3440 :  0.546009064  - accuracy:  0.877944171\n",
            "3450 :  0.227716953  - accuracy:  0.878035307\n",
            "3460 :  0.267685115  - accuracy:  0.878162\n",
            "3470 :  0.255796552  - accuracy:  0.87827903\n",
            "3480 :  0.160289183  - accuracy:  0.878404379\n",
            "3490 :  0.277557373  - accuracy:  0.878502071\n",
            "3500 :  0.135863364  - accuracy:  0.878572464\n",
            "3510 :  0.145350784  - accuracy:  0.878686965\n",
            "3520 :  0.412488401  - accuracy:  0.878712\n",
            "3530 :  0.303098738  - accuracy:  0.87879\n",
            "3540 :  0.094552733  - accuracy:  0.878867626\n",
            "3550 :  0.218393117  - accuracy:  0.878962398\n",
            "3560 :  0.232137293  - accuracy:  0.879056633\n",
            "3570 :  0.52150178  - accuracy:  0.879124045\n",
            "3580 :  0.186261982  - accuracy:  0.879182398\n",
            "3590 :  0.341859877  - accuracy:  0.879249096\n",
            "3600 :  0.358822435  - accuracy:  0.879298091\n",
            "3610 :  0.189606816  - accuracy:  0.879407406\n",
            "3620 :  0.556284547  - accuracy:  0.87953335\n",
            "3630 :  0.486759096  - accuracy:  0.879581153\n",
            "3640 :  0.13975206  - accuracy:  0.879748881\n",
            "3650 :  0.175101265  - accuracy:  0.879864335\n",
            "3660 :  0.252785593  - accuracy:  0.87989378\n",
            "3670 :  0.264927328  - accuracy:  0.879906\n",
            "3680 :  0.168508381  - accuracy:  0.880045533\n",
            "3690 :  0.25579682  - accuracy:  0.880150437\n",
            "3700 :  0.250802338  - accuracy:  0.880254805\n",
            "3710 :  0.139731646  - accuracy:  0.880367041\n",
            "3720 :  0.185664624  - accuracy:  0.880453408\n",
            "3730 :  0.526835144  - accuracy:  0.880497456\n",
            "3740 :  0.194884762  - accuracy:  0.880583048\n",
            "3750 :  0.215239897  - accuracy:  0.880643189\n",
            "3760 :  0.305140197  - accuracy:  0.880752861\n",
            "3770 :  0.236121669  - accuracy:  0.880837083\n",
            "3780 :  0.270392716  - accuracy:  0.880945683\n",
            "3790 :  0.289891124  - accuracy:  0.880962968\n",
            "3800 :  0.167439863  - accuracy:  0.881054223\n",
            "3810 :  0.182636574  - accuracy:  0.881136775\n",
            "3820 :  0.396994621  - accuracy:  0.881227076\n",
            "3830 :  0.238427281  - accuracy:  0.881308734\n",
            "3840 :  0.356995761  - accuracy:  0.881381869\n",
            "3850 :  0.144302309  - accuracy:  0.881381512\n",
            "3860 :  0.206122458  - accuracy:  0.881413579\n",
            "3870 :  0.280193508  - accuracy:  0.881582797\n",
            "3880 :  0.263488173  - accuracy:  0.881622195\n",
            "3890 :  0.230100304  - accuracy:  0.881717682\n",
            "3900 :  0.0932659656  - accuracy:  0.881932855\n",
            "3910 :  0.174549699  - accuracy:  0.881995082\n",
            "3920 :  0.330412596  - accuracy:  0.882001162\n",
            "3930 :  0.0944957  - accuracy:  0.88212651\n",
            "3940 :  0.154130936  - accuracy:  0.882306755\n",
            "3950 :  0.0559158772  - accuracy:  0.88240695\n",
            "3960 :  0.248878598  - accuracy:  0.882467151\n",
            "3970 :  0.241715491  - accuracy:  0.882629454\n",
            "3980 :  0.220297322  - accuracy:  0.8827281\n",
            "3990 :  0.260715038  - accuracy:  0.882826209\n",
            "4000 :  0.11537303  - accuracy:  0.88293165\n",
            "4010 :  0.230402812  - accuracy:  0.88300544\n",
            "4020 :  0.310862541  - accuracy:  0.883148789\n",
            "4030 :  0.183708519  - accuracy:  0.883206129\n",
            "4040 :  0.0880747586  - accuracy:  0.883340538\n",
            "4050 :  0.262491882  - accuracy:  0.883443415\n",
            "4060 :  0.102316514  - accuracy:  0.883522749\n",
            "4070 :  0.200958714  - accuracy:  0.883678436\n",
            "4080 :  0.251874417  - accuracy:  0.88381803\n",
            "4090 :  0.258781284  - accuracy:  0.883926392\n",
            "4100 :  0.109835401  - accuracy:  0.884026587\n",
            "4110 :  0.0301914662  - accuracy:  0.884156704\n",
            "4120 :  0.251068354  - accuracy:  0.884255886\n",
            "4130 :  0.313370794  - accuracy:  0.884347\n",
            "4140 :  0.225191593  - accuracy:  0.884331942\n",
            "4150 :  0.307178736  - accuracy:  0.884407401\n",
            "4160 :  0.30525437  - accuracy:  0.884527504\n",
            "4170 :  0.540319085  - accuracy:  0.884594619\n",
            "4180 :  0.155328766  - accuracy:  0.884661376\n",
            "4190 :  0.297630429  - accuracy:  0.884727836\n",
            "4200 :  0.162253201  - accuracy:  0.884816349\n",
            "4210 :  0.261468887  - accuracy:  0.884948909\n",
            "4220 :  0.135948211  - accuracy:  0.885006785\n",
            "4230 :  0.224869847  - accuracy:  0.885079205\n",
            "4240 :  0.161365151  - accuracy:  0.885180771\n",
            "4250 :  0.18736726  - accuracy:  0.88527447\n",
            "4260 :  0.337665021  - accuracy:  0.885309041\n",
            "4270 :  0.3095043  - accuracy:  0.885358095\n",
            "4280 :  0.295618087  - accuracy:  0.885385036\n",
            "4290 :  0.20698002  - accuracy:  0.885411799\n",
            "4300 :  0.19014436  - accuracy:  0.88543123\n",
            "4310 :  0.240614831  - accuracy:  0.885544777\n",
            "4320 :  0.240806445  - accuracy:  0.885556579\n",
            "4330 :  0.230693549  - accuracy:  0.88563323\n",
            "4340 :  0.181237102  - accuracy:  0.885738373\n",
            "4350 :  0.21465373  - accuracy:  0.885799885\n",
            "4360 :  0.12826094  - accuracy:  0.885925651\n",
            "4370 :  0.236305445  - accuracy:  0.886072338\n",
            "4380 :  0.22806251  - accuracy:  0.886111259\n",
            "4390 :  0.0577564463  - accuracy:  0.886185646\n",
            "4400 :  0.189399615  - accuracy:  0.886273861\n",
            "4410 :  0.147827953  - accuracy:  0.88639003\n",
            "4420 :  0.31432426  - accuracy:  0.886427939\n",
            "4430 :  0.142178282  - accuracy:  0.886592627\n",
            "4440 :  0.18101728  - accuracy:  0.886658\n",
            "4450 :  0.0755223781  - accuracy:  0.886737168\n",
            "4460 :  0.0234958064  - accuracy:  0.886815965\n",
            "4470 :  0.185883433  - accuracy:  0.886838496\n",
            "4480 :  0.508752227  - accuracy:  0.88684\n",
            "4490 :  0.37698105  - accuracy:  0.886925\n",
            "4500 :  0.207293019  - accuracy:  0.887009621\n",
            "4510 :  0.259973288  - accuracy:  0.8870731\n",
            "4520 :  0.254975677  - accuracy:  0.887143195\n",
            "4530 :  0.145385966  - accuracy:  0.887247443\n",
            "4540 :  0.129969418  - accuracy:  0.887358189\n",
            "4550 :  0.23097223  - accuracy:  0.88744092\n",
            "4560 :  0.182241797  - accuracy:  0.887475312\n",
            "4570 :  0.21799691  - accuracy:  0.887564301\n",
            "4580 :  0.0543634295  - accuracy:  0.887646\n",
            "4590 :  0.166127041  - accuracy:  0.887747884\n",
            "4600 :  0.27557826  - accuracy:  0.887794912\n",
            "4610 :  0.0949932858  - accuracy:  0.887929857\n",
            "4620 :  0.100255191  - accuracy:  0.887983084\n",
            "4630 :  0.0711408705  - accuracy:  0.888042748\n",
            "4640 :  0.128908351  - accuracy:  0.888088763\n",
            "4650 :  0.101067178  - accuracy:  0.888188303\n",
            "4660 :  0.098310113  - accuracy:  0.888274\n",
            "4670 :  0.294790715  - accuracy:  0.888339341\n",
            "4680 :  0.276464224  - accuracy:  0.888390958\n",
            "4690 :  0.106139541  - accuracy:  0.888455689\n",
            "4700 :  0.357396305  - accuracy:  0.888486922\n",
            "4710 :  0.21485351  - accuracy:  0.888591\n",
            "4720 :  0.325026512  - accuracy:  0.888595283\n",
            "4730 :  0.472911328  - accuracy:  0.888626039\n",
            "4740 :  0.160644263  - accuracy:  0.888729155\n",
            "4750 :  0.104216598  - accuracy:  0.888858199\n",
            "4760 :  0.271956325  - accuracy:  0.888881564\n",
            "4770 :  0.120747358  - accuracy:  0.888983548\n",
            "4780 :  0.166006774  - accuracy:  0.889058888\n",
            "4790 :  0.0505412146  - accuracy:  0.88915354\n",
            "4800 :  0.247254714  - accuracy:  0.889241219\n",
            "4810 :  0.164893627  - accuracy:  0.889322102\n",
            "4820 :  0.113281451  - accuracy:  0.889402628\n",
            "4830 :  0.157952189  - accuracy:  0.889508724\n",
            "4840 :  0.247719079  - accuracy:  0.889620781\n",
            "4850 :  0.255187511  - accuracy:  0.889706612\n",
            "4860 :  0.28335017  - accuracy:  0.889792144\n",
            "4870 :  0.189541668  - accuracy:  0.889858\n",
            "4880 :  0.240067  - accuracy:  0.889942884\n",
            "4890 :  0.523416  - accuracy:  0.889969826\n",
            "4900 :  0.27024132  - accuracy:  0.890022218\n",
            "4910 :  0.222965062  - accuracy:  0.89008069\n",
            "4920 :  0.043670103  - accuracy:  0.890170753\n",
            "4930 :  0.235729039  - accuracy:  0.890273154\n",
            "4940 :  0.204613805  - accuracy:  0.890349746\n",
            "4950 :  0.0317888856  - accuracy:  0.89045769\n",
            "4960 :  0.11580354  - accuracy:  0.890546203\n",
            "4970 :  0.113508552  - accuracy:  0.890621841\n",
            "4980 :  0.308946848  - accuracy:  0.89070344\n",
            "4990 :  0.169823945  - accuracy:  0.890809774\n",
            "5000 :  0.145623118  - accuracy:  0.890890658\n",
            "5010 :  0.0785858259  - accuracy:  0.890902638\n",
            "5020 :  0.65387  - accuracy:  0.890989244\n",
            "5030 :  0.0848353803  - accuracy:  0.891056895\n",
            "5040 :  0.0948049873  - accuracy:  0.891149044\n",
            "5050 :  0.0501815379  - accuracy:  0.891240835\n",
            "5060 :  0.173853427  - accuracy:  0.891326129\n",
            "5070 :  0.139907479  - accuracy:  0.891355515\n",
            "5080 :  0.0446387492  - accuracy:  0.891471\n",
            "5090 :  0.0936896503  - accuracy:  0.891512334\n",
            "5100 :  0.118408829  - accuracy:  0.891633153\n",
            "5110 :  0.188829124  - accuracy:  0.891747415\n",
            "5120 :  0.0778093487  - accuracy:  0.89186728\n",
            "5130 :  0.184017539  - accuracy:  0.891974568\n",
            "5140 :  0.16826123  - accuracy:  0.892020583\n",
            "5150 :  0.137815237  - accuracy:  0.892133176\n",
            "5160 :  0.149573609  - accuracy:  0.892172635\n",
            "5170 :  0.0991728082  - accuracy:  0.892248273\n",
            "5180 :  0.26819095  - accuracy:  0.892299414\n",
            "5190 :  0.0872779191  - accuracy:  0.892344356\n",
            "5200 :  0.209508836  - accuracy:  0.892413199\n",
            "5210 :  0.181908086  - accuracy:  0.892499745\n",
            "5220 :  0.159373671  - accuracy:  0.892574\n",
            "5230 :  0.099830538  - accuracy:  0.892665923\n",
            "5240 :  0.0872813  - accuracy:  0.892721653\n",
            "5250 :  0.145616293  - accuracy:  0.892818868\n",
            "5260 :  0.0391796902  - accuracy:  0.892886\n",
            "5270 :  0.186066911  - accuracy:  0.892946959\n",
            "5280 :  0.149634272  - accuracy:  0.893025458\n",
            "5290 :  0.148423046  - accuracy:  0.893115401\n",
            "5300 :  0.13527444  - accuracy:  0.893187404\n",
            "5310 :  0.20726423  - accuracy:  0.89320612\n",
            "5320 :  0.173527598  - accuracy:  0.893230617\n",
            "5330 :  0.212494165  - accuracy:  0.893301964\n",
            "5340 :  0.192384511  - accuracy:  0.893414\n",
            "5350 :  0.209211  - accuracy:  0.893490613\n",
            "5360 :  0.320753396  - accuracy:  0.893584371\n",
            "5370 :  0.224284515  - accuracy:  0.893648744\n",
            "5380 :  0.116325647  - accuracy:  0.8937186\n",
            "5390 :  0.121515602  - accuracy:  0.893747687\n",
            "5400 :  0.153657764  - accuracy:  0.893770814\n",
            "5410 :  0.245553523  - accuracy:  0.893828571\n",
            "5420 :  0.218638271  - accuracy:  0.893868804\n",
            "5430 :  0.0785974  - accuracy:  0.893937647\n",
            "5440 :  0.0644336  - accuracy:  0.89400053\n",
            "5450 :  0.24074246  - accuracy:  0.894040167\n",
            "5460 :  0.147336885  - accuracy:  0.894085467\n",
            "5470 :  0.112758704  - accuracy:  0.89416486\n",
            "5480 :  0.374037236  - accuracy:  0.894198298\n",
            "5490 :  0.231757909  - accuracy:  0.894260108\n",
            "5500 :  0.10226807  - accuracy:  0.89432168\n",
            "5510 :  0.315970749  - accuracy:  0.894360363\n",
            "5520 :  0.404921621  - accuracy:  0.894432843\n",
            "5530 :  0.294747621  - accuracy:  0.894499481\n",
            "5540 :  0.223910943  - accuracy:  0.8945207\n",
            "5550 :  0.11445874  - accuracy:  0.894609392\n",
            "5560 :  0.182938784  - accuracy:  0.894697785\n",
            "5570 :  0.0807114244  - accuracy:  0.894774616\n",
            "5580 :  0.0497928448  - accuracy:  0.89484\n",
            "5590 :  0.250074148  - accuracy:  0.894899607\n",
            "5600 :  0.132528514  - accuracy:  0.894986808\n",
            "5610 :  0.272865474  - accuracy:  0.895018041\n",
            "5620 :  0.19491601  - accuracy:  0.895054698\n",
            "5630 :  0.141303957  - accuracy:  0.895141244\n",
            "5640 :  0.0820715502  - accuracy:  0.895221889\n",
            "5650 :  0.112156108  - accuracy:  0.895302296\n",
            "5660 :  0.557117  - accuracy:  0.895360291\n",
            "5670 :  0.175312102  - accuracy:  0.895396\n",
            "5680 :  0.146746725  - accuracy:  0.895492136\n",
            "5690 :  0.234676242  - accuracy:  0.895555\n",
            "5700 :  0.0837958306  - accuracy:  0.895590246\n",
            "5710 :  0.0688189566  - accuracy:  0.895630777\n",
            "5720 :  0.121820718  - accuracy:  0.895665765\n",
            "5730 :  0.197426975  - accuracy:  0.895667851\n",
            "5740 :  0.212264732  - accuracy:  0.895724416\n",
            "5750 :  0.231319457  - accuracy:  0.895797074\n",
            "5760 :  0.0831329897  - accuracy:  0.895869493\n",
            "5770 :  0.0207838491  - accuracy:  0.895995855\n",
            "5780 :  0.180436909  - accuracy:  0.89609468\n",
            "5790 :  0.0657329708  - accuracy:  0.896144629\n",
            "5800 :  0.186409384  - accuracy:  0.896189\n",
            "5810 :  0.0485857464  - accuracy:  0.896287\n",
            "5820 :  0.0228534974  - accuracy:  0.896384716\n",
            "5830 :  0.11114274  - accuracy:  0.896460593\n",
            "5840 :  0.275150597  - accuracy:  0.896514833\n",
            "5850 :  0.180617601  - accuracy:  0.896579564\n",
            "5860 :  0.299723923  - accuracy:  0.896654725\n",
            "5870 :  0.108936615  - accuracy:  0.896692395\n",
            "5880 :  0.286870658  - accuracy:  0.896745861\n",
            "5890 :  0.065122664  - accuracy:  0.896815062\n",
            "5900 :  0.321439177  - accuracy:  0.896905184\n",
            "5910 :  0.234169424  - accuracy:  0.896942139\n",
            "5920 :  0.331090152  - accuracy:  0.897042334\n",
            "5930 :  0.204020187  - accuracy:  0.897079\n",
            "5940 :  0.117059261  - accuracy:  0.897120714\n",
            "5950 :  0.166120499  - accuracy:  0.897209644\n",
            "5960 :  0.123586446  - accuracy:  0.897272\n",
            "5970 :  0.179891586  - accuracy:  0.897376\n",
            "5980 :  0.115073726  - accuracy:  0.897464037\n",
            "5990 :  0.170437649  - accuracy:  0.897520483\n",
            "6000 :  0.0252813138  - accuracy:  0.897613168\n",
            "6010 :  0.268211782  - accuracy:  0.897658706\n",
            "6020 :  0.0572142079  - accuracy:  0.897761285\n",
            "6030 :  0.0707012191  - accuracy:  0.897827148\n",
            "6040 :  0.129382044  - accuracy:  0.897887707\n",
            "6050 :  0.238130033  - accuracy:  0.897932529\n",
            "6060 :  0.100901738  - accuracy:  0.898039103\n",
            "6070 :  0.305237412  - accuracy:  0.898083508\n",
            "6080 :  0.130625829  - accuracy:  0.898143172\n",
            "6090 :  0.195384771  - accuracy:  0.898192465\n",
            "6100 :  0.148307279  - accuracy:  0.898251772\n",
            "6110 :  0.15129219  - accuracy:  0.898331344\n",
            "6120 :  0.128332138  - accuracy:  0.89839536\n",
            "6130 :  0.308587432  - accuracy:  0.898474455\n",
            "6140 :  0.0477939285  - accuracy:  0.898553312\n",
            "6150 :  0.310596824  - accuracy:  0.898575962\n",
            "6160 :  0.153065294  - accuracy:  0.898634136\n",
            "6170 :  0.1311225  - accuracy:  0.898646474\n",
            "6180 :  0.163923204  - accuracy:  0.898694158\n",
            "6190 :  0.110350855  - accuracy:  0.89871645\n",
            "6200 :  0.354838908  - accuracy:  0.898733675\n",
            "6210 :  0.161370978  - accuracy:  0.898806155\n",
            "6220 :  0.251415908  - accuracy:  0.898863375\n",
            "6230 :  0.375172317  - accuracy:  0.898920357\n",
            "6240 :  0.13288036  - accuracy:  0.899007261\n",
            "6250 :  0.045319777  - accuracy:  0.89908886\n",
            "6260 :  0.209470361  - accuracy:  0.899150252\n",
            "6270 :  0.131057501  - accuracy:  0.899231315\n",
            "6280 :  0.131867513  - accuracy:  0.899287283\n",
            "6290 :  0.199413538  - accuracy:  0.899377882\n",
            "6300 :  0.104272403  - accuracy:  0.899438381\n",
            "6310 :  0.159212932  - accuracy:  0.899508655\n",
            "6320 :  0.145700604  - accuracy:  0.899549\n",
            "6330 :  0.0540008619  - accuracy:  0.899643481\n",
            "6340 :  0.217241719  - accuracy:  0.89969337\n",
            "6350 :  0.356019437  - accuracy:  0.899693847\n",
            "6360 :  0.194090873  - accuracy:  0.899733663\n",
            "6370 :  0.148427069  - accuracy:  0.899778247\n",
            "6380 :  0.111755833  - accuracy:  0.899856925\n",
            "6390 :  0.110723078  - accuracy:  0.899930537\n",
            "6400 :  0.164889261  - accuracy:  0.90000391\n",
            "6410 :  0.031449452  - accuracy:  0.900077045\n",
            "6420 :  0.0699831769  - accuracy:  0.900188863\n",
            "6430 :  0.156603307  - accuracy:  0.90024209\n",
            "6440 :  0.218713939  - accuracy:  0.900299907\n",
            "6450 :  0.146763265  - accuracy:  0.900352776\n",
            "6460 :  0.0806949437  - accuracy:  0.900463521\n",
            "6470 :  0.221138299  - accuracy:  0.90052557\n",
            "6480 :  0.335162818  - accuracy:  0.900592327\n",
            "6490 :  0.190137058  - accuracy:  0.900639534\n",
            "6500 :  0.0328248702  - accuracy:  0.900720298\n",
            "6510 :  0.0412362814  - accuracy:  0.900791228\n",
            "6520 :  0.167936474  - accuracy:  0.900857091\n",
            "6530 :  0.0803989917  - accuracy:  0.900903642\n",
            "6540 :  0.0813275725  - accuracy:  0.900993109\n",
            "6550 :  0.0805239528  - accuracy:  0.901044071\n",
            "6560 :  0.0540277362  - accuracy:  0.901094854\n",
            "6570 :  0.0634456128  - accuracy:  0.901164532\n",
            "6580 :  0.140962452  - accuracy:  0.901219785\n",
            "6590 :  0.28495419  - accuracy:  0.901270092\n",
            "6600 :  0.0825260878  - accuracy:  0.90128237\n",
            "6610 :  0.140401796  - accuracy:  0.901304066\n",
            "6620 :  0.0974492654  - accuracy:  0.901382387\n",
            "6630 :  0.13001  - accuracy:  0.901432157\n",
            "6640 :  0.105496511  - accuracy:  0.901467681\n",
            "6650 :  0.212490171  - accuracy:  0.901554763\n",
            "6660 :  0.0743202344  - accuracy:  0.901646256\n",
            "6670 :  0.0894163549  - accuracy:  0.901728153\n",
            "6680 :  0.155906618  - accuracy:  0.901791036\n",
            "6690 :  0.0681209  - accuracy:  0.901863158\n",
            "6700 :  0.0802035779  - accuracy:  0.901935\n",
            "6710 :  0.0766071081  - accuracy:  0.902025282\n",
            "6720 :  0.0624869801  - accuracy:  0.902110636\n",
            "6730 :  0.0927654058  - accuracy:  0.902181804\n",
            "6740 :  0.190480947  - accuracy:  0.902220309\n",
            "6750 :  0.0705631  - accuracy:  0.902300358\n",
            "6760 :  0.216903359  - accuracy:  0.902357042\n",
            "6770 :  0.0854170471  - accuracy:  0.902372\n",
            "6780 :  0.0783904344  - accuracy:  0.902446866\n",
            "6790 :  0.21025154  - accuracy:  0.902498543\n",
            "6800 :  0.0771431625  - accuracy:  0.90255\n",
            "6810 :  0.189199209  - accuracy:  0.902628899\n",
            "6820 :  0.180280074  - accuracy:  0.902680039\n",
            "6830 :  0.0843176395  - accuracy:  0.902781308\n",
            "6840 :  0.222713232  - accuracy:  0.902804673\n",
            "6850 :  0.2150359  - accuracy:  0.902869046\n",
            "6860 :  0.0973561928  - accuracy:  0.90293777\n",
            "6870 :  0.0922760516  - accuracy:  0.903001726\n",
            "6880 :  0.12747325  - accuracy:  0.903051853\n",
            "6890 :  0.0904095918  - accuracy:  0.90312\n",
            "6900 :  0.243738383  - accuracy:  0.90319252\n",
            "6910 :  0.113584138  - accuracy:  0.903246701\n",
            "6920 :  0.0308602583  - accuracy:  0.903327823\n",
            "6930 :  0.256326377  - accuracy:  0.903426707\n",
            "6940 :  0.190385401  - accuracy:  0.903489351\n",
            "6950 :  0.0696264952  - accuracy:  0.903529286\n",
            "6960 :  0.121361859  - accuracy:  0.903564632\n",
            "6970 :  0.141335428  - accuracy:  0.903617799\n",
            "6980 :  0.149840787  - accuracy:  0.903670847\n",
            "6990 :  0.0166075  - accuracy:  0.903741598\n",
            "7000 :  0.0815321  - accuracy:  0.903821111\n",
            "7010 :  0.280260593  - accuracy:  0.903900325\n",
            "7020 :  0.106642693  - accuracy:  0.90397048\n",
            "7030 :  0.256231278  - accuracy:  0.904031515\n",
            "7040 :  0.121980235  - accuracy:  0.90408349\n",
            "7050 :  0.396388  - accuracy:  0.904144228\n",
            "7060 :  0.187886506  - accuracy:  0.904182613\n",
            "7070 :  0.225979805  - accuracy:  0.90422529\n",
            "7080 :  0.0755125061  - accuracy:  0.904303193\n",
            "7090 :  0.196157232  - accuracy:  0.904367685\n",
            "7100 :  0.259557217  - accuracy:  0.904431939\n",
            "7110 :  0.184932739  - accuracy:  0.904465318\n",
            "7120 :  0.0583265238  - accuracy:  0.904489756\n",
            "7130 :  0.126735941  - accuracy:  0.904549181\n",
            "7140 :  0.0502752811  - accuracy:  0.904582202\n",
            "7150 :  0.489388794  - accuracy:  0.904619515\n",
            "7160 :  0.119034901  - accuracy:  0.904669821\n",
            "7170 :  0.11771781  - accuracy:  0.904746115\n",
            "7180 :  0.317464203  - accuracy:  0.904756904\n",
            "7190 :  0.088221617  - accuracy:  0.904824197\n",
            "7200 :  0.0445284098  - accuracy:  0.904895663\n",
            "7210 :  0.11055211  - accuracy:  0.904936552\n",
            "7220 :  0.264690399  - accuracy:  0.904985964\n",
            "7230 :  0.0953915119  - accuracy:  0.905082822\n",
            "7240 :  0.058166  - accuracy:  0.905131936\n",
            "7250 :  0.191222295  - accuracy:  0.905202448\n",
            "7260 :  0.151718497  - accuracy:  0.905251265\n",
            "7270 :  0.348276228  - accuracy:  0.90526551\n",
            "7280 :  0.253984123  - accuracy:  0.905301213\n",
            "7290 :  0.361390948  - accuracy:  0.905341089\n",
            "7300 :  0.138343856  - accuracy:  0.905406535\n",
            "7310 :  0.146673918  - accuracy:  0.905454755\n",
            "7320 :  0.160337627  - accuracy:  0.905494273\n",
            "7330 :  0.217162982  - accuracy:  0.905537903\n",
            "7340 :  0.24004662  - accuracy:  0.905615568\n",
            "7350 :  0.192377269  - accuracy:  0.905650437\n",
            "7360 :  0.0550569147  - accuracy:  0.905702174\n",
            "7370 :  0.179474771  - accuracy:  0.905770779\n",
            "7380 :  0.174411789  - accuracy:  0.905788362\n",
            "7390 :  0.26637131  - accuracy:  0.905827045\n",
            "7400 :  0.0823735222  - accuracy:  0.905903697\n",
            "7410 :  0.200185508  - accuracy:  0.905954778\n",
            "7420 :  0.106541827  - accuracy:  0.906001508\n",
            "7430 :  0.0845908821  - accuracy:  0.906085968\n",
            "7440 :  0.167052388  - accuracy:  0.906136572\n",
            "7450 :  0.152575806  - accuracy:  0.906199634\n",
            "7460 :  0.0341257043  - accuracy:  0.906254172\n",
            "7470 :  0.149840027  - accuracy:  0.906316936\n",
            "7480 :  0.095677413  - accuracy:  0.906371176\n",
            "7490 :  0.276129514  - accuracy:  0.90637517\n",
            "7500 :  0.0797676519  - accuracy:  0.906441689\n",
            "7510 :  0.17522  - accuracy:  0.906491399\n",
            "7520 :  0.0884396806  - accuracy:  0.906561732\n",
            "7530 :  0.237777457  - accuracy:  0.906598628\n",
            "7540 :  0.185533434  - accuracy:  0.906618893\n",
            "7550 :  0.101892754  - accuracy:  0.906684637\n",
            "7560 :  0.0743723139  - accuracy:  0.906737804\n",
            "7570 :  0.119601011  - accuracy:  0.906761944\n",
            "7580 :  0.142813206  - accuracy:  0.906790137\n",
            "7590 :  0.104720287  - accuracy:  0.906855345\n",
            "7600 :  0.0704803  - accuracy:  0.906866848\n",
            "7610 :  0.063894406  - accuracy:  0.90691942\n",
            "7620 :  0.306091905  - accuracy:  0.906996489\n",
            "7630 :  0.151239723  - accuracy:  0.90706104\n",
            "7640 :  0.0756955072  - accuracy:  0.907133639\n",
            "7650 :  0.12193194  - accuracy:  0.907230496\n",
            "7660 :  0.0959961414  - accuracy:  0.907290459\n",
            "7670 :  0.427915394  - accuracy:  0.907321692\n",
            "7680 :  0.0744207054  - accuracy:  0.907393515\n",
            "7690 :  0.26846838  - accuracy:  0.907465219\n",
            "7700 :  0.0876875371  - accuracy:  0.907532632\n",
            "7710 :  0.20513387  - accuracy:  0.907567441\n",
            "7720 :  0.19005236  - accuracy:  0.907630503\n",
            "7730 :  0.121807761  - accuracy:  0.907677233\n",
            "7740 :  0.146128431  - accuracy:  0.907715797\n",
            "7750 :  0.0949419  - accuracy:  0.907782435\n",
            "7760 :  0.14800249  - accuracy:  0.907816708\n",
            "7770 :  0.0576094  - accuracy:  0.907871\n",
            "7780 :  0.152214512  - accuracy:  0.907917142\n",
            "7790 :  0.14172174  - accuracy:  0.907995224\n",
            "7800 :  0.125416487  - accuracy:  0.908045113\n",
            "7810 :  0.062134482  - accuracy:  0.908082843\n",
            "7820 :  0.0608152449  - accuracy:  0.908144414\n",
            "7830 :  0.291240394  - accuracy:  0.908201873\n",
            "7840 :  0.134620443  - accuracy:  0.908251226\n",
            "7850 :  0.111348584  - accuracy:  0.908316374\n",
            "7860 :  0.100334831  - accuracy:  0.908385277\n",
            "7870 :  0.0795281827  - accuracy:  0.908446133\n",
            "7880 :  0.0616145283  - accuracy:  0.908479035\n",
            "7890 :  0.150929958  - accuracy:  0.908492\n",
            "7900 :  0.0941073149  - accuracy:  0.908564389\n",
            "7910 :  0.053923212  - accuracy:  0.908628643\n",
            "7920 :  0.48102057  - accuracy:  0.908657193\n",
            "7930 :  0.243593  - accuracy:  0.908693552\n",
            "7940 :  0.306248844  - accuracy:  0.908749521\n",
            "7950 :  0.126321495  - accuracy:  0.908789635\n",
            "7960 :  0.120194986  - accuracy:  0.908845305\n",
            "7970 :  0.0174429566  - accuracy:  0.908885241\n",
            "7980 :  0.0963133201  - accuracy:  0.908948481\n",
            "7990 :  0.0874611735  - accuracy:  0.908996\n",
            "8000 :  0.0335826203  - accuracy:  0.909055054\n",
            "8010 :  0.378294826  - accuracy:  0.909098387\n",
            "8020 :  0.111701131  - accuracy:  0.909161031\n",
            "8030 :  0.0866323113  - accuracy:  0.909188569\n",
            "8040 :  0.072089687  - accuracy:  0.909251\n",
            "8050 :  0.130259  - accuracy:  0.909301639\n",
            "8060 :  0.0907023549  - accuracy:  0.909367621\n",
            "8070 :  0.0358515307  - accuracy:  0.909387\n",
            "8080 :  0.0928304791  - accuracy:  0.909437299\n",
            "8090 :  0.254432142  - accuracy:  0.909491301\n",
            "8100 :  0.195104361  - accuracy:  0.909537435\n",
            "8110 :  0.0826730579  - accuracy:  0.909579635\n",
            "8120 :  0.15939872  - accuracy:  0.90966022\n",
            "8130 :  0.0793912485  - accuracy:  0.909709811\n",
            "8140 :  0.0815226957  - accuracy:  0.909759343\n",
            "8150 :  0.114096612  - accuracy:  0.909801066\n",
            "8160 :  0.235633388  - accuracy:  0.909858\n",
            "8170 :  0.0530978367  - accuracy:  0.909910917\n",
            "8180 :  0.129400074  - accuracy:  0.909979045\n",
            "8190 :  0.072378695  - accuracy:  0.910024107\n",
            "8200 :  0.107872076  - accuracy:  0.910061419\n",
            "8210 :  0.173479483  - accuracy:  0.910106301\n",
            "8220 :  0.198211506  - accuracy:  0.910120606\n",
            "8230 :  0.102929205  - accuracy:  0.910142481\n",
            "8240 :  0.277140439  - accuracy:  0.91019845\n",
            "8250 :  0.106306754  - accuracy:  0.910265625\n",
            "8260 :  0.0877128243  - accuracy:  0.910313725\n",
            "8270 :  0.150572628  - accuracy:  0.910350382\n",
            "8280 :  0.103906125  - accuracy:  0.910417199\n",
            "8290 :  0.0983440578  - accuracy:  0.910498857\n",
            "8300 :  0.090041481  - accuracy:  0.910565257\n",
            "8310 :  0.0240234658  - accuracy:  0.910601437\n",
            "8320 :  0.227510363  - accuracy:  0.910667598\n",
            "8330 :  0.175900564  - accuracy:  0.910707295\n",
            "8340 :  0.03721039  - accuracy:  0.910773158\n",
            "8350 :  0.166505203  - accuracy:  0.910816431\n",
            "8360 :  0.10138835  - accuracy:  0.910900652\n",
            "8370 :  0.0359083675  - accuracy:  0.910943687\n",
            "8380 :  0.123128906  - accuracy:  0.910997748\n",
            "8390 :  0.0630109757  - accuracy:  0.911055386\n",
            "8400 :  0.28857547  - accuracy:  0.91112411\n",
            "8410 :  0.126829281  - accuracy:  0.911170304\n",
            "8420 :  0.0692254454  - accuracy:  0.91123873\n",
            "8430 :  0.0858670622  - accuracy:  0.911281\n",
            "8440 :  0.118403859  - accuracy:  0.911334276\n",
            "8450 :  0.227850705  - accuracy:  0.911383748\n",
            "8460 :  0.0864556134  - accuracy:  0.911447883\n",
            "8470 :  0.166862443  - accuracy:  0.911478639\n",
            "8480 :  0.257508039  - accuracy:  0.911494553\n",
            "8490 :  0.0613723025  - accuracy:  0.911554635\n",
            "8500 :  0.0949846059  - accuracy:  0.91160357\n",
            "8510 :  0.0830342695  - accuracy:  0.911623\n",
            "8520 :  0.124675609  - accuracy:  0.911690056\n",
            "8530 :  0.0984277  - accuracy:  0.911727607\n",
            "8540 :  0.0740549192  - accuracy:  0.911783457\n",
            "8550 :  0.117270976  - accuracy:  0.911820829\n",
            "8560 :  0.114301965  - accuracy:  0.911861777\n",
            "8570 :  0.219032794  - accuracy:  0.911906302\n",
            "8580 :  0.0232033543  - accuracy:  0.911968887\n",
            "8590 :  0.0735477656  - accuracy:  0.912045956\n",
            "8600 :  0.0314643495  - accuracy:  0.912097335\n",
            "8610 :  0.0580673739  - accuracy:  0.912155867\n",
            "8620 :  0.147730455  - accuracy:  0.912214279\n",
            "8630 :  0.157906145  - accuracy:  0.912276208\n",
            "8640 :  0.217375159  - accuracy:  0.912323475\n",
            "8650 :  0.111141026  - accuracy:  0.912377894\n",
            "8660 :  0.139640898  - accuracy:  0.912428558\n",
            "8670 :  0.0343629234  - accuracy:  0.912468255\n",
            "8680 :  0.0588140301  - accuracy:  0.912540317\n",
            "8690 :  0.0244161878  - accuracy:  0.912594199\n",
            "8700 :  0.0746827051  - accuracy:  0.91264081\n",
            "8710 :  0.117650896  - accuracy:  0.912701666\n",
            "8720 :  0.0289666653  - accuracy:  0.91273725\n",
            "8730 :  0.0947333723  - accuracy:  0.91278\n",
            "8740 :  0.11057236  - accuracy:  0.912829697\n",
            "8750 :  0.061985895  - accuracy:  0.91289717\n",
            "8760 :  0.135636881  - accuracy:  0.912950277\n",
            "8770 :  0.0397429  - accuracy:  0.912988961\n",
            "8780 :  0.0762447715  - accuracy:  0.91304177\n",
            "8790 :  0.0495473444  - accuracy:  0.913087368\n",
            "8800 :  0.0252700448  - accuracy:  0.913157761\n",
            "8810 :  0.0885445  - accuracy:  0.913196\n",
            "8820 :  0.0298453197  - accuracy:  0.913227141\n",
            "8830 :  0.0425090045  - accuracy:  0.91329354\n",
            "8840 :  0.0149744181  - accuracy:  0.913335085\n",
            "8850 :  0.112172499  - accuracy:  0.913397729\n",
            "8860 :  0.0968878567  - accuracy:  0.913453102\n",
            "8870 :  0.00935298577  - accuracy:  0.913529575\n",
            "8880 :  0.0142797623  - accuracy:  0.913591743\n",
            "8890 :  0.152029917  - accuracy:  0.913639724\n",
            "8900 :  0.106128544  - accuracy:  0.91371572\n",
            "8910 :  0.115010276  - accuracy:  0.91374594\n",
            "8920 :  0.00677162316  - accuracy:  0.913811088\n",
            "8930 :  0.0827997476  - accuracy:  0.913851619\n",
            "8940 :  0.0873559639  - accuracy:  0.91389209\n",
            "8950 :  0.0840429  - accuracy:  0.913949907\n",
            "8960 :  0.0792643577  - accuracy:  0.913997114\n",
            "8970 :  0.0205080658  - accuracy:  0.914054632\n",
            "8980 :  0.253084481  - accuracy:  0.914091229\n",
            "8990 :  0.0371540748  - accuracy:  0.914138138\n",
            "9000 :  0.0424326956  - accuracy:  0.914174497\n",
            "9010 :  0.0564795285  - accuracy:  0.914200366\n",
            "9020 :  0.0707638785  - accuracy:  0.91425395\n",
            "9030 :  0.0149412956  - accuracy:  0.914293528\n",
            "9040 :  0.15975216  - accuracy:  0.914329588\n",
            "9050 :  0.170560032  - accuracy:  0.91436553\n",
            "9060 :  0.177374318  - accuracy:  0.914380729\n",
            "9070 :  0.0736598819  - accuracy:  0.914413095\n",
            "9080 :  0.0111324526  - accuracy:  0.914452314\n",
            "9090 :  0.284564435  - accuracy:  0.914501727\n",
            "9100 :  0.112323575  - accuracy:  0.914551079\n",
            "9110 :  0.219485387  - accuracy:  0.91457969\n",
            "9120 :  0.00543025043  - accuracy:  0.914611638\n",
            "9130 :  0.172581971  - accuracy:  0.914647\n",
            "9140 :  0.129614204  - accuracy:  0.914648056\n",
            "9150 :  0.192706078  - accuracy:  0.914710641\n",
            "9160 :  0.213260278  - accuracy:  0.914752543\n",
            "9170 :  0.0440191776  - accuracy:  0.914797843\n",
            "9180 :  0.0256659724  - accuracy:  0.914856613\n",
            "9190 :  0.0647524744  - accuracy:  0.914908469\n",
            "9200 :  0.0465575531  - accuracy:  0.914946616\n",
            "9210 :  0.132642701  - accuracy:  0.914998233\n",
            "9220 :  0.185135916  - accuracy:  0.915056527\n",
            "9230 :  0.068841964  - accuracy:  0.915101171\n",
            "9240 :  0.225000694  - accuracy:  0.91515249\n",
            "9250 :  0.0713116154  - accuracy:  0.915203691\n",
            "9260 :  0.218127862  - accuracy:  0.915241241\n",
            "9270 :  0.138294548  - accuracy:  0.915299\n",
            "9280 :  0.109285429  - accuracy:  0.915356576\n",
            "9290 :  0.0467177816  - accuracy:  0.915400624\n",
            "9300 :  0.0365636274  - accuracy:  0.915468037\n",
            "9310 :  0.0328436494  - accuracy:  0.915538728\n",
            "9320 :  0.0214354843  - accuracy:  0.915599167\n",
            "9330 :  0.0601905212  - accuracy:  0.9156394\n",
            "9340 :  0.23343797  - accuracy:  0.915679514\n",
            "9350 :  0.0252281297  - accuracy:  0.915736318\n",
            "9360 :  0.160350263  - accuracy:  0.915772915\n",
            "9370 :  0.0744210705  - accuracy:  0.915779412\n",
            "9380 :  0.0611983687  - accuracy:  0.915839255\n",
            "9390 :  0.0544314384  - accuracy:  0.915885627\n",
            "9400 :  0.128018588  - accuracy:  0.915925205\n",
            "9410 :  0.105965108  - accuracy:  0.915974736\n",
            "9420 :  0.071878776  - accuracy:  0.916020811\n",
            "9430 :  0.0318115167  - accuracy:  0.916070104\n",
            "9440 :  0.17004469  - accuracy:  0.916119277\n",
            "9450 :  0.24963969  - accuracy:  0.916132\n",
            "9460 :  0.0942758918  - accuracy:  0.916167796\n",
            "9470 :  0.160305053  - accuracy:  0.916200221\n",
            "9480 :  0.0255537182  - accuracy:  0.916259\n",
            "9490 :  0.120782398  - accuracy:  0.916317582\n",
            "9500 :  0.0243292563  - accuracy:  0.91636622\n",
            "9510 :  0.10300269  - accuracy:  0.916401565\n",
            "9520 :  0.0905264  - accuracy:  0.916469693\n",
            "9530 :  0.0869184434  - accuracy:  0.916527808\n",
            "9540 :  0.111614436  - accuracy:  0.916579306\n",
            "9550 :  0.0738744512  - accuracy:  0.916617572\n",
            "9560 :  0.026594704  - accuracy:  0.916685164\n",
            "9570 :  0.0819717348  - accuracy:  0.916736364\n",
            "9580 :  0.105571255  - accuracy:  0.916774333\n",
            "9590 :  0.0212456174  - accuracy:  0.916812241\n",
            "9600 :  0.0413766019  - accuracy:  0.916872859\n",
            "9610 :  0.204571098  - accuracy:  0.916923583\n",
            "9620 :  0.0186002366  - accuracy:  0.916964471\n",
            "9630 :  0.16768764  - accuracy:  0.917008519\n",
            "9640 :  0.0158618297  - accuracy:  0.917046\n",
            "9650 :  0.296281636  - accuracy:  0.917102814\n",
            "9660 :  0.188366115  - accuracy:  0.917149782\n",
            "9670 :  0.0484522209  - accuracy:  0.917206407\n",
            "9680 :  0.0142077859  - accuracy:  0.917237043\n",
            "9690 :  0.0394040868  - accuracy:  0.917274117\n",
            "9700 :  0.130000293  - accuracy:  0.917333603\n",
            "9710 :  0.0454114862  - accuracy:  0.917380154\n",
            "9720 :  0.0257085785  - accuracy:  0.917436182\n",
            "9730 :  0.13950184  - accuracy:  0.917482555\n",
            "9740 :  0.0789968222  - accuracy:  0.917538404\n",
            "9750 :  0.0352427252  - accuracy:  0.917590916\n",
            "9760 :  0.209613785  - accuracy:  0.917608082\n",
            "9770 :  0.0161246061  - accuracy:  0.917660475\n",
            "9780 :  0.0531185381  - accuracy:  0.917703092\n",
            "9790 :  0.0535106175  - accuracy:  0.917764843\n",
            "9800 :  0.200740799  - accuracy:  0.917791367\n",
            "9810 :  0.0424564295  - accuracy:  0.917852879\n",
            "9820 :  0.225847751  - accuracy:  0.917901516\n",
            "9830 :  0.131315321  - accuracy:  0.917953253\n",
            "9840 :  0.162160754  - accuracy:  0.917979479\n",
            "9850 :  0.0466253497  - accuracy:  0.918027818\n",
            "9860 :  0.191531971  - accuracy:  0.918082476\n",
            "9870 :  0.235895053  - accuracy:  0.918121159\n",
            "9880 :  0.178162277  - accuracy:  0.918159723\n",
            "9890 :  0.213302985  - accuracy:  0.918207705\n",
            "9900 :  0.207368612  - accuracy:  0.918220878\n",
            "9910 :  0.103342265  - accuracy:  0.918265581\n",
            "9920 :  0.0324072726  - accuracy:  0.918313324\n",
            "9930 :  0.106703073  - accuracy:  0.918367267\n",
            "9940 :  0.179199606  - accuracy:  0.918411672\n",
            "9950 :  0.0345768742  - accuracy:  0.918462276\n",
            "9960 :  0.192169428  - accuracy:  0.918493927\n",
            "9970 :  0.21581623  - accuracy:  0.918534935\n",
            "9980 :  0.115212679  - accuracy:  0.918569624\n",
            "9990 :  0.0389290377  - accuracy:  0.918607354\n",
            "10000 :  0.0208426118  - accuracy:  0.918644965\n",
            "10010 :  0.186787248  - accuracy:  0.918679416\n",
            "10020 :  0.228383899  - accuracy:  0.918707609\n",
            "10030 :  0.121796928  - accuracy:  0.918741882\n",
            "10040 :  0.0871993303  - accuracy:  0.918785512\n",
            "10050 :  0.205079019  - accuracy:  0.918813467\n",
            "10060 :  0.16309306  - accuracy:  0.918869317\n",
            "10070 :  0.0540966094  - accuracy:  0.918915749\n",
            "10080 :  0.296195269  - accuracy:  0.918983757\n",
            "10090 :  0.0763443485  - accuracy:  0.919002116\n",
            "10100 :  0.0579175241  - accuracy:  0.91904211\n",
            "10110 :  0.0758400857  - accuracy:  0.919085085\n",
            "10120 :  0.0199113432  - accuracy:  0.919121802\n",
            "10130 :  0.0151185524  - accuracy:  0.919155419\n",
            "10140 :  0.0770293549  - accuracy:  0.919185817\n",
            "10150 :  0.0418206379  - accuracy:  0.919231594\n",
            "10160 :  0.0994652584  - accuracy:  0.919271111\n",
            "10170 :  0.0610039756  - accuracy:  0.919322848\n",
            "10180 :  0.24026078  - accuracy:  0.919362187\n",
            "10190 :  0.0606505089  - accuracy:  0.919392228\n",
            "10200 :  0.0699870437  - accuracy:  0.919419169\n",
            "10210 :  0.161037251  - accuracy:  0.919470549\n",
            "10220 :  0.307123065  - accuracy:  0.919524908\n",
            "10230 :  0.0642855167  - accuracy:  0.919582188\n",
            "10240 :  0.138126194  - accuracy:  0.919618\n",
            "10250 :  0.127694502  - accuracy:  0.919650674\n",
            "10260 :  0.111155093  - accuracy:  0.919713795\n",
            "10270 :  0.0237767063  - accuracy:  0.919764578\n",
            "10280 :  0.0441328511  - accuracy:  0.919794\n",
            "10290 :  0.134493306  - accuracy:  0.919847667\n",
            "10300 :  0.100963622  - accuracy:  0.919889092\n",
            "10310 :  0.0203328542  - accuracy:  0.919918299\n",
            "10320 :  0.152376562  - accuracy:  0.919959545\n",
            "10330 :  0.0466597788  - accuracy:  0.920009792\n",
            "10340 :  0.0677091852  - accuracy:  0.920066\n",
            "10350 :  0.158176601  - accuracy:  0.920079827\n",
            "10360 :  0.0694333389  - accuracy:  0.920099676\n",
            "10370 :  0.238746539  - accuracy:  0.920131505\n",
            "10380 :  0.0403848365  - accuracy:  0.920166314\n",
            "10390 :  0.0921633244  - accuracy:  0.920219123\n",
            "10400 :  0.0586484931  - accuracy:  0.920283794\n",
            "10410 :  0.116863847  - accuracy:  0.920321345\n",
            "10420 :  0.0812909529  - accuracy:  0.920367837\n",
            "10430 :  0.0853926092  - accuracy:  0.920405269\n",
            "10440 :  0.117955588  - accuracy:  0.920448542\n",
            "10450 :  0.0511946753  - accuracy:  0.920491815\n",
            "10460 :  0.033959873  - accuracy:  0.920528948\n",
            "10470 :  0.0927775204  - accuracy:  0.920572042\n",
            "10480 :  0.00630605035  - accuracy:  0.920618\n",
            "10490 :  0.0767376199  - accuracy:  0.920672834\n",
            "10500 :  0.0385028981  - accuracy:  0.920730531\n",
            "10510 :  0.0647417456  - accuracy:  0.920764327\n",
            "10520 :  0.0406574793  - accuracy:  0.920792162\n",
            "10530 :  0.00736139622  - accuracy:  0.920840681\n",
            "10540 :  0.0150022293  - accuracy:  0.920886099\n",
            "10550 :  0.0719439462  - accuracy:  0.920922577\n",
            "10560 :  0.0465841629  - accuracy:  0.920962\n",
            "10570 :  0.0610121675  - accuracy:  0.921010137\n",
            "10580 :  0.114824526  - accuracy:  0.921061158\n",
            "10590 :  0.261809856  - accuracy:  0.921094418\n",
            "10600 :  0.0946927592  - accuracy:  0.92114526\n",
            "10610 :  0.147885472  - accuracy:  0.921181321\n",
            "10620 :  0.0992643908  - accuracy:  0.921217263\n",
            "10630 :  0.156373501  - accuracy:  0.921247303\n",
            "10640 :  0.0451699682  - accuracy:  0.921289\n",
            "10650 :  0.14911142  - accuracy:  0.921336532\n",
            "10660 :  0.0739193857  - accuracy:  0.921389759\n",
            "10670 :  0.0304177087  - accuracy:  0.921434164\n",
            "10680 :  0.00938885659  - accuracy:  0.921490192\n",
            "10690 :  0.199603066  - accuracy:  0.921531498\n",
            "10700 :  0.112170175  - accuracy:  0.921563923\n",
            "10710 :  0.0062061809  - accuracy:  0.921608\n",
            "10720 :  0.109362252  - accuracy:  0.921646118\n",
            "10730 :  0.0247137398  - accuracy:  0.921710432\n",
            "10740 :  0.0690696836  - accuracy:  0.921757162\n",
            "10750 :  0.10851258  - accuracy:  0.921812475\n",
            "10760 :  0.140790731  - accuracy:  0.921856105\n",
            "10770 :  0.186994091  - accuracy:  0.921899676\n",
            "10780 :  0.257481337  - accuracy:  0.921940207\n",
            "10790 :  0.0981517658  - accuracy:  0.921989381\n",
            "10800 :  0.0767867789  - accuracy:  0.922038496\n",
            "10810 :  0.0180267375  - accuracy:  0.922073066\n",
            "10820 :  0.111084044  - accuracy:  0.92211616\n",
            "10830 :  0.034696456  - accuracy:  0.922147691\n",
            "10840 :  0.0337859  - accuracy:  0.922202229\n",
            "10850 :  0.0309435632  - accuracy:  0.922245145\n",
            "10860 :  0.219968498  - accuracy:  0.922276437\n",
            "10870 :  0.0257454924  - accuracy:  0.922322094\n",
            "10880 :  0.0707942247  - accuracy:  0.92236191\n",
            "10890 :  0.141367942  - accuracy:  0.922398746\n",
            "10900 :  0.127366856  - accuracy:  0.922409713\n",
            "10910 :  0.0366808921  - accuracy:  0.922463655\n",
            "10920 :  0.246017978  - accuracy:  0.922500372\n",
            "10930 :  0.0205774438  - accuracy:  0.922536969\n",
            "10940 :  0.135519266  - accuracy:  0.922573447\n",
            "10950 :  0.0536446869  - accuracy:  0.922618508\n",
            "10960 :  0.113373511  - accuracy:  0.922660589\n",
            "10970 :  0.161992326  - accuracy:  0.922685504\n",
            "10980 :  0.0658617243  - accuracy:  0.922736\n",
            "10990 :  0.124978915  - accuracy:  0.922760844\n",
            "11000 :  0.118263192  - accuracy:  0.922791302\n",
            "11010 :  0.264413595  - accuracy:  0.922827363\n",
            "11020 :  0.0399694592  - accuracy:  0.922863364\n",
            "11030 :  0.0271163955  - accuracy:  0.922904968\n",
            "11040 :  0.322701335  - accuracy:  0.922935188\n",
            "11050 :  0.0508161895  - accuracy:  0.922973812\n",
            "11060 :  0.071512267  - accuracy:  0.923012376\n",
            "11070 :  0.223141342  - accuracy:  0.923048\n",
            "11080 :  0.106030703  - accuracy:  0.923086464\n",
            "11090 :  0.0215805843  - accuracy:  0.923130453\n",
            "11100 :  0.209318161  - accuracy:  0.923165917\n",
            "11110 :  0.0602129139  - accuracy:  0.923212588\n",
            "11120 :  0.0679024756  - accuracy:  0.923245132\n",
            "11130 :  0.03755638  - accuracy:  0.923288822\n",
            "11140 :  0.0737020522  - accuracy:  0.923335254\n",
            "11150 :  0.0965214819  - accuracy:  0.923384368\n",
            "11160 :  0.0473781675  - accuracy:  0.923433423\n",
            "11170 :  0.0425677933  - accuracy:  0.923471212\n",
            "11180 :  0.0227364264  - accuracy:  0.923508942\n",
            "11190 :  0.145396471  - accuracy:  0.923546553\n",
            "11200 :  0.104755051  - accuracy:  0.923606455\n",
            "11210 :  0.0105120204  - accuracy:  0.923646748\n",
            "11220 :  0.201175839  - accuracy:  0.923681378\n",
            "11230 :  0.249710709  - accuracy:  0.923704803\n",
            "11240 :  0.175403088  - accuracy:  0.923719883\n",
            "11250 :  0.30759728  - accuracy:  0.923765421\n",
            "11260 :  0.0887589082  - accuracy:  0.923797071\n",
            "11270 :  0.0957414061  - accuracy:  0.923834205\n",
            "11280 :  0.0999979228  - accuracy:  0.923876762\n",
            "11290 :  0.168415636  - accuracy:  0.92390269\n",
            "11300 :  0.00385583658  - accuracy:  0.923950672\n",
            "11310 :  0.0685307384  - accuracy:  0.923982\n",
            "11320 :  0.0650153831  - accuracy:  0.924021542\n",
            "11330 :  0.0560101345  - accuracy:  0.924047232\n",
            "11340 :  0.162956297  - accuracy:  0.924089432\n",
            "11350 :  0.031931743  - accuracy:  0.924120545\n",
            "11360 :  0.0349531323  - accuracy:  0.924157083\n",
            "11370 :  0.217374012  - accuracy:  0.924199045\n",
            "11380 :  0.0327964835  - accuracy:  0.924240947\n",
            "11390 :  0.0437543914  - accuracy:  0.92428\n",
            "11400 :  0.222853437  - accuracy:  0.924332738\n",
            "11410 :  0.0437228382  - accuracy:  0.924374402\n",
            "11420 :  0.0659028739  - accuracy:  0.924399555\n",
            "11430 :  0.0482399315  - accuracy:  0.92445755\n",
            "11440 :  0.151276976  - accuracy:  0.924504459\n",
            "11450 :  0.0456463844  - accuracy:  0.924548566\n",
            "11460 :  0.116279975  - accuracy:  0.924573481\n",
            "11470 :  0.103222728  - accuracy:  0.924625635\n",
            "11480 :  0.0241702199  - accuracy:  0.924672246\n",
            "11490 :  0.111392729  - accuracy:  0.9246943\n",
            "11500 :  0.0451044366  - accuracy:  0.92473805\n",
            "11510 :  0.0490113795  - accuracy:  0.924757242\n",
            "11520 :  0.0140177244  - accuracy:  0.924811721\n",
            "11530 :  0.112243176  - accuracy:  0.924847126\n",
            "11540 :  0.0247625448  - accuracy:  0.924877048\n",
            "11550 :  0.107807741  - accuracy:  0.924909651\n",
            "11560 :  0.131878883  - accuracy:  0.924942136\n",
            "11570 :  0.0354149118  - accuracy:  0.924985409\n",
            "11580 :  0.286037296  - accuracy:  0.925015092\n",
            "11590 :  0.0697023571  - accuracy:  0.925066352\n",
            "11600 :  0.0954147279  - accuracy:  0.925109386\n",
            "11610 :  0.0498008057  - accuracy:  0.925149679\n",
            "11620 :  0.0318588726  - accuracy:  0.925189912\n",
            "11630 :  0.0948325545  - accuracy:  0.925224662\n",
            "11640 :  0.142807886  - accuracy:  0.925235212\n",
            "11650 :  0.103210837  - accuracy:  0.925261796\n",
            "11660 :  0.193584412  - accuracy:  0.925293744\n",
            "11670 :  0.0335365534  - accuracy:  0.925328314\n",
            "11680 :  0.0136768501  - accuracy:  0.925365508\n",
            "11690 :  0.126867533  - accuracy:  0.925405324\n",
            "11700 :  0.104055777  - accuracy:  0.925445\n",
            "11710 :  0.166192174  - accuracy:  0.925479352\n",
            "11720 :  0.0183066931  - accuracy:  0.92551893\n",
            "11730 :  0.181721881  - accuracy:  0.925563753\n",
            "11740 :  0.0558078587  - accuracy:  0.925595224\n",
            "11750 :  0.124775745  - accuracy:  0.925639927\n",
            "11760 :  0.0313395038  - accuracy:  0.925673962\n",
            "11770 :  0.0427761748  - accuracy:  0.925702572\n",
            "11780 :  0.0404220596  - accuracy:  0.925736487\n",
            "11790 :  0.157133102  - accuracy:  0.925743818\n",
            "11800 :  0.00811999664  - accuracy:  0.925764382\n",
            "11810 :  0.0404529572  - accuracy:  0.925800741\n",
            "11820 :  0.240036696  - accuracy:  0.92580539\n",
            "11830 :  0.0635858551  - accuracy:  0.925831139\n",
            "11840 :  0.109933883  - accuracy:  0.925872624\n",
            "11850 :  0.100010782  - accuracy:  0.925895631\n",
            "11860 :  0.171859309  - accuracy:  0.925931811\n",
            "11870 :  0.0254923701  - accuracy:  0.925973117\n",
            "11880 :  0.0989311635  - accuracy:  0.926001251\n",
            "11890 :  0.0904059857  - accuracy:  0.926039815\n",
            "11900 :  0.0191383213  - accuracy:  0.92607832\n",
            "11910 :  0.18567732  - accuracy:  0.926119447\n",
            "11920 :  0.0542780831  - accuracy:  0.92615521\n",
            "11930 :  0.0266730152  - accuracy:  0.926193535\n",
            "11940 :  0.0321903862  - accuracy:  0.926213443\n",
            "11950 :  0.0332959183  - accuracy:  0.926243842\n",
            "11960 :  0.00915170368  - accuracy:  0.926266313\n",
            "11970 :  0.105699614  - accuracy:  0.926299214\n",
            "11980 :  0.267184585  - accuracy:  0.926316381\n",
            "11990 :  0.159899503  - accuracy:  0.926333487\n",
            "12000 :  0.0776679516  - accuracy:  0.926368892\n",
            "12010 :  0.0606746078  - accuracy:  0.926398933\n",
            "12020 :  0.133319288  - accuracy:  0.926431596\n",
            "12030 :  0.0259523708  - accuracy:  0.926456392\n",
            "12040 :  0.0133095151  - accuracy:  0.926499307\n",
            "12050 :  0.100433595  - accuracy:  0.926537\n",
            "12060 :  0.0564029813  - accuracy:  0.926556408\n",
            "12070 :  0.084655188  - accuracy:  0.926588774\n",
            "12080 :  0.0293421764  - accuracy:  0.926621079\n",
            "12090 :  0.081019789  - accuracy:  0.926668882\n",
            "12100 :  0.0534740426  - accuracy:  0.926703632\n",
            "12110 :  0.0254222173  - accuracy:  0.926751256\n",
            "12120 :  0.0237569492  - accuracy:  0.926785946\n",
            "12130 :  0.00330789271  - accuracy:  0.926820517\n",
            "12140 :  0.0241328143  - accuracy:  0.92685765\n",
            "12150 :  0.0736308247  - accuracy:  0.926892102\n",
            "12160 :  0.0558116212  - accuracy:  0.926918864\n",
            "12170 :  0.0386437438  - accuracy:  0.92695576\n",
            "12180 :  0.247950882  - accuracy:  0.926992655\n",
            "12190 :  0.0983665  - accuracy:  0.92702949\n",
            "12200 :  0.225278676  - accuracy:  0.927053452\n",
            "12210 :  0.0149578061  - accuracy:  0.927097857\n",
            "12220 :  0.216727316  - accuracy:  0.927131951\n",
            "12230 :  0.0794780403  - accuracy:  0.927150607\n",
            "12240 :  0.137449294  - accuracy:  0.927174389\n",
            "12250 :  0.137037173  - accuracy:  0.927208364\n",
            "12260 :  0.0593350083  - accuracy:  0.927237153\n",
            "12270 :  0.0398421064  - accuracy:  0.927268445\n",
            "12280 :  0.0903020278  - accuracy:  0.927314937\n",
            "12290 :  0.0721146762  - accuracy:  0.92735374\n",
            "12300 :  0.139150694  - accuracy:  0.927384853\n",
            "12310 :  0.0509936959  - accuracy:  0.927421\n",
            "12320 :  0.0781483427  - accuracy:  0.927459598\n",
            "12330 :  0.0478318706  - accuracy:  0.927498162\n",
            "12340 :  0.0522828363  - accuracy:  0.927544296\n",
            "12350 :  0.0227128696  - accuracy:  0.927580178\n",
            "12360 :  0.100374028  - accuracy:  0.927621067\n",
            "12370 :  0.170995981  - accuracy:  0.927651763\n",
            "12380 :  0.0582831651  - accuracy:  0.927697599\n",
            "12390 :  0.0939270407  - accuracy:  0.9277156\n",
            "12400 :  0.0949419066  - accuracy:  0.92774117\n",
            "12410 :  0.0702170283  - accuracy:  0.927774191\n",
            "12420 :  0.0826628953  - accuracy:  0.927819788\n",
            "12430 :  0.149243712  - accuracy:  0.927855194\n",
            "12440 :  0.078250289  - accuracy:  0.927885592\n",
            "12450 :  0.071163252  - accuracy:  0.927920938\n",
            "12460 :  0.0108152293  - accuracy:  0.927958727\n",
            "12470 :  0.067936711  - accuracy:  0.927996457\n",
            "12480 :  0.036292579  - accuracy:  0.928026557\n",
            "12490 :  0.0529553331  - accuracy:  0.928056717\n",
            "12500 :  0.132569537  - accuracy:  0.928101718\n",
            "12510 :  0.021521477  - accuracy:  0.928141713\n",
            "12520 :  0.0287196748  - accuracy:  0.928164184\n",
            "12530 :  0.22729899  - accuracy:  0.928196609\n",
            "12540 :  0.0233852752  - accuracy:  0.928233922\n",
            "12550 :  0.00817023  - accuracy:  0.928273678\n",
            "12560 :  0.0496718884  - accuracy:  0.928320825\n",
            "12570 :  0.244027853  - accuracy:  0.928348\n",
            "12580 :  0.095051758  - accuracy:  0.928387582\n",
            "12590 :  0.0225236062  - accuracy:  0.928429604\n",
            "12600 :  0.0487533435  - accuracy:  0.928459108\n",
            "12610 :  0.074706316  - accuracy:  0.92850095\n",
            "12620 :  0.0111151356  - accuracy:  0.92854774\n",
            "12630 :  0.0353595912  - accuracy:  0.928591907\n",
            "12640 :  0.0278860088  - accuracy:  0.92862618\n",
            "12650 :  0.0202841163  - accuracy:  0.928672731\n",
            "12660 :  0.117847472  - accuracy:  0.928714275\n",
            "12670 :  0.00603206735  - accuracy:  0.928755701\n",
            "12680 :  0.0364381075  - accuracy:  0.928794682\n",
            "12690 :  0.0621108823  - accuracy:  0.928826153\n",
            "12700 :  0.010562175  - accuracy:  0.928860068\n",
            "12710 :  0.0510193482  - accuracy:  0.928893924\n",
            "12720 :  0.0561599731  - accuracy:  0.928930163\n",
            "12730 :  0.153207377  - accuracy:  0.928949177\n",
            "12740 :  0.0558938123  - accuracy:  0.928975463\n",
            "12750 :  0.0558046103  - accuracy:  0.929004252\n",
            "12760 :  0.0522940271  - accuracy:  0.929040313\n",
            "12770 :  0.119610921  - accuracy:  0.929078698\n",
            "12780 :  0.0033765058  - accuracy:  0.929112196\n",
            "12790 :  0.0395338647  - accuracy:  0.929145634\n",
            "12800 :  0.100771032  - accuracy:  0.929176569\n",
            "12810 :  0.185825691  - accuracy:  0.929202616\n",
            "12820 :  0.0546223745  - accuracy:  0.929243207\n",
            "12830 :  0.039236404  - accuracy:  0.929281294\n",
            "12840 :  0.110967509  - accuracy:  0.929316938\n",
            "12850 :  0.0623229221  - accuracy:  0.929354906\n",
            "12860 :  0.311335146  - accuracy:  0.929380715\n",
            "12870 :  0.0388612077  - accuracy:  0.929413736\n",
            "12880 :  0.0386188067  - accuracy:  0.929444253\n",
            "12890 :  0.0864624456  - accuracy:  0.929465055\n",
            "12900 :  0.113293156  - accuracy:  0.929502785\n",
            "12910 :  0.050207898  - accuracy:  0.929533184\n",
            "12920 :  0.0746749192  - accuracy:  0.929558694\n",
            "12930 :  0.00749953091  - accuracy:  0.929603517\n",
            "12940 :  0.130360335  - accuracy:  0.929641\n",
            "12950 :  0.0342601314  - accuracy:  0.929668784\n",
            "12960 :  0.0102977138  - accuracy:  0.9297086\n",
            "12970 :  0.139945075  - accuracy:  0.9297387\n",
            "12980 :  0.0493851602  - accuracy:  0.929780781\n",
            "12990 :  0.0333894603  - accuracy:  0.929825246\n",
            "13000 :  0.00757683394  - accuracy:  0.92986\n",
            "13010 :  0.0384165496  - accuracy:  0.929887474\n",
            "13020 :  0.351556182  - accuracy:  0.929912508\n",
            "13030 :  0.022510035  - accuracy:  0.929942369\n",
            "13040 :  0.0252859872  - accuracy:  0.929972112\n",
            "13050 :  0.00827531144  - accuracy:  0.930018604\n",
            "13060 :  0.078751117  - accuracy:  0.930041075\n",
            "13070 :  0.0554899052  - accuracy:  0.930075467\n",
            "13080 :  0.0816672891  - accuracy:  0.930097878\n",
            "13090 :  0.2876091  - accuracy:  0.930125\n",
            "13100 :  0.0227837041  - accuracy:  0.930161655\n",
            "13110 :  0.0761360228  - accuracy:  0.930183947\n",
            "13120 :  0.11915613  - accuracy:  0.93020618\n",
            "13130 :  0.00689585414  - accuracy:  0.930247426\n",
            "13140 :  0.00137586915  - accuracy:  0.93028152\n",
            "13150 :  0.151556522  - accuracy:  0.930317879\n",
            "13160 :  0.0401763096  - accuracy:  0.930344701\n",
            "13170 :  0.0330932327  - accuracy:  0.930373907\n",
            "13180 :  0.0889362  - accuracy:  0.930414855\n",
            "13190 :  0.0139035592  - accuracy:  0.93045342\n",
            "13200 :  0.0304857269  - accuracy:  0.930487156\n",
            "13210 :  0.0566704795  - accuracy:  0.930520833\n",
            "13220 :  0.035465885  - accuracy:  0.9305498\n",
            "13230 :  0.0252677985  - accuracy:  0.930588126\n",
            "13240 :  0.0998341963  - accuracy:  0.930616915\n",
            "13250 :  0.0267001651  - accuracy:  0.930662215\n",
            "13260 :  0.0753096491  - accuracy:  0.930702746\n",
            "13270 :  0.0267707407  - accuracy:  0.930745542\n",
            "13280 :  0.0174772  - accuracy:  0.930781186\n",
            "13290 :  0.0239128135  - accuracy:  0.930805087\n",
            "13300 :  0.0235989913  - accuracy:  0.930826545\n",
            "13310 :  0.032823015  - accuracy:  0.930871487\n",
            "13320 :  0.0120589156  - accuracy:  0.930899918\n",
            "13330 :  0.0773888305  - accuracy:  0.930933\n",
            "13340 :  0.147000074  - accuracy:  0.930954337\n",
            "13350 :  0.121356726  - accuracy:  0.930978\n",
            "13360 :  0.109820262  - accuracy:  0.931008577\n",
            "13370 :  0.0394939221  - accuracy:  0.93103683\n",
            "13380 :  0.0576815344  - accuracy:  0.931065\n",
            "13390 :  0.0937129706  - accuracy:  0.931095481\n",
            "13400 :  0.0928051546  - accuracy:  0.931128263\n",
            "13410 :  0.0228426959  - accuracy:  0.931158662\n",
            "13420 :  0.0345962979  - accuracy:  0.931196\n",
            "13430 :  0.0444576815  - accuracy:  0.931207657\n",
            "13440 :  0.0781739727  - accuracy:  0.931235611\n",
            "13450 :  0.17633456  - accuracy:  0.931270421\n",
            "13460 :  0.107539169  - accuracy:  0.931302965\n",
            "13470 :  0.0142156286  - accuracy:  0.931337714\n",
            "13480 :  0.017642824  - accuracy:  0.931372404\n",
            "13490 :  0.045121409  - accuracy:  0.931404769\n",
            "13500 :  0.109735183  - accuracy:  0.93143934\n",
            "13510 :  0.0628017485  - accuracy:  0.931473911\n",
            "13520 :  0.0145506579  - accuracy:  0.931513071\n",
            "13530 :  0.0144053809  - accuracy:  0.931552112\n",
            "13540 :  0.0383868292  - accuracy:  0.931593478\n",
            "13550 :  0.122145019  - accuracy:  0.931620896\n",
            "13560 :  0.155167282  - accuracy:  0.931650579\n",
            "13570 :  0.128999442  - accuracy:  0.931677878\n",
            "13580 :  0.211332321  - accuracy:  0.931698322\n",
            "13590 :  0.110365763  - accuracy:  0.931730151\n",
            "13600 :  0.0272111744  - accuracy:  0.93175739\n",
            "13610 :  0.0833500698  - accuracy:  0.931786895\n",
            "13620 :  0.018917691  - accuracy:  0.931827784\n",
            "13630 :  0.118417211  - accuracy:  0.931854844\n",
            "13640 :  0.106156409  - accuracy:  0.93187505\n",
            "13650 :  0.0309205242  - accuracy:  0.931904376\n",
            "13660 :  0.033517044  - accuracy:  0.931935906\n",
            "13670 :  0.00953534245  - accuracy:  0.931965113\n",
            "13680 :  0.0409898125  - accuracy:  0.932001173\n",
            "13690 :  0.10953515  - accuracy:  0.93203485\n",
            "13700 :  0.0887826532  - accuracy:  0.932063937\n",
            "13710 :  0.010341187  - accuracy:  0.9320907\n",
            "13720 :  0.127440348  - accuracy:  0.932119668\n",
            "13730 :  0.052302137  - accuracy:  0.9321509\n",
            "13740 :  0.166225836  - accuracy:  0.932184398\n",
            "13750 :  0.0263239536  - accuracy:  0.932211\n",
            "13760 :  0.0571913868  - accuracy:  0.93223983\n",
            "13770 :  0.0709249824  - accuracy:  0.93226862\n",
            "13780 :  0.0316250063  - accuracy:  0.932304144\n",
            "13790 :  0.0193356145  - accuracy:  0.932339609\n",
            "13800 :  0.189652622  - accuracy:  0.932361484\n",
            "13810 :  0.0761340857  - accuracy:  0.932401419\n",
            "13820 :  0.0373074859  - accuracy:  0.932427704\n",
            "13830 :  0.00341247325  - accuracy:  0.932460785\n",
            "13840 :  0.0285654776  - accuracy:  0.932471216\n",
            "13850 :  0.0639108568  - accuracy:  0.932495117\n",
            "13860 :  0.0498016886  - accuracy:  0.932521284\n",
            "13870 :  0.0158231463  - accuracy:  0.932549655\n",
            "13880 :  0.233992457  - accuracy:  0.932580233\n",
            "13890 :  0.00987021625  - accuracy:  0.932613\n",
            "13900 :  0.058767736  - accuracy:  0.932645798\n",
            "13910 :  0.10968215  - accuracy:  0.932676256\n",
            "13920 :  0.0917671323  - accuracy:  0.932711124\n",
            "13930 :  0.0391331054  - accuracy:  0.932732522\n",
            "13940 :  0.029449258  - accuracy:  0.932758331\n",
            "13950 :  0.050012935  - accuracy:  0.932777464\n",
            "13960 :  0.186209947  - accuracy:  0.932812154\n",
            "13970 :  0.142845035  - accuracy:  0.932842374\n",
            "13980 :  0.00919929892  - accuracy:  0.93287921\n",
            "13990 :  0.111528717  - accuracy:  0.932904899\n",
            "14000 :  0.00807733554  - accuracy:  0.93293494\n",
            "14010 :  0.178557083  - accuracy:  0.932965\n",
            "14020 :  0.00369213591  - accuracy:  0.932999432\n",
            "14030 :  0.048091989  - accuracy:  0.933033824\n",
            "14040 :  0.0520477407  - accuracy:  0.933079302\n",
            "14050 :  0.0463914573  - accuracy:  0.93311137\n",
            "14060 :  0.0906472951  - accuracy:  0.933141112\n",
            "14070 :  0.0396841317  - accuracy:  0.93317312\n",
            "14080 :  0.0881620795  - accuracy:  0.933202803\n",
            "14090 :  0.0955408  - accuracy:  0.933232486\n",
            "14100 :  0.158229932  - accuracy:  0.933257699\n",
            "14110 :  0.038475804  - accuracy:  0.933280647\n",
            "14120 :  0.0126925334  - accuracy:  0.933319032\n",
            "14130 :  0.104835019  - accuracy:  0.933341861\n",
            "14140 :  0.0394007228  - accuracy:  0.933364689\n",
            "14150 :  0.0396804921  - accuracy:  0.933398545\n",
            "14160 :  0.106113352  - accuracy:  0.93342793\n",
            "14170 :  0.0722355247  - accuracy:  0.933448434\n",
            "14180 :  0.156676948  - accuracy:  0.933464527\n",
            "14190 :  0.0614694282  - accuracy:  0.933498204\n",
            "14200 :  0.0182939246  - accuracy:  0.93352747\n",
            "14210 :  0.0198200569  - accuracy:  0.933556616\n",
            "14220 :  0.0127698109  - accuracy:  0.933585763\n",
            "14230 :  0.0202311091  - accuracy:  0.933612704\n",
            "14240 :  0.0237652957  - accuracy:  0.933637381\n",
            "14250 :  0.0341028  - accuracy:  0.933668613\n",
            "14260 :  0.0538204759  - accuracy:  0.933701932\n",
            "14270 :  0.00781522132  - accuracy:  0.9337309\n",
            "14280 :  0.105129778  - accuracy:  0.933757603\n",
            "14290 :  0.0927296951  - accuracy:  0.933784306\n",
            "14300 :  0.0995735  - accuracy:  0.933819652\n",
            "14310 :  0.16728662  - accuracy:  0.933848441\n",
            "14320 :  0.0217923932  - accuracy:  0.933879375\n",
            "14330 :  0.0108874757  - accuracy:  0.933914602\n",
            "14340 :  0.0786680281  - accuracy:  0.93393892\n",
            "14350 :  0.00172676495  - accuracy:  0.933978438\n",
            "14360 :  0.127827987  - accuracy:  0.934000432\n",
            "14370 :  0.0751122162  - accuracy:  0.934029\n",
            "14380 :  0.125143647  - accuracy:  0.934053123\n",
            "14390 :  0.087934345  - accuracy:  0.934088111\n",
            "14400 :  0.0804708898  - accuracy:  0.934120834\n",
            "14410 :  0.0321220905  - accuracy:  0.934153557\n",
            "14420 :  0.0499764122  - accuracy:  0.934179723\n",
            "14430 :  0.039437782  - accuracy:  0.934208035\n",
            "14440 :  0.039624054  - accuracy:  0.934234083\n",
            "14450 :  0.0859967917  - accuracy:  0.93426013\n",
            "14460 :  0.0235332772  - accuracy:  0.934292674\n",
            "14470 :  0.011017981  - accuracy:  0.934320807\n",
            "14480 :  0.0223508161  - accuracy:  0.934342384\n",
            "14490 :  0.103989542  - accuracy:  0.934366167\n",
            "14500 :  0.0254854858  - accuracy:  0.934400678\n",
            "14510 :  0.103585035  - accuracy:  0.934424341\n",
            "14520 :  0.0895547569  - accuracy:  0.934456587\n",
            "14530 :  0.0244950168  - accuracy:  0.934497356\n",
            "14540 :  0.0122886784  - accuracy:  0.934536\n",
            "14550 :  0.0401109941  - accuracy:  0.934565961\n",
            "14560 :  0.0148048121  - accuracy:  0.934598\n",
            "14570 :  0.076332  - accuracy:  0.934634328\n",
            "14580 :  0.0526456945  - accuracy:  0.93466413\n",
            "14590 :  0.0348349176  - accuracy:  0.934698224\n",
            "14600 :  0.170127749  - accuracy:  0.934725821\n",
            "14610 :  0.146473199  - accuracy:  0.93475771\n",
            "14620 :  0.128861517  - accuracy:  0.934780955\n",
            "14630 :  0.0206455514  - accuracy:  0.934804142\n",
            "14640 :  0.0264480412  - accuracy:  0.934842288\n",
            "14650 :  0.0217443071  - accuracy:  0.934865415\n",
            "14660 :  0.00959487259  - accuracy:  0.934899211\n",
            "14670 :  0.0501530096  - accuracy:  0.934932947\n",
            "14680 :  0.0124478657  - accuracy:  0.934955955\n",
            "14690 :  0.139932156  - accuracy:  0.934978962\n",
            "14700 :  0.0693318099  - accuracy:  0.935016811\n",
            "14710 :  0.0179446936  - accuracy:  0.935050368\n",
            "14720 :  0.0269467011  - accuracy:  0.935083926\n",
            "14730 :  0.0231593139  - accuracy:  0.93511951\n",
            "14740 :  0.0461097956  - accuracy:  0.935138047\n",
            "14750 :  0.0597942919  - accuracy:  0.935169339\n",
            "14760 :  0.0140816495  - accuracy:  0.935200572\n",
            "14770 :  0.135915086  - accuracy:  0.935233831\n",
            "14780 :  0.0040216553  - accuracy:  0.935265\n",
            "14790 :  0.123471946  - accuracy:  0.935287654\n",
            "14800 :  0.099970907  - accuracy:  0.935322881\n",
            "14810 :  0.0359048322  - accuracy:  0.935356\n",
            "14820 :  0.0442738906  - accuracy:  0.935378551\n",
            "14830 :  0.146908164  - accuracy:  0.935409486\n",
            "14840 :  0.00366699696  - accuracy:  0.935429871\n",
            "14850 :  0.0603522435  - accuracy:  0.935456514\n",
            "14860 :  0.0448411852  - accuracy:  0.935481\n",
            "14870 :  0.06990394  - accuracy:  0.935513914\n",
            "14880 :  0.0428119227  - accuracy:  0.935532033\n",
            "14890 :  0.221428961  - accuracy:  0.935556471\n",
            "14900 :  0.0238443501  - accuracy:  0.935585\n",
            "14910 :  0.181129098  - accuracy:  0.935596764\n",
            "14920 :  0.0143970856  - accuracy:  0.935612738\n",
            "14930 :  0.00645729108  - accuracy:  0.935645401\n",
            "14940 :  0.00821888633  - accuracy:  0.935667574\n",
            "14950 :  0.0744142756  - accuracy:  0.93569386\n",
            "14960 :  0.11116986  - accuracy:  0.935716\n",
            "14970 :  0.0205773581  - accuracy:  0.935742199\n",
            "14980 :  0.158653185  - accuracy:  0.935768425\n",
            "14990 :  0.0313988216  - accuracy:  0.935786247\n",
            "15000 :  0.00602608919  - accuracy:  0.935816526\n",
            "15010 :  0.000192922103  - accuracy:  0.935836434\n",
            "15020 :  0.0273094773  - accuracy:  0.93586874\n",
            "15030 :  0.107094295  - accuracy:  0.935901\n",
            "15040 :  0.0144172981  - accuracy:  0.935922861\n",
            "15050 :  0.012886798  - accuracy:  0.935950875\n",
            "15060 :  0.161518082  - accuracy:  0.935974777\n",
            "15070 :  0.0772800595  - accuracy:  0.935994446\n",
            "15080 :  0.016394902  - accuracy:  0.936014056\n",
            "15090 :  0.242409319  - accuracy:  0.936037838\n",
            "15100 :  0.0484155491  - accuracy:  0.936049163\n",
            "15110 :  0.0451716855  - accuracy:  0.936079085\n",
            "15120 :  0.227640152  - accuracy:  0.936096549\n",
            "15130 :  0.0226155929  - accuracy:  0.936128438\n",
            "15140 :  0.00492234435  - accuracy:  0.936160326\n",
            "15150 :  0.0210882779  - accuracy:  0.936190069\n",
            "15160 :  0.0141001213  - accuracy:  0.936223924\n",
            "15170 :  0.0219067875  - accuracy:  0.936243296\n",
            "15180 :  0.0494478345  - accuracy:  0.936268866\n",
            "15190 :  0.0123007074  - accuracy:  0.936302602\n",
            "15200 :  0.0635081902  - accuracy:  0.936338305\n",
            "15210 :  0.065202482  - accuracy:  0.936357558\n",
            "15220 :  0.0601585396  - accuracy:  0.936380923\n",
            "15230 :  0.0402200297  - accuracy:  0.936398089\n",
            "15240 :  0.0667383447  - accuracy:  0.936415195\n",
            "15250 :  0.017015025  - accuracy:  0.936452806\n",
            "15260 :  0.172830105  - accuracy:  0.936476\n",
            "15270 :  0.0254306942  - accuracy:  0.93651557\n",
            "15280 :  0.0128800822  - accuracy:  0.936546922\n",
            "15290 :  0.00113876187  - accuracy:  0.936576128\n",
            "15300 :  0.00882384  - accuracy:  0.936605334\n",
            "15310 :  0.0440768823  - accuracy:  0.936628401\n",
            "15320 :  0.0114414142  - accuracy:  0.936661601\n",
            "15330 :  0.121444672  - accuracy:  0.936682522\n",
            "15340 :  0.259343356  - accuracy:  0.936713576\n",
            "15350 :  0.0302930083  - accuracy:  0.936736524\n",
            "15360 :  0.0782934949  - accuracy:  0.936769545\n",
            "15370 :  0.0397377  - accuracy:  0.936798513\n",
            "15380 :  0.00599821936  - accuracy:  0.936829448\n",
            "15390 :  0.0397007689  - accuracy:  0.936858296\n",
            "15400 :  0.270019263  - accuracy:  0.936883092\n",
            "15410 :  0.723301113  - accuracy:  0.936909854\n",
            "15420 :  0.110172279  - accuracy:  0.936936557\n",
            "15430 :  0.0132416682  - accuracy:  0.936961234\n",
            "15440 :  0.150588036  - accuracy:  0.936969697\n",
            "15450 :  0.0250417851  - accuracy:  0.937000394\n",
            "15460 :  0.0665283  - accuracy:  0.93703711\n",
            "15470 :  0.0119304238  - accuracy:  0.937065661\n",
            "15480 :  0.0244368427  - accuracy:  0.937094212\n",
            "15490 :  0.0066700289  - accuracy:  0.937120676\n",
            "15500 :  0.0144286631  - accuracy:  0.93715322\n",
            "15510 :  0.00467820605  - accuracy:  0.937167525\n",
            "15520 :  0.0259387  - accuracy:  0.937191904\n",
            "15530 :  0.0282827094  - accuracy:  0.937222302\n",
            "15540 :  0.0359536372  - accuracy:  0.937250614\n",
            "15550 :  0.279779434  - accuracy:  0.93726486\n",
            "15560 :  0.00476408098  - accuracy:  0.937287092\n",
            "15570 :  0.155252889  - accuracy:  0.937303305\n",
            "15580 :  0.0622717626  - accuracy:  0.937311471\n",
            "15590 :  0.146736309  - accuracy:  0.93734163\n",
            "15600 :  0.0831556171  - accuracy:  0.937363744\n",
            "15610 :  0.0483196042  - accuracy:  0.93738991\n",
            "15620 :  0.150199577  - accuracy:  0.937418\n",
            "15630 :  0.00342844  - accuracy:  0.937436044\n",
            "15640 :  0.195591092  - accuracy:  0.937460065\n",
            "15650 :  0.157294437  - accuracy:  0.937472045\n",
            "15660 :  0.0107467985  - accuracy:  0.93750596\n",
            "15670 :  0.0210417509  - accuracy:  0.937531888\n",
            "15680 :  0.0742194578  - accuracy:  0.937559783\n",
            "15690 :  0.0107676489  - accuracy:  0.937585652\n",
            "15700 :  0.00511299446  - accuracy:  0.937613487\n",
            "15710 :  0.00329086  - accuracy:  0.937639236\n",
            "15720 :  0.00142407895  - accuracy:  0.937667\n",
            "15730 :  0.129230723  - accuracy:  0.937684774\n",
            "15740 :  0.069823958  - accuracy:  0.937714458\n",
            "15750 :  0.0116809784  - accuracy:  0.937740088\n",
            "15760 :  0.00890109595  - accuracy:  0.937761784\n",
            "15770 :  0.0858196318  - accuracy:  0.937779427\n",
            "15780 :  0.246635899  - accuracy:  0.937801\n",
            "15790 :  0.184979677  - accuracy:  0.937830508\n",
            "15800 :  0.0614744201  - accuracy:  0.937862\n",
            "15810 :  0.0117117874  - accuracy:  0.937893391\n",
            "15820 :  0.189849064  - accuracy:  0.937912881\n",
            "15830 :  0.053049162  - accuracy:  0.937942207\n",
            "15840 :  0.0792651251  - accuracy:  0.937969565\n",
            "15850 :  0.0506780595  - accuracy:  0.937985063\n",
            "15860 :  0.120185666  - accuracy:  0.938014269\n",
            "15870 :  0.152717158  - accuracy:  0.938041568\n",
            "15880 :  0.00302816066  - accuracy:  0.938068748\n",
            "15890 :  0.0531621613  - accuracy:  0.938095927\n",
            "15900 :  0.21761182  - accuracy:  0.93812108\n",
            "15910 :  0.00549960416  - accuracy:  0.938154101\n",
            "15920 :  0.00694211852  - accuracy:  0.938181162\n",
            "15930 :  0.0100847706  - accuracy:  0.938204288\n",
            "15940 :  0.00278301025  - accuracy:  0.938225448\n",
            "15950 :  0.038478449  - accuracy:  0.93823868\n",
            "15960 :  0.0298082605  - accuracy:  0.938271523\n",
            "15970 :  0.336351961  - accuracy:  0.938298404\n",
            "15980 :  0.0795469359  - accuracy:  0.938319445\n",
            "15990 :  0.0126965009  - accuracy:  0.938336492\n",
            "16000 :  0.0370614938  - accuracy:  0.938361406\n",
            "16010 :  0.0446517617  - accuracy:  0.938372552\n",
            "16020 :  0.0802578479  - accuracy:  0.938397348\n",
            "16030 :  0.0952593088  - accuracy:  0.938416302\n",
            "16040 :  0.013215377  - accuracy:  0.938448846\n",
            "16050 :  0.0302721504  - accuracy:  0.938471615\n",
            "16060 :  0.0779228359  - accuracy:  0.938496351\n",
            "16070 :  0.13379322  - accuracy:  0.938511252\n",
            "16080 :  0.00815190934  - accuracy:  0.938535929\n",
            "16090 :  0.0146167083  - accuracy:  0.93856442\n",
            "16100 :  0.00196256116  - accuracy:  0.938596725\n",
            "16110 :  0.0252083335  - accuracy:  0.938634872\n",
            "16120 :  0.0217500608  - accuracy:  0.938665152\n",
            "16130 :  0.00635320321  - accuracy:  0.938691556\n",
            "16140 :  0.192032859  - accuracy:  0.93871212\n",
            "16150 :  0.0311498  - accuracy:  0.938738465\n",
            "16160 :  0.0188546069  - accuracy:  0.9387725\n",
            "16170 :  0.00222747074  - accuracy:  0.938800693\n",
            "16180 :  0.389884561  - accuracy:  0.938821137\n",
            "16190 :  0.00821833219  - accuracy:  0.9388358\n",
            "16200 :  0.0124853533  - accuracy:  0.93886584\n",
            "16210 :  0.0844419673  - accuracy:  0.938891947\n",
            "16220 :  0.0125705674  - accuracy:  0.938914239\n",
            "16230 :  0.0622931048  - accuracy:  0.938932598\n",
            "16240 :  0.00230922271  - accuracy:  0.93895483\n",
            "16250 :  0.0774971172  - accuracy:  0.938973188\n",
            "16260 :  0.0277220439  - accuracy:  0.938997269\n",
            "16270 :  0.0146460244  - accuracy:  0.939017475\n",
            "16280 :  0.0079125762  - accuracy:  0.939043403\n",
            "16290 :  0.0140407234  - accuracy:  0.939075053\n",
            "16300 :  0.000659481273  - accuracy:  0.939102888\n",
            "16310 :  0.0632219  - accuracy:  0.939130604\n",
            "16320 :  0.0124979094  - accuracy:  0.939150691\n",
            "16330 :  0.0228723101  - accuracy:  0.939178407\n",
            "16340 :  0.00789413601  - accuracy:  0.939208\n",
            "16350 :  0.00402459223  - accuracy:  0.939239383\n",
            "16360 :  0.0055624987  - accuracy:  0.939270794\n",
            "16370 :  0.126193792  - accuracy:  0.939302206\n",
            "16380 :  0.00899260212  - accuracy:  0.939337313\n",
            "16390 :  0.0445849895  - accuracy:  0.939357162\n",
            "16400 :  0.0195917562  - accuracy:  0.939386547\n",
            "16410 :  0.0630978867  - accuracy:  0.939412057\n",
            "16420 :  0.0108795557  - accuracy:  0.939445138\n",
            "16430 :  0.172850236  - accuracy:  0.939468682\n",
            "16440 :  0.0103165759  - accuracy:  0.939484596\n",
            "16450 :  0.106339164  - accuracy:  0.939504325\n",
            "16460 :  0.0120619256  - accuracy:  0.939522088\n",
            "16470 :  0.00953973178  - accuracy:  0.93954742\n",
            "16480 :  0.162291124  - accuracy:  0.939574599\n",
            "16490 :  0.0647430569  - accuracy:  0.939596117\n",
            "16500 :  0.00111116632  - accuracy:  0.939608097\n",
            "16510 :  0.0369663909  - accuracy:  0.939631402\n",
            "16520 :  0.0904780105  - accuracy:  0.939654708\n",
            "16530 :  0.0287249349  - accuracy:  0.939683676\n",
            "16540 :  0.0662089288  - accuracy:  0.939708769\n",
            "16550 :  0.225077093  - accuracy:  0.93972826\n",
            "16560 :  0.182376981  - accuracy:  0.939747632\n",
            "16570 :  0.08938694  - accuracy:  0.939765155\n",
            "16580 :  0.034815  - accuracy:  0.939790189\n",
            "16590 :  0.0487254709  - accuracy:  0.939807653\n",
            "16600 :  0.0267271511  - accuracy:  0.939834476\n",
            "16610 :  0.0700880364  - accuracy:  0.939855635\n",
            "16620 :  0.00485090632  - accuracy:  0.939878702\n",
            "16630 :  0.166755646  - accuracy:  0.939897895\n",
            "16640 :  0.051641982  - accuracy:  0.939920902\n",
            "16650 :  0.064352259  - accuracy:  0.939949453\n",
            "16660 :  0.03667593  - accuracy:  0.939970493\n",
            "16670 :  0.0296173654  - accuracy:  0.940000892\n",
            "16680 :  0.00588057609  - accuracy:  0.940029383\n",
            "16690 :  0.07045874  - accuracy:  0.940054059\n",
            "16700 :  0.0320681073  - accuracy:  0.940073133\n",
            "16710 :  0.0410346314  - accuracy:  0.940101504\n",
            "16720 :  0.0706811249  - accuracy:  0.940124273\n",
            "16730 :  0.0127224447  - accuracy:  0.940150738\n",
            "16740 :  0.00995442271  - accuracy:  0.940177143\n",
            "16750 :  0.017698722  - accuracy:  0.940207243\n",
            "16760 :  0.17291297  - accuracy:  0.940226138\n",
            "16770 :  0.017069459  - accuracy:  0.940252483\n",
            "16780 :  0.0192643516  - accuracy:  0.940273166\n",
            "16790 :  0.0133987311  - accuracy:  0.940297604\n",
            "16800 :  0.0054563256  - accuracy:  0.940322\n",
            "16810 :  0.107896425  - accuracy:  0.940335155\n",
            "16820 :  0.0370286182  - accuracy:  0.940350175\n",
            "16830 :  0.00405193493  - accuracy:  0.940376341\n",
            "16840 :  0.114935502  - accuracy:  0.940402508\n",
            "16850 :  0.0387592241  - accuracy:  0.94042486\n",
            "16860 :  0.0428642109  - accuracy:  0.940445364\n",
            "16870 :  0.0182882361  - accuracy:  0.940467715\n",
            "16880 :  0.0477891415  - accuracy:  0.94048816\n",
            "16890 :  0.0177439153  - accuracy:  0.940517843\n",
            "16900 :  0.135344908  - accuracy:  0.940527201\n",
            "16910 :  0.0658931807  - accuracy:  0.940547585\n",
            "16920 :  0.00848619454  - accuracy:  0.940567911\n",
            "16930 :  0.0724057257  - accuracy:  0.940595627\n",
            "16940 :  0.0499635637  - accuracy:  0.940623343\n",
            "16950 :  0.026421424  - accuracy:  0.940636218\n",
            "16960 :  0.0344606787  - accuracy:  0.940662\n",
            "16970 :  0.0566978  - accuracy:  0.940682292\n",
            "16980 :  0.183441624  - accuracy:  0.940695107\n",
            "16990 :  0.0353040248  - accuracy:  0.940719\n",
            "17000 :  0.00923544168  - accuracy:  0.94074285\n",
            "17010 :  0.0766456276  - accuracy:  0.940766633\n",
            "17020 :  0.00188744161  - accuracy:  0.940792263\n",
            "17030 :  0.0803208724  - accuracy:  0.940819681\n",
            "17040 :  0.0650830343  - accuracy:  0.940839767\n",
            "17050 :  0.0293727946  - accuracy:  0.940854311\n",
            "17060 :  0.179586321  - accuracy:  0.940879822\n",
            "17070 :  0.00753209973  - accuracy:  0.940899789\n",
            "17080 :  0.00401796447  - accuracy:  0.940927088\n",
            "17090 :  0.00539681967  - accuracy:  0.940952539\n",
            "17100 :  0.0804198384  - accuracy:  0.940981567\n",
            "17110 :  0.263494045  - accuracy:  0.941001475\n",
            "17120 :  0.0675661117  - accuracy:  0.941023111\n",
            "17130 :  0.0162107628  - accuracy:  0.941037476\n",
            "17140 :  0.0243750289  - accuracy:  0.94106096\n",
            "17150 :  0.0138678979  - accuracy:  0.941091657\n",
            "17160 :  0.00625471957  - accuracy:  0.94110781\n",
            "17170 :  0.104644358  - accuracy:  0.941138446\n",
            "17180 :  0.0241894778  - accuracy:  0.941167295\n",
            "17190 :  0.153271601  - accuracy:  0.941186965\n",
            "17200 :  0.0236167163  - accuracy:  0.941217542\n",
            "17210 :  0.0332708508  - accuracy:  0.941237152\n",
            "17220 :  0.00180272444  - accuracy:  0.94125855\n",
            "17230 :  0.0299432445  - accuracy:  0.941279948\n",
            "17240 :  0.0330779441  - accuracy:  0.941305\n",
            "17250 :  0.0843112618  - accuracy:  0.941328108\n",
            "17260 :  0.00931827  - accuracy:  0.941347659\n",
            "17270 :  0.033112146  - accuracy:  0.941372573\n",
            "17280 :  0.0445607454  - accuracy:  0.941399217\n",
            "17290 :  0.0403671339  - accuracy:  0.94142592\n",
            "17300 :  0.0899101645  - accuracy:  0.941448927\n",
            "17310 :  0.087436676  - accuracy:  0.941471934\n",
            "17320 :  0.0128373168  - accuracy:  0.94149667\n",
            "17330 :  0.0732206553  - accuracy:  0.941516042\n",
            "17340 :  0.00880776905  - accuracy:  0.941540718\n",
            "17350 :  0.00152171357  - accuracy:  0.941565454\n",
            "17360 :  0.00308686099  - accuracy:  0.941595495\n",
            "17370 :  0.189845055  - accuracy:  0.941616535\n",
            "17380 :  0.0557961054  - accuracy:  0.941635728\n",
            "17390 :  0.00869214814  - accuracy:  0.941656709\n",
            "17400 :  0.0366134  - accuracy:  0.94167769\n",
            "17410 :  0.0610737875  - accuracy:  0.941698611\n",
            "17420 :  0.00419791322  - accuracy:  0.941726685\n",
            "17430 :  0.0361552462  - accuracy:  0.941747606\n",
            "17440 :  0.0626317337  - accuracy:  0.941770256\n",
            "17450 :  0.0211534  - accuracy:  0.941794634\n",
            "17460 :  0.0656867251  - accuracy:  0.941817284\n",
            "17470 :  0.0206772238  - accuracy:  0.941829085\n",
            "17480 :  0.0155797973  - accuracy:  0.941853464\n",
            "17490 :  0.0323568173  - accuracy:  0.941876\n",
            "17500 :  0.00233294419  - accuracy:  0.941898465\n",
            "17510 :  0.1232481  - accuracy:  0.941919148\n",
            "17520 :  0.00681139436  - accuracy:  0.941939831\n",
            "17530 :  0.0153439064  - accuracy:  0.941969395\n",
            "17540 :  0.108913459  - accuracy:  0.941993535\n",
            "17550 :  0.0289447363  - accuracy:  0.942014158\n",
            "17560 :  0.214982435  - accuracy:  0.942032933\n",
            "17570 :  0.171565056  - accuracy:  0.942048132\n",
            "17580 :  0.00515225064  - accuracy:  0.942072213\n",
            "17590 :  0.0260809287  - accuracy:  0.942094505\n",
            "17600 :  0.0505783446  - accuracy:  0.942113161\n",
            "17610 :  0.0513086207  - accuracy:  0.942131877\n",
            "17620 :  0.0142687485  - accuracy:  0.94215405\n",
            "17630 :  0.00471395487  - accuracy:  0.942176223\n",
            "17640 :  0.0278865024  - accuracy:  0.942203701\n",
            "17650 :  0.0329975523  - accuracy:  0.942225814\n",
            "17660 :  0.0247422811  - accuracy:  0.942255\n",
            "17670 :  0.00771760847  - accuracy:  0.942275286\n",
            "17680 :  0.00641751196  - accuracy:  0.942302644\n",
            "17690 :  0.0701769739  - accuracy:  0.942315817\n",
            "17700 :  0.0193350445  - accuracy:  0.942341387\n",
            "17710 :  0.0226477496  - accuracy:  0.942370415\n",
            "17720 :  0.0869733915  - accuracy:  0.942395866\n",
            "17730 :  0.0186986234  - accuracy:  0.942424834\n",
            "17740 :  0.0889683217  - accuracy:  0.942448497\n",
            "17750 :  0.171279892  - accuracy:  0.9424721\n",
            "17760 :  0.0429283902  - accuracy:  0.942495704\n",
            "17770 :  0.0101375282  - accuracy:  0.942510486\n",
            "17780 :  0.112506345  - accuracy:  0.942525268\n",
            "17790 :  0.00122303539  - accuracy:  0.942552269\n",
            "17800 :  0.00370486593  - accuracy:  0.942568779\n",
            "17810 :  0.138266429  - accuracy:  0.942586958\n",
            "17820 :  0.0197927076  - accuracy:  0.9426139\n",
            "17830 :  0.131057903  - accuracy:  0.942633867\n",
            "17840 :  0.00211356394  - accuracy:  0.942662477\n",
            "17850 :  0.0780448243  - accuracy:  0.94267714\n",
            "17860 :  0.0306475349  - accuracy:  0.942686439\n",
            "17870 :  0.0720523894  - accuracy:  0.942704558\n",
            "17880 :  0.151054725  - accuracy:  0.942712128\n",
            "17890 :  0.0193454511  - accuracy:  0.942731917\n",
            "17900 :  0.0264887512  - accuracy:  0.942751706\n",
            "17910 :  0.0920588449  - accuracy:  0.942762733\n",
            "17920 :  0.0104058925  - accuracy:  0.942785919\n",
            "17930 :  0.0393542275  - accuracy:  0.942802191\n",
            "17940 :  0.0472346321  - accuracy:  0.942823589\n",
            "17950 :  0.0357696153  - accuracy:  0.942848504\n",
            "17960 :  0.00479748147  - accuracy:  0.942862928\n",
            "17970 :  0.116075844  - accuracy:  0.942879081\n",
            "17980 :  0.0206205212  - accuracy:  0.942905605\n",
            "17990 :  0.125726312  - accuracy:  0.942921698\n",
            "18000 :  0.00450612744  - accuracy:  0.942944765\n",
            "18010 :  0.0703327581  - accuracy:  0.942962527\n",
            "18020 :  0.00069385895  - accuracy:  0.942978621\n",
            "18030 :  0.029329814  - accuracy:  0.942996383\n",
            "18040 :  0.0747165903  - accuracy:  0.943019271\n",
            "18050 :  0.0257834438  - accuracy:  0.943045676\n",
            "18060 :  0.0384373516  - accuracy:  0.94306165\n",
            "18070 :  0.0407513343  - accuracy:  0.943087935\n",
            "18080 :  0.127123326  - accuracy:  0.943110764\n",
            "18090 :  0.00995745137  - accuracy:  0.943133593\n",
            "18100 :  0.00173302053  - accuracy:  0.94315809\n",
            "18110 :  0.0409396626  - accuracy:  0.943175673\n",
            "18120 :  0.00616341131  - accuracy:  0.943198442\n",
            "18130 :  0.0903221  - accuracy:  0.943224609\n",
            "18140 :  0.242952153  - accuracy:  0.943252444\n",
            "18150 :  0.0142072486  - accuracy:  0.943275094\n",
            "18160 :  0.00516069774  - accuracy:  0.943294287\n",
            "18170 :  0.0288604312  - accuracy:  0.943322062\n",
            "18180 :  0.136303753  - accuracy:  0.943341196\n",
            "18190 :  0.0602479205  - accuracy:  0.943367243\n",
            "18200 :  0.00340937078  - accuracy:  0.943389773\n",
            "18210 :  0.00656253705  - accuracy:  0.943415701\n",
            "18220 :  0.00296917604  - accuracy:  0.94344157\n",
            "18230 :  0.0192365404  - accuracy:  0.943469226\n",
            "18240 :  0.0140757225  - accuracy:  0.943491638\n",
            "18250 :  0.0172899608  - accuracy:  0.943515718\n",
            "18260 :  0.0257456452  - accuracy:  0.943536401\n",
            "18270 :  0.0131218713  - accuracy:  0.943557\n",
            "18280 :  0.02533249  - accuracy:  0.943584502\n",
            "18290 :  0.0418521464  - accuracy:  0.943608522\n",
            "18300 :  0.0730127916  - accuracy:  0.943634212\n",
            "18310 :  0.0572559536  - accuracy:  0.943653047\n",
            "18320 :  0.109908581  - accuracy:  0.943671882\n",
            "18330 :  0.080461  - accuracy:  0.943689\n",
            "18340 :  0.0263333  - accuracy:  0.943706036\n",
            "18350 :  0.0034533008  - accuracy:  0.943728209\n",
            "18360 :  0.0850208402  - accuracy:  0.943750322\n",
            "18370 :  0.0126476102  - accuracy:  0.943770766\n",
            "18380 :  0.0963724479  - accuracy:  0.943796277\n",
            "18390 :  0.00036762713  - accuracy:  0.943823397\n",
            "18400 :  0.00358223  - accuracy:  0.943842053\n",
            "18410 :  0.0143534364  - accuracy:  0.943865776\n",
            "18420 :  0.139048293  - accuracy:  0.943884373\n",
            "18430 :  0.0494058803  - accuracy:  0.943904638\n",
            "18440 :  0.0239521302  - accuracy:  0.943924904\n",
            "18450 :  0.117672876  - accuracy:  0.943940043\n",
            "18460 :  0.00950243883  - accuracy:  0.943962\n",
            "18470 :  0.0356967337  - accuracy:  0.943978786\n",
            "18480 :  0.0275422  - accuracy:  0.944000602\n",
            "18490 :  0.0340669826  - accuracy:  0.94401741\n",
            "18500 :  0.0804082304  - accuracy:  0.944034159\n",
            "18510 :  0.174635425  - accuracy:  0.944054246\n",
            "18520 :  0.0260380022  - accuracy:  0.944079399\n",
            "18530 :  0.0469337553  - accuracy:  0.944091\n",
            "18540 :  0.0893124193  - accuracy:  0.944112778\n",
            "18550 :  0.00459850393  - accuracy:  0.944134474\n",
            "18560 :  0.0352832265  - accuracy:  0.94415611\n",
            "18570 :  0.0280296542  - accuracy:  0.944179475\n",
            "18580 :  0.0924885422  - accuracy:  0.944197774\n",
            "18590 :  0.0274668876  - accuracy:  0.944214344\n",
            "18600 :  0.0109811462  - accuracy:  0.944235921\n",
            "18610 :  0.0119019011  - accuracy:  0.944259167\n",
            "18620 :  0.000393319118  - accuracy:  0.944282413\n",
            "18630 :  0.0139486548  - accuracy:  0.944300532\n",
            "18640 :  0.0213904269  - accuracy:  0.944320381\n",
            "18650 :  0.0132086379  - accuracy:  0.944346905\n",
            "18660 :  0.0611930639  - accuracy:  0.944368362\n",
            "18670 :  0.109252714  - accuracy:  0.944386423\n",
            "18680 :  0.0755725726  - accuracy:  0.944406152\n",
            "18690 :  0.0953483358  - accuracy:  0.944419205\n",
            "18700 :  0.0402834043  - accuracy:  0.944440544\n",
            "18710 :  0.0258353241  - accuracy:  0.944461882\n",
            "18720 :  0.0517571419  - accuracy:  0.944483221\n",
            "18730 :  0.157765806  - accuracy:  0.944501162\n",
            "18740 :  0.0254341606  - accuracy:  0.944524109\n",
            "18750 :  0.0387674905  - accuracy:  0.944540381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oGo5y4wm-Qf",
        "colab_type": "text"
      },
      "source": [
        "You are invited to read the source code carefully and compare it with the custom training loop from the previous chapter, Chapter 4, TensorFlow 2.0 Architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2jCQUzLlW43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpHpAIUhZDkX",
        "colab_type": "text"
      },
      "source": [
        "# Image Classification Using TensorFlow Hub\n",
        "\n",
        "We have discussed the image classification task in all of the previous chapters of this book. We have seen how it is possible to define a convolutional neural network by stacking several convolutional layers and how to train it using Keras. We also looked at eager execution and saw that using AutoGraph is straightforward.\n",
        "\n",
        "이 책의 모든 이전 장에서 이미지 분류 작업에 대해 논의했습니다. 우리는 여러 개의 컨볼 루션 레이어를 쌓아 컨볼 루션 신경망을 정의하는 방법과 Keras를 사용하여이를 훈련시키는 방법을 보았습니다. 우리는 또한 간절한 실행을보고 AutoGraph를 사용하는 것이 간단하다는 것을 알았습니다.\n",
        "\n",
        "So far, the convolutional architecture used has been a LeNet-like architecture, with an expected input size of 28 x 28, trained end to end every time to make the network learn how to extract the correct features to solve the fashion-MNIST classification task.\n",
        "\n",
        "지금까지 사용 된 컨벌루션 아키텍처는 Lenet과 유사한 아키텍처로, 예상 입력 크기는 28 x 28이며, 네트워크가 패션 -MNIST 분류 작업을 해결하기 위해 올바른 기능을 추출하는 방법을 배우도록 매번 훈련되었습니다. .\n",
        "\n",
        "Building a classifier from scratch, defining the architecture layer by layer, is an excellent didactical exercise that allows you to experiment with how different layer configurations can change the network performance. However, in real-life scenarios, the amount of data available to train a classifier is often limited. Gathering clean and correctly labeled data is a time-consuming process, and collecting a dataset with thousands of samples is tough. Moreover, even when the dataset size is adequate (thus, we are in a big data regime), training a classifier on it is a slow process; the training process might require several hours of GPU time since architectures more complicated than our LeNet-like architecture are necessary to achieve satisfactory results. Different architectures have been developed over the years, all of them introducing some novelties that have allowed the correct classification of color images with a resolution higher than 28 x 28.\n",
        "\n",
        "계층별로 아키텍처를 정의하는 분류기를 처음부터 작성하는 것은 다른 계층 구성이 네트워크 성능을 어떻게 변경할 수 있는지 실험 해 볼 수있는 훌륭한 실습입니다. 그러나 실제 시나리오에서는 분류자를 훈련시키는 데 사용할 수있는 데이터의 양이 종종 제한됩니다. 깨끗하고 올바르게 레이블이 지정된 데이터를 수집하는 것은 시간이 많이 걸리는 프로세스이며 수천 개의 샘플이 포함 된 데이터 세트를 수집하는 것은 어렵습니다. 더욱이, 데이터 세트 크기가 충분하더라도 (따라서 우리는 큰 데이터 체제에있다), 분류기를 훈련시키는 것은 느린 과정이다. 만족스러운 결과를 얻으려면 LeNet과 유사한 아키텍처보다 복잡한 아키텍처가 필요하기 때문에 훈련 과정에는 몇 시간의 GPU 시간이 필요할 수 있습니다. 수년에 걸쳐 다양한 아키텍처가 개발되었으며, 모두 28 x 28보다 높은 해상도로 컬러 이미지를 올바르게 분류 할 수있는 몇 가지 참신함을 소개합니다.\n",
        "\n",
        "Academia and industry release new classification architectures to improve the state of the art year on year. Their performance for an image classification task is measured by looking at the top-1 accuracy reached by the architecture when trained and tested on massive datasets such as ImageNet.\n",
        "\n",
        "학계와 산업계는 매년 최신 상태를 개선하기 위해 새로운 분류 아키텍처를 출시합니다. 이미지 분류 작업의 성능은 ImageNet과 같은 대규모 데이터 세트에서 교육 및 테스트 할 때 아키텍처가 달성 한 최고의 정확도를 확인하여 측정됩니다.\n",
        "\n",
        "ImageNet is a dataset of over 15 million high-resolution images with more than 22,000 categories, all of them manually labeled. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC ) is a yearly object detection and classification challenge that uses a subset of ImageNet of 1,000 images for 1,000 categories. The dataset used for the computation is made up of roughly 1.2 million training images, 50,000 validation images, and 100,000 testing images.\n",
        "\n",
        "ImageNet은 22,000 개가 넘는 범주를 가진 1,500 만 개가 넘는 고해상도 이미지의 데이터 세트로, 모두 수동으로 레이블이 지정됩니다. ImageNet Largescale Visual Recognition Challenge (ILSVRC)는 1,000 개 범주에 대해 1,000 개 이미지의 ImageNet 하위 집합을 사용하는 연간 개체 감지 및 분류 문제입니다. 계산에 사용 된 데이터 세트는 대략 120 만 개의 훈련 이미지, 50,000 개의 검증 이미지 및 100,000 개의 테스트 이미지로 구성됩니다.\n",
        "\n",
        "To achieve impressive results on an image classification task, researchers found that deep architectures were needed. This approach has a drawback—the deeper the network, the higher the number of parameters to train. But a higher number of parameters implies that a lot of computing power is needed (and computing power costs!). Since academia and industry have already developed and trained their models, why don't we take advantage of their work to speed up our development without reinventing the wheel every time?\n",
        "\n",
        "이미지 분류 작업에서 인상적인 결과를 달성하기 위해 연구원들은 깊은 아키텍처가 필요하다는 것을 발견했습니다. 이 방법에는 네트워크가 깊을수록 학습 할 매개 변수 수가 더 많다는 단점이 있습니다. 그러나 더 많은 수의 매개 변수는 많은 컴퓨팅 성능이 필요하다는 것을 의미합니다 (및 컴퓨팅 성능 비용). 학계와 산업계에서 이미 모델을 개발하고 교육했기 때문에 매번 바퀴를 다시 만들지 않고 개발 속도를 높이기 위해 작업을 활용하지 않겠습니까?\n",
        "\n",
        "In this chapter, we'll discuss transfer learning and fine-tuning, showing how they can speed up development. TensorFlow Hub is used as a tool to quickly get the models we need and speed up development.\n",
        "\n",
        "이 장에서는 전이 학습 및 미세 조정에 대해 논의하고 이들이 개발 속도를 높이는 방법을 보여줍니다. TensorFlow Hub는 필요한 모델을 신속하게 얻고 개발 속도를 높이는 도구로 사용됩니다.\n",
        "\n",
        "By the end of this chapter, you will know how to transfer the knowledge embedded in a model to a new task, using TensorFlow Hub easily, thanks to its Keras integration.\n",
        "\n",
        "이 장을 마치면 Keras 통합 덕분에 TensorFlow Hub를 사용하여 모델에 포함 된 지식을 새로운 작업으로 이전하는 방법을 알 수 있습니다.\n",
        "\n",
        "In this chapter, we will cover the following topics:\n",
        "\n",
        "* Getting the data\n",
        "* Transfer learning\n",
        "* Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADTsy8GMaO4e",
        "colab_type": "text"
      },
      "source": [
        "## Getting the data\n",
        "\n",
        "The task we are going to solve in this chapter is a classification problem on a dataset of flowers, which is available in tensorflow-datasets (tfds). The dataset's name is tf_flowers and it consists of images of five different flower species at different resolutions. Using tfds, gathering the data is straightforward, and we can get the dataset's information by looking at the info variable returned by the tfds.load invocation, as shown here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD8gP7VAZOId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "226ba306-f8ba-4c7f-c97c-1888e2bb67da"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "dataset, info = tfds.load(\"tf_flowers\", with_info=True)\n",
        "print(info)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='tf_flowers',\n",
            "    version=1.0.0,\n",
            "    description='A large set of images of flowers',\n",
            "    homepage='https://www.tensorflow.org/tutorials/load_data/images',\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=5),\n",
            "    }),\n",
            "    total_num_examples=3670,\n",
            "    splits={\n",
            "        'train': 3670,\n",
            "    },\n",
            "    supervised_keys=('image', 'label'),\n",
            "    citation=\"\"\"@ONLINE {tfflowers,\n",
            "    author = \"The TensorFlow Team\",\n",
            "    title = \"Flowers\",\n",
            "    month = \"jan\",\n",
            "    year = \"2019\",\n",
            "    url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\" }\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWymNvQCaUfv",
        "colab_type": "text"
      },
      "source": [
        "The preceding code produces the following dataset description:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb9AHYAUaSnm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "07b938a6-833e-4d90-c2d6-9dba0709b5b9"
      },
      "source": [
        "dataset = dataset[\"train\"]\n",
        "tot = 3670\n",
        "\n",
        "train_set_size = tot // 2\n",
        "validation_set_size = tot - train_set_size - train_set_size // 2\n",
        "test_set_size = tot - train_set_size - validation_set_size\n",
        "\n",
        "\n",
        "print(\"train set size: \", train_set_size)\n",
        "print(\"validation set size: \", validation_set_size)\n",
        "print(\"test set size: \", test_set_size)\n",
        "\n",
        "train, test, validation = (\n",
        "    dataset.take(train_set_size),\n",
        "    dataset.skip(train_set_size).take(validation_set_size),\n",
        "    dataset.skip(train_set_size + validation_set_size).take(test_set_size),\n",
        ")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train set size:  1835\n",
            "validation set size:  918\n",
            "test set size:  917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8jdTgfqisZS",
        "colab_type": "text"
      },
      "source": [
        "## Transfer learning\n",
        "\n",
        "Only academia and some industries have the required budget and computing power to train an entire CNN from scratch, starting from random weights, on a massive dataset such as ImageNet.\n",
        "\n",
        "Since this expensive and time-consuming work has already been done, it is a smart idea to reuse parts of the trained model to solve our classification problem.\n",
        "\n",
        "In fact, it is possible to transfer what the network has learned from one dataset to a new one, thereby transferring the knowledge.\n",
        "\n",
        "Transfer learning is the process of learning a new task by relying on a previously learned task: the learning process can be faster, more accurate, and require less training data.\n",
        "\n",
        "The transfer learning idea is bright, and it can be successfully applied when using convolutional neural networks.\n",
        "\n",
        "In fact, all convolutional architectures for classification have a fixed structure, and we can reuse parts of them as building blocks for our applications. The general structure is composed of three elements: \n",
        "\n",
        "* Input layer: The architecture is designed to accept an image with a precise resolution. The input resolution influences all of the architecture; if the input layer resolution is high, the network will be deeper.\n",
        "* Feature extractor: This is the set of convolution, pooling, normalizations, and every other layer that is in between the input layer and the first dense layer. The architecture learns to summarize all the information contained in the input image in a low-dimensional representation (in the diagram that follows, an image with a size of 227 x 227 x 3 is projected into a 9216-dimensional vector).\n",
        "* Classification layers: These are a stack of fully connected layers—a fully-connected classifier built on top of the low-dimensional representation of the input extracted by the classifier:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbzDrumPjiQ9",
        "colab_type": "text"
      },
      "source": [
        "## TensorFlow Hub\n",
        "\n",
        "The description of TensorFlow Hub that can be found on the official documentation describes what TensorFlow Hub is and what it's about pretty well:\n",
        "\n",
        "TensorFlow Hub is a library for the publication, discovery, and consumption of reusable parts of machine learning models. A module is a self-contained piece of a TensorFlow graph, along with its weights and assets, that can be reused across different tasks in a process known as transfer learning. Transfer learning can:\n",
        "\n",
        "- Train a model with a smaller dataset\n",
        "- Improve generalization, and\n",
        "- Speed up training\n",
        "Thus, TensorFlow Hub is a library we can browse while a looking for a pre-trained model that best fits our needs. TensorFlow Hub comes both as a website we can browse (https://tfhub.dev) and as a Python package.\n",
        "\n",
        "Installing the Python package allows us to have perfect integration with the modules loaded on TensorFlow Hub and TensorFlow 2.0:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt6PS4M_aeB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install tensorflow-hub>0.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgK8vg1Xjyyl",
        "colab_type": "text"
      },
      "source": [
        "That is all we need to do to get access to a complete library of pre-trained models compatible and integrated with TensorFlow.\n",
        "\n",
        "The TensorFlow 2.0 integration is terrific—we only need the URL of the module on TensorFlow Hub to create a Keras layer that contains the parts of the model we need!\n",
        "\n",
        "Browsing the catalog on https://tfhub.dev is intuitive. The screenshot that follows shows how to use the search engine to find any module that contains the string tf2 (this is a fast way to find an uploaded module that is TensorFlow 2.0 compatible and ready to use):\n",
        "\n",
        "The TensorFlow Hub website (https://tfhub.dev): it is possible to search for modules by query string (in this case, tf2) and refine the results by using the filter column on the left.\n",
        "There are models in both versions: feature vector-only and classification, which means a feature vector plus the trained classification head. The TensorFlow Hub catalog already contains everything we need for transfer learning. In the next section, we will see how easy it is to integrate the Inception v3 module from TensorFlow Hub into TensorFlow 2.0 source code, thanks to the Keras API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1AQeojvkJfw",
        "colab_type": "text"
      },
      "source": [
        "## Using Inception v3 as a feature extractor\n",
        "\n",
        "The complete analysis of the Inception v3 architecture is beyond the scope of this book; however, it is worth noting some peculiarities of this architecture so as to use it correctly for transfer learning on a different dataset.\n",
        "\n",
        "Inception v3 is a deep architecture with 42 layers, which won the ImageNet Large Scale Visual Recognition Competition (ILSVRC) in 2015. Its architecture is shown in the following screenshot:\n",
        "\n",
        "![](https://learning.oreilly.com/library/view/hands-on-neural-networks/9781789615555/assets/0ef8f6ae-c05d-4b57-9342-033cc828716f.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tni1ArWAkg2Z",
        "colab_type": "text"
      },
      "source": [
        "Inception v3 architecture. The model architecture is complicated and very deep. The network accepts a 299 x 299 x 3 image as input and produces an 8 x 8 x 2,048 feature map, which is the input of the final part; that is, a classifier trained on 1,000 +1 classes of ImageNet. Image source: https://cloud.google.com/tpu/docs/inception-v3-advanced.\n",
        "The network expects an input image with a resolution of 299 x 299 x 3 and produces an 8 x 8 x 2,048 feature map. It has been trained on 1,000 classes of the ImageNet dataset, and the input images have been scaled in the [0,1] range.\n",
        "\n",
        "All this information is available on the module page, reachable by clicking on the search result on the TensorFlow Hub website. Unlike the official architecture shown previously, on this page, we can find information about the extracted feature vector. The documentation says that it is a 2,048-feature vector, which means that the feature vector used is not the flattened feature map (that would have been an 8 * 8 * 2048 dimensional vector) but one of the fully-connected layers placed at the end of the network.\n",
        "\n",
        "It is essential to know the expected input shape and the feature vector size to feed the network with correctly resized images and to attach the final layers, knowing how many connections there would be between the feature vector and the first fully-connected layer.\n",
        "\n",
        "More importantly, it is necessary to know on which dataset the network was trained since transfer learning works well if the original dataset shares some features with the target (new) dataset. The following screenshot shows some samples gathered from the dataset used for the ILSVRC in 2015:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJRlr9RBlzT2",
        "colab_type": "text"
      },
      "source": [
        "### Adapting data to the model\n",
        "\n",
        "The information found on the module page also tells us that it is necessary to add a pre-processing step to the dataset split built earlier: the tf_flower images are tf.uint8, which means they are in the [0,255] range, while Inception v3 has been trained on images in the [0,1] range, which are thus tf.float32:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w19juAKjuu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_float_image(example):\n",
        "    example[\"image\"] = tf.image.convert_image_dtype(example[\"image\"], tf.float32)\n",
        "    return example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FAhYhvEl6Vp",
        "colab_type": "text"
      },
      "source": [
        "Moreover, the Inception architecture requires a fixed input shape of 299 x 299 x 3. Therefore, we have to ensure that all our images are correctly resized to the expected input size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG7cqxHrl7-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize(example):\n",
        "    example[\"image\"] = tf.image.resize(example[\"image\"], (299, 299))\n",
        "    return example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1jQMZP_l-ic",
        "colab_type": "text"
      },
      "source": [
        "All the required pre-processing operations have been defined, so we are ready to apply them to the train, validation, and test splits:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJiq3CHxl9SZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = train.map(to_float_image).map(resize)\n",
        "validation = validation.map(to_float_image).map(resize)\n",
        "test = test.map(to_float_image).map(resize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7T4AHonmEin",
        "colab_type": "text"
      },
      "source": [
        "To summarize: the target dataset is ready; we know which model we want to use as a feature extractor;  the module information page told us that some preprocessing steps were required to make the data compatible with the model.\n",
        "\n",
        "Everything is set up to design the classification model that uses Inception v3 as the feature extractor. In the next section, the extreme ease of use of the tensorflow-hub module is shown, thanks to its Keras integration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re_6o4WXmzYd",
        "colab_type": "text"
      },
      "source": [
        "## Building the model – hub.KerasLayer\n",
        "\n",
        "The TensorFlow Hub Python package has already been installed, and this is all we need to do:\n",
        "\n",
        "* Download the model parameters and graph description\n",
        "* Restore the parameters in its graph\n",
        "* Create a Keras layer that wraps the graph and allows us to use it like any other Keras layer we are used to using\n",
        "These three points are executed under the hook of the KerasLayer tensorflow-hub function:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5g36pIsm2Ai",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "7f423af7-624d-4189-b6db-94be1c0a4de5"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "hub.KerasLayer(\n",
        "    \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2\",\n",
        "    output_shape=[2048],\n",
        "    trainable=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_hub.keras_layer.KerasLayer at 0x7f8c25130da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smmufApinIjs",
        "colab_type": "text"
      },
      "source": [
        "The hub.KerasLayer function creates hub.keras_layer.KerasLayer, which is a tf.keras.layers.Layer object. Therefore, it can be used in the same way as any other Keras layer—this is powerful!\n",
        "\n",
        "This strict integration allows us to define a model that uses the Inception v3 as a feature extractor and it has two fully connected layers as classification layers in very few lines:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkMMVvbJnI0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 5\n",
        "\n",
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        hub.KerasLayer(\n",
        "            \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2\",\n",
        "            output_shape=[2048],\n",
        "            trainable=False,\n",
        "        ),\n",
        "        tf.keras.layers.Dense(512),\n",
        "        tf.keras.layers.ReLU(),\n",
        "        tf.keras.layers.Dense(num_classes), # linear\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1O5pBM3nRGN",
        "colab_type": "text"
      },
      "source": [
        "The model definition is straightforward, thanks to the Keras integration. Everything is set up to define the training loop, measure the performance, and see whether the transfer learning approach gives us the expected classification results.\n",
        "\n",
        "Unfortunately, the process of downloading a pre-trained model from TensorFlow Hub is fast only on high-speed internet connections. A progress bar that shows the download progress is not enabled by default and, therefore, a lot of time could be required (depending on the internet speed) to build the model for the first time.\n",
        "\n",
        "To enable a progress bar, using the TFHUB_DOWNLOAD_PROGRESS environment variable is required by hub.KerasLayer. Therefore, on top of the script, the following snippet can be added, which defines this environment variable and puts the value of 1 inside it; in this way, a handy progress bar will be shown on the first download:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw5a9JqpnLAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"TFHUB_DOWNLOAD_PROGRESS\"] = \"1\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9EwhIffogSq",
        "colab_type": "text"
      },
      "source": [
        "## Training and evaluating\n",
        "\n",
        "Using a pre-trained feature extractor allows us to speed up the training while keeping the training loop, the losses, and optimizers unchanged, using the same structure of every standard classifier train.\n",
        "\n",
        "Since the dataset labels are tf.int64 scalars, the loss that is going to be used is the standard sparse categorical cross-entropy, setting the from_logits parameter to True. As seen in the previous chapter, Chapter 5, Efficient Data Input Pipelines and Estimator API, setting this parameter to True is a good practice since, in this way, it's the loss function itself that applies the softmax activation function, being sure to compute it in a numerically stable way, and thereby preventing the loss becoming NaN:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnoEaphhoa9O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cedac648-a223-40b0-aa9b-5ca89d0f721a"
      },
      "source": [
        "# Training utilities\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "step = tf.Variable(1, name=\"global_step\", trainable=False)\n",
        "optimizer = tf.optimizers.Adam(1e-3)\n",
        "\n",
        "train_summary_writer = tf.summary.create_file_writer(\"./log/transfer/train\")\n",
        "validation_summary_writer = tf.summary.create_file_writer(\"./log/transfer/validation\")\n",
        "\n",
        "# Metrics\n",
        "accuracy = tf.metrics.Accuracy()\n",
        "mean_loss = tf.metrics.Mean(name=\"loss\")\n",
        "\n",
        "@tf.function\n",
        "def train_step(inputs, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(inputs)\n",
        "        loss_value = loss(labels, logits)\n",
        "\n",
        "    gradients = tape.gradient(loss_value, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    step.assign_add(1)\n",
        "\n",
        "    accuracy.update_state(labels, tf.argmax(logits, -1))\n",
        "    return loss_value\n",
        "\n",
        "# Configure the training set to use batches and prefetch\n",
        "train = train.batch(32).prefetch(1)\n",
        "validation = validation.batch(32).prefetch(1)\n",
        "test = test.batch(32).prefetch(1)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for example in train:\n",
        "        image, label = example[\"image\"], example[\"label\"]\n",
        "        loss_value = train_step(image, label)\n",
        "        mean_loss.update_state(loss_value)\n",
        "\n",
        "        if tf.equal(tf.math.mod(step, 10), 0):\n",
        "            tf.print(\n",
        "                step, \" loss: \", mean_loss.result(), \" acccuracy: \", accuracy.result()\n",
        "            )\n",
        "            mean_loss.reset_states()\n",
        "            accuracy.reset_states()\n",
        "\n",
        "    # Epoch ended, measure performance on validation set\n",
        "    tf.print(\"## VALIDATION - \", epoch)\n",
        "    accuracy.reset_states()\n",
        "    for example in validation:\n",
        "        image, label = example[\"image\"], example[\"label\"]\n",
        "        logits = model(image)\n",
        "        accuracy.update_state(label, tf.argmax(logits, -1))\n",
        "    tf.print(\"accuracy: \", accuracy.result())\n",
        "    accuracy.reset_states()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10  loss:  1.4032892  acccuracy:  0.479166657\n",
            "20  loss:  0.699153781  acccuracy:  0.715625\n",
            "30  loss:  0.460169077  acccuracy:  0.81875\n",
            "40  loss:  0.442007244  acccuracy:  0.81875\n",
            "50  loss:  0.490934193  acccuracy:  0.815625\n",
            "## VALIDATION -  0\n",
            "accuracy:  0.813522339\n",
            "60  loss:  0.43458119  acccuracy:  0.96875\n",
            "70  loss:  0.495889515  acccuracy:  0.8375\n",
            "80  loss:  0.314885676  acccuracy:  0.88125\n",
            "90  loss:  0.272703886  acccuracy:  0.9\n",
            "100  loss:  0.211857349  acccuracy:  0.925\n",
            "110  loss:  0.283404648  acccuracy:  0.8875\n",
            "## VALIDATION -  1\n",
            "accuracy:  0.866957486\n",
            "120  loss:  0.285670698  acccuracy:  0.90625\n",
            "130  loss:  0.258526415  acccuracy:  0.903125\n",
            "140  loss:  0.180402458  acccuracy:  0.934375\n",
            "150  loss:  0.174977928  acccuracy:  0.9375\n",
            "160  loss:  0.154107615  acccuracy:  0.946875\n",
            "170  loss:  0.184987038  acccuracy:  0.9375\n",
            "## VALIDATION -  2\n",
            "accuracy:  0.877862573\n",
            "180  loss:  0.199962854  acccuracy:  0.91875\n",
            "190  loss:  0.135458723  acccuracy:  0.953125\n",
            "200  loss:  0.108723208  acccuracy:  0.953125\n",
            "210  loss:  0.12651059  acccuracy:  0.95625\n",
            "220  loss:  0.136738956  acccuracy:  0.959375\n",
            "230  loss:  0.135062486  acccuracy:  0.94375\n",
            "## VALIDATION -  3\n",
            "accuracy:  0.863685906\n",
            "240  loss:  0.127110451  acccuracy:  0.955357134\n",
            "250  loss:  0.0823029354  acccuracy:  0.984375\n",
            "260  loss:  0.0893819109  acccuracy:  0.96875\n",
            "270  loss:  0.0733094364  acccuracy:  0.971875\n",
            "280  loss:  0.0817203522  acccuracy:  0.98125\n",
            "290  loss:  0.0763656721  acccuracy:  0.971875\n",
            "## VALIDATION -  4\n",
            "accuracy:  0.890948772\n",
            "300  loss:  0.0821932554  acccuracy:  0.975694418\n",
            "310  loss:  0.0572496168  acccuracy:  0.9875\n",
            "320  loss:  0.0735980719  acccuracy:  0.984375\n",
            "330  loss:  0.0525555126  acccuracy:  0.984375\n",
            "340  loss:  0.0666934103  acccuracy:  0.990625\n",
            "## VALIDATION -  5\n",
            "accuracy:  0.896401286\n",
            "350  loss:  0.0575850494  acccuracy:  0.96875\n",
            "360  loss:  0.0660181493  acccuracy:  0.9875\n",
            "370  loss:  0.0749291629  acccuracy:  0.975\n",
            "380  loss:  0.0377932265  acccuracy:  1\n",
            "390  loss:  0.0594712794  acccuracy:  0.9875\n",
            "400  loss:  0.0547059886  acccuracy:  0.990625\n",
            "## VALIDATION -  6\n",
            "accuracy:  0.875681579\n",
            "410  loss:  0.0619379207  acccuracy:  0.979166687\n",
            "420  loss:  0.0546870306  acccuracy:  0.990625\n",
            "430  loss:  0.0617809109  acccuracy:  0.978125\n",
            "440  loss:  0.0374584571  acccuracy:  0.99375\n",
            "450  loss:  0.0852948353  acccuracy:  0.959375\n",
            "460  loss:  0.105923846  acccuracy:  0.95625\n",
            "## VALIDATION -  7\n",
            "accuracy:  0.857142866\n",
            "470  loss:  0.114943899  acccuracy:  0.94375\n",
            "480  loss:  0.0731958672  acccuracy:  0.975\n",
            "490  loss:  0.0847677588  acccuracy:  0.959375\n",
            "500  loss:  0.0688740462  acccuracy:  0.965625\n",
            "510  loss:  0.132943809  acccuracy:  0.940625\n",
            "520  loss:  0.141197518  acccuracy:  0.946875\n",
            "## VALIDATION -  8\n",
            "accuracy:  0.860414386\n",
            "530  loss:  0.153286427  acccuracy:  0.941964269\n",
            "540  loss:  0.108754858  acccuracy:  0.95\n",
            "550  loss:  0.0771886557  acccuracy:  0.975\n",
            "560  loss:  0.0849920288  acccuracy:  0.9625\n",
            "570  loss:  0.0583152846  acccuracy:  0.978125\n",
            "580  loss:  0.120678566  acccuracy:  0.95\n",
            "## VALIDATION -  9\n",
            "accuracy:  0.856052339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlW23v6Fo-Cs",
        "colab_type": "text"
      },
      "source": [
        "After a single training epoch, we got a validation accuracy of 0.87, while the training accuracy was even lower (0.83). But by the end of the tenth epoch, the validation accuracy had even decreased (0.86), while the model was overfitting the training data.\n",
        "\n",
        "In the Exercises section, you will find several exercises that use the previous code as a starting point; the overfitting problem should be tackled from several points of view, finding the best way to deal with it.\n",
        "\n",
        "Before starting the next main section, it's worth adding a simple performance measurement that measures how much time is needed to compute a single training epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR4JGCOkpr4a",
        "colab_type": "text"
      },
      "source": [
        "## Training speed\n",
        "\n",
        "Faster prototyping and training is one of the strengths of the transfer learning approach. One of the reasons behind the fact that transfer learning is often used in industry is the financial savings that it produces, reducing both the development and training time.\n",
        "\n",
        "To measure the training time, the Python time package can be used. time.time() returns the current timestamp, allowing you to measure (in milliseconds) how much time is needed to perform a training epoch.\n",
        "\n",
        "The training loop of the previous section can thus be extended by adding the time module import and the duration measurement:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1XjGWXCouGs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b3177dbc-0810-4c3a-b202-8c4802e0d75a"
      },
      "source": [
        "from time import time\n",
        "\n",
        "# [...]\n",
        "for epoch in range(num_epochs):\n",
        "    start = time()\n",
        "    for example in train:\n",
        "        image, label = example[\"image\"], example[\"label\"]\n",
        "        loss_value = train_step(image, label)\n",
        "        mean_loss.update_state(loss_value)\n",
        "\n",
        "        if tf.equal(tf.math.mod(step, 10), 0):\n",
        "            tf.print(\n",
        "                step, \" loss: \", mean_loss.result(), \" acccuracy: \", accuracy.result()\n",
        "            )\n",
        "            mean_loss.reset_states()\n",
        "            accuracy.reset_states()\n",
        "    end = time()\n",
        "    print(\"Time per epoch: \", end-start)\n",
        "# remeaning code"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "590  loss:  0.16144824  acccuracy:  0.954861104\n",
            "600  loss:  0.0597647242  acccuracy:  0.9875\n",
            "610  loss:  0.0527747273  acccuracy:  0.984375\n",
            "620  loss:  0.0505279601  acccuracy:  0.984375\n",
            "630  loss:  0.101979196  acccuracy:  0.9625\n",
            "Time per epoch:  5.251030921936035\n",
            "640  loss:  0.0909952298  acccuracy:  0.969899654\n",
            "650  loss:  0.0716021508  acccuracy:  0.984375\n",
            "660  loss:  0.080451481  acccuracy:  0.96875\n",
            "670  loss:  0.0813035071  acccuracy:  0.96875\n",
            "680  loss:  0.0819518417  acccuracy:  0.965625\n",
            "690  loss:  0.15046224  acccuracy:  0.934375\n",
            "Time per epoch:  5.181036949157715\n",
            "700  loss:  0.128995061  acccuracy:  0.946488321\n",
            "710  loss:  0.180704758  acccuracy:  0.928125\n",
            "720  loss:  0.0911093801  acccuracy:  0.95625\n",
            "730  loss:  0.0698170885  acccuracy:  0.975\n",
            "740  loss:  0.0626426861  acccuracy:  0.975\n",
            "750  loss:  0.12250489  acccuracy:  0.95625\n",
            "Time per epoch:  5.160876035690308\n",
            "760  loss:  0.109201357  acccuracy:  0.956521749\n",
            "770  loss:  0.29387337  acccuracy:  0.896875\n",
            "780  loss:  0.116944477  acccuracy:  0.95\n",
            "790  loss:  0.06775181  acccuracy:  0.96875\n",
            "800  loss:  0.0672911331  acccuracy:  0.975\n",
            "810  loss:  0.101786077  acccuracy:  0.975\n",
            "Time per epoch:  5.318925619125366\n",
            "820  loss:  0.117590286  acccuracy:  0.953177273\n",
            "830  loss:  0.179160804  acccuracy:  0.95\n",
            "840  loss:  0.129746333  acccuracy:  0.94375\n",
            "850  loss:  0.0763826072  acccuracy:  0.971875\n",
            "860  loss:  0.0918291733  acccuracy:  0.959375\n",
            "870  loss:  0.0991394073  acccuracy:  0.959375\n",
            "Time per epoch:  4.968677997589111\n",
            "880  loss:  0.0771546513  acccuracy:  0.956521749\n",
            "890  loss:  0.0751793459  acccuracy:  0.9875\n",
            "900  loss:  0.0508796573  acccuracy:  0.975\n",
            "910  loss:  0.08541058  acccuracy:  0.96875\n",
            "920  loss:  0.0544046946  acccuracy:  0.971875\n",
            "Time per epoch:  5.1665120124816895\n",
            "930  loss:  0.016650673  acccuracy:  0.996655524\n",
            "940  loss:  0.0441706702  acccuracy:  0.98125\n",
            "950  loss:  0.0400045365  acccuracy:  0.98125\n",
            "960  loss:  0.0488646701  acccuracy:  0.98125\n",
            "970  loss:  0.0561905578  acccuracy:  0.975\n",
            "980  loss:  0.0966351181  acccuracy:  0.96875\n",
            "Time per epoch:  4.977979898452759\n",
            "990  loss:  0.0328431278  acccuracy:  0.983277619\n",
            "1000  loss:  0.032663174  acccuracy:  0.984375\n",
            "1010  loss:  0.0244172681  acccuracy:  0.990625\n",
            "1020  loss:  0.0219685845  acccuracy:  0.990625\n",
            "1030  loss:  0.0443467908  acccuracy:  0.978125\n",
            "1040  loss:  0.170268491  acccuracy:  0.940625\n",
            "Time per epoch:  5.156139373779297\n",
            "1050  loss:  0.0919851288  acccuracy:  0.966555178\n",
            "1060  loss:  0.0571668856  acccuracy:  0.978125\n",
            "1070  loss:  0.0209997874  acccuracy:  0.996875\n",
            "1080  loss:  0.0329496041  acccuracy:  0.9875\n",
            "1090  loss:  0.0331586115  acccuracy:  0.99375\n",
            "1100  loss:  0.0507713333  acccuracy:  0.984375\n",
            "Time per epoch:  5.286965370178223\n",
            "1110  loss:  0.0889233425  acccuracy:  0.976588607\n",
            "1120  loss:  0.0819337815  acccuracy:  0.965625\n",
            "1130  loss:  0.0807305127  acccuracy:  0.959375\n",
            "1140  loss:  0.076994583  acccuracy:  0.965625\n",
            "1150  loss:  0.0219728  acccuracy:  0.996875\n",
            "1160  loss:  0.0255032815  acccuracy:  0.990625\n",
            "Time per epoch:  5.065440893173218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JebZ-zUqp9J8",
        "colab_type": "text"
      },
      "source": [
        "On average, running the training loop on a Colab notebook (https://colab.research.google.com) equipped with an Nvidia k40 GPU, we obtain an execution speed as follows:\n",
        "\n",
        "```\n",
        "Time per epoch: 16.206\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCkB5hnJqCvY",
        "colab_type": "text"
      },
      "source": [
        "As shown in the next section, transfer learning using a pre-trained model as a feature extractor gives a considerable speed boost.\n",
        "\n",
        "Sometimes, using a pre-trained model as a feature extractor only is not the best way to transfer knowledge from one domain to another, often because the domains are too different and the features learned are useless for solving the new task.\n",
        "\n",
        "In these cases, it is possible—and recommended—to not have a fixed-feature extractor part but let the optimization algorithm change it, training the whole model end to end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpN96-iHuAb6",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tuning\n",
        "\n",
        "Fine-tuning is a different approach to transfer learning. Both share the same goal of transferring the knowledge learned on a dataset on a specific task to a different dataset and a different task. Transfer learning, as shown in the previous section, reuses the pre-trained model without making any changes to its feature extraction part; in fact, it is considered a non-trainable part of the network.\n",
        "\n",
        "Fine-tuning, instead, consists of fine-tuning the pre-trained network weights by continuing backpropagation.\n",
        "\n",
        "## When to fine-tune\n",
        "\n",
        "Fine-tuning a network requires having the correct hardware; backpropagating the gradients through a deeper network requires you to load more information in memory. Very deep networks have been trained from scratch in data centers with thousands of GPUs. Therefore, prepare to lower your batch size to as low as 1, depending on how much available memory you have.\n",
        "\n",
        "Hardware requirements aside, there are other different points to keep in mind when thinking about fine-tuning:\n",
        "\n",
        "* Dataset size: Fine-tuning a network means using a network with a lot of trainable parameters, and, as we know from the previous chapters, a network with a lot of parameters is prone to overfitting.\n",
        "If the target dataset size is small, it is not a good idea to fine-tune the network. Using the network as a fixed-feature extractor will probably bring in better results.\n",
        "* Dataset similarity: If the dataset size is large (where large means with a size comparable to the one the pre-trained model has been trained on) and it is similar to the original one, fine-tuning the model is probably a good idea. Slightly adjusting the network parameters will help the network to specialize in the extraction of features that are specific to this dataset, while correctly reusing the knowledge from the previous, similar dataset.\n",
        "If the dataset size is large and it is very different from the original, fine-tuning the network could help. In fact, the initial solution of the optimization problem is likely to be close to a good minimum when starting with a pre-trained model, even if the dataset has different features to learn (this is because the lower layers of the CNN usually learn low-level features that are common to every classification task).\n",
        "\n",
        "If the new dataset satisfies the similarity and size constraints, fine-tuning the model is a good idea. **One important parameter to look at closely is the learning rate.** When fine-tuning a pre-trained model, we suppose the model parameters are good (and they are since they are the parameters of the model that achieved state-of-the-art results on a difficult challenge), and, for this reason, a small learning rate is suggested.\n",
        "\n",
        "Using a high learning rate would change the network parameters too much, and we don't want to change them in this way. Instead, using a small learning rate, we slightly adjust the parameters to make them adapt to the new dataset, without distorting them too much, thus reusing the knowledge without destroying it.\n",
        "\n",
        "Of course, if the fine-tuning approach is chosen, the hardware requirements have to be kept in mind: lowering the batch size is perhaps the only way to fine-tune very deep models when using a standard GPU to do the work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNR3dSwRzCdm",
        "colab_type": "text"
      },
      "source": [
        "## TensorFlow Hub integration\n",
        "\n",
        "Fine-tuning a model downloaded from TensorFlow Hub might sound difficult; we have to do the following:\n",
        "\n",
        "1. Download the model parameters and graph\n",
        "1. Restore the model parameters in the graph\n",
        "1. Restore all the operations that are executed only during the training (activating dropout layers and enabling the moving mean and variance computed by the batch normalization layers)\n",
        "1. Attach the new layers on top of the feature vector\n",
        "1. Train the model end to end\n",
        "\n",
        "In practice, the integration of TensorFlow Hub and Keras models is so tight that we can achieve all this by setting the trainable Boolean flag to True when importing the model using hub.KerasLayer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQf1cbxZqCiQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "c1e36d2e-5e71-46dc-efa1-43e4250cd9a4"
      },
      "source": [
        "hub.KerasLayer(\n",
        "    \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2\",\n",
        "    output_shape=[2048],\n",
        "    trainable=True) # <- That's all!"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_hub.keras_layer.KerasLayer at 0x7f8b90916128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGvQb3-Pzdsy",
        "colab_type": "text"
      },
      "source": [
        "## Train and evaluate\n",
        "\n",
        "What happens if we build the same model as in the previous chapter, Chapter 5, Efficient Data Input Pipelines and Estimator API, and we train it on the tf_flower dataset, fine-tuning the weights?\n",
        "\n",
        "The model is thus the one that follows; please note how the learning rate of the optimizer has been reduced from 1e-3 to 1e-5:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAAeojO722Rr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "253e936d-008b-4bf7-d044-de4f7854ecff"
      },
      "source": [
        "train"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: {image: (299, 299, 3), label: ()}, types: {image: tf.float32, label: tf.int64}>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aH7MXLKp46B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "f32f7383-8ba0-4427-9ca6-c5c12b173ff9"
      },
      "source": [
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        hub.KerasLayer(\n",
        "            \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2\",\n",
        "            output_shape=[2048],\n",
        "            trainable=True, # <- enables fine tuning\n",
        "        ),\n",
        "        tf.keras.layers.Dense(512),\n",
        "        tf.keras.layers.ReLU(),\n",
        "        tf.keras.layers.Dense(num_classes), # linear\n",
        "    ]\n",
        ")\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "step = tf.Variable(1, name=\"global_step\", trainable=False)\n",
        "optimizer = tf.optimizers.Adam(1e-5)\n",
        "\n",
        "train_summary_writer = tf.summary.create_file_writer(\"./log/transfer/train\")\n",
        "validation_summary_writer = tf.summary.create_file_writer(\"./log/transfer/validation\")\n",
        "\n",
        "# Metrics\n",
        "accuracy = tf.metrics.Accuracy()\n",
        "mean_loss = tf.metrics.Mean(name=\"loss\")\n",
        "\n",
        "@tf.function\n",
        "def train_step(inputs, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(inputs)\n",
        "        loss_value = loss(labels, logits)\n",
        "\n",
        "    gradients = tape.gradient(loss_value, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    step.assign_add(1)\n",
        "\n",
        "    accuracy.update_state(labels, tf.argmax(logits, -1))\n",
        "    return loss_value\n",
        "\n",
        "# Configure the training set to use batches and prefetch\n",
        "train = train.batch(32).prefetch(1)\n",
        "validation = validation.batch(32).prefetch(1)\n",
        "test = test.batch(32).prefetch(1)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for example in train:\n",
        "        image, label = example[\"image\"], example[\"label\"]\n",
        "        loss_value = train_step(image, label)\n",
        "        mean_loss.update_state(loss_value)\n",
        "\n",
        "        if tf.equal(tf.math.mod(step, 10), 0):\n",
        "            tf.print(\n",
        "                step, \" loss: \", mean_loss.result(), \" acccuracy: \", accuracy.result()\n",
        "            )\n",
        "            mean_loss.reset_states()\n",
        "            accuracy.reset_states()\n",
        "\n",
        "    # Epoch ended, measure performance on validation set\n",
        "    tf.print(\"## VALIDATION - \", epoch)\n",
        "    accuracy.reset_states()\n",
        "    for example in validation:\n",
        "        image, label = example[\"image\"], example[\"label\"]\n",
        "        logits = model(image)\n",
        "        accuracy.update_state(label, tf.argmax(logits, -1))\n",
        "    tf.print(\"accuracy: \", accuracy.result())\n",
        "    accuracy.reset_states()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10  loss:  1.46650445  acccuracy:  0.40625\n",
            "20  loss:  1.12444711  acccuracy:  0.615625\n",
            "30  loss:  0.757353902  acccuracy:  0.790625\n",
            "40  loss:  0.498438358  acccuracy:  0.85625\n",
            "50  loss:  0.43775019  acccuracy:  0.8375\n",
            "## VALIDATION -  0\n",
            "accuracy:  0.869138479\n",
            "60  loss:  0.344315886  acccuracy:  0.96875\n",
            "70  loss:  0.337916344  acccuracy:  0.890625\n",
            "80  loss:  0.198741242  acccuracy:  0.93125\n",
            "90  loss:  0.192094058  acccuracy:  0.925\n",
            "100  loss:  0.11699377  acccuracy:  0.971875\n",
            "110  loss:  0.115660056  acccuracy:  0.978125\n",
            "## VALIDATION -  1\n",
            "accuracy:  0.884405673\n",
            "120  loss:  0.0963624641  acccuracy:  0.96875\n",
            "130  loss:  0.0865608603  acccuracy:  0.98125\n",
            "140  loss:  0.0634469837  acccuracy:  0.9875\n",
            "150  loss:  0.0516169295  acccuracy:  0.990625\n",
            "160  loss:  0.0408696234  acccuracy:  0.996875\n",
            "170  loss:  0.0368052311  acccuracy:  0.99375\n",
            "## VALIDATION -  2\n",
            "accuracy:  0.897491813\n",
            "180  loss:  0.0522813089  acccuracy:  0.99375\n",
            "190  loss:  0.0380167142  acccuracy:  0.990625\n",
            "200  loss:  0.0273939613  acccuracy:  0.99375\n",
            "210  loss:  0.0223091152  acccuracy:  1\n",
            "220  loss:  0.016185971  acccuracy:  0.996875\n",
            "230  loss:  0.0191529579  acccuracy:  1\n",
            "## VALIDATION -  3\n",
            "accuracy:  0.894220293\n",
            "240  loss:  0.0275812652  acccuracy:  0.995535731\n",
            "250  loss:  0.0270102229  acccuracy:  0.996875\n",
            "260  loss:  0.0366355292  acccuracy:  0.984375\n",
            "270  loss:  0.0262363255  acccuracy:  0.990625\n",
            "280  loss:  0.0170074217  acccuracy:  0.996875\n",
            "290  loss:  0.0118701402  acccuracy:  0.996875\n",
            "## VALIDATION -  4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNSvDmmyzkJu",
        "colab_type": "text"
      },
      "source": [
        "In the following box, the first and last training epochs' output is shown:\n",
        "\n",
        "```\n",
        "10 loss: 1.59038031 acccuracy: 0.288194448\n",
        "20 loss: 1.25725865 acccuracy: 0.55625\n",
        "30 loss: 0.932323813 acccuracy: 0.721875\n",
        "40 loss: 0.63251847 acccuracy: 0.81875\n",
        "50 loss: 0.498087496 acccuracy: 0.84375\n",
        "## VALIDATION - 0\n",
        "accuracy: 0.872410059\n",
        "\n",
        "[...]\n",
        "\n",
        "530 loss: 0.000400377758 acccuracy: 1\n",
        "540 loss: 0.000466914673 acccuracy: 1\n",
        "550 loss: 0.000909397728 acccuracy: 1\n",
        "560 loss: 0.000376881275 acccuracy: 1\n",
        "570 loss: 0.000533850689 acccuracy: 1\n",
        "580 loss: 0.000438459858 acccuracy: 1\n",
        "## VALIDATION - 9\n",
        "accuracy: 0.925845146\n",
        "```\n",
        "\n",
        "As expected, the test accuracy reached the constant value of 1; hence we overfitted the training set. This was something expected since the tf_flower dataset is smaller and simpler than ImageNet. However, to see the overfitting problem clearly, we had to wait longer since having more parameters to train makes the whole learning process extremely slow, especially compared to the previous train when the pre-trained model was not trainable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLDLZ3xJ1V2G",
        "colab_type": "text"
      },
      "source": [
        "## Training speed\n",
        "\n",
        "By adding the time measurements as we did in the previous section, it is possible to see how the fine-tuning process is extremely slow compared to transfer learning, using the model as a non-trainable feature extractor.\n",
        "\n",
        "In fact, if, in the previous scenario, we reached an average training speed per epoch of about 16.2 seconds, now we have to wait, on average, 60.04 seconds, which is a 370% slowdown!\n",
        "\n",
        "Moreover, it is interesting to see that at the end of the first epoch, we reached the same validation accuracy as was achieved in the previous training and that, despite overfitting the training data, the validation accuracy obtained at the end of the tenth epoch is greater than the previous one.\n",
        "\n",
        "This simple experiment showed how using a pre-trained model as a feature extractor could lead to worse performance than fine-tuning it. This means that the features the network learned to extract on the ImageNet dataset are too different from the features that would be needed to classify the flowers, dataset correctly.\n",
        "\n",
        "Choosing whether to use a pre-trained model as a fixed-feature extractor or to fine-tune it is a tough decision, involving a lot of trade-offs. Understanding whether the pre-trained model extracts features that are correct for the new task is complicated; merely looking at dataset size and similarity is a guideline, but in practice, this decision requires several tests.\n",
        "\n",
        "Of course, it is better to use the pre-trained model as a feature extractor first, and, if the new model's performance is already satisfactory, there is no need to waste time trying to fine-tune it. If the results are not satisfying, it is worth trying a different pre-trained model and, as a last resort, trying the fine-tuning approach (because this requires more computational power, and it is expansive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV7mJUmLziI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}